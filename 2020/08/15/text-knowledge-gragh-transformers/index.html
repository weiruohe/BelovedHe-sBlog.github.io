<!DOCTYPE HTML>
<html lang="en">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="text knowledge-gragh transformers, NLP，知识图谱，liudongdong1 .etc">
    <meta name="description" content="">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>text knowledge-gragh transformers | BelovedHe&#39;s Blogs</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 4.2.0"><link rel="alternate" href="/atom.xml" title="BelovedHe's Blogs" type="application/atom+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">BelovedHe&#39;s Blogs</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>Index</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>Tags</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>Categories</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>Archives</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">

      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>About</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/about">
          
          <i class="fas fa-user-circle" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>about</span>
        </a>
      </li>
      
      <li>
        <a href="/resume">
          
          <i class="fa fa-user-secret" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>resume</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>Friends</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="Search" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">BelovedHe&#39;s Blogs</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			Index
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			Tags
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			Categories
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			Archives
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-user-circle"></i>
			
			About
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/about " style="margin-left:75px">
				  
				   <i class="fa fas fa-user-circle" style="position: absolute;left:50px" ></i>
			      
		          <span>about</span>
                  </a>
                </li>
              
                <li>

                  <a href="/resume " style="margin-left:75px">
				  
				   <i class="fa fa fa-user-secret" style="position: absolute;left:50px" ></i>
			      
		          <span>resume</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			Friends
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/weiruohe" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/weiruohe" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">text knowledge-gragh transformers</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        height: calc(100vh - 250px);
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    .toc-fixed .toc-link::before{
        position: fixed!important;/*当toc的位置改为fixed时，.toc-link::before也要改为fixed*/
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/gragh-transformer/">
                                <span class="chip bg-color">gragh transformer</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/PaperLookThrough/" class="post-category">
                                PaperLookThrough
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>Publish Date:&nbsp;&nbsp;
                    2020-08-15
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>Update Date:&nbsp;&nbsp;
                    2020-08-27
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>Word Count:&nbsp;&nbsp;
                    4.3k
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-clock fa-fw"></i>Read Times:&nbsp;&nbsp;
                    18 Min
                </div>
                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>Read Count:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <h3 id="Name"><a href="#Name" class="headerlink" title="Name:"></a>Name:</h3><p><strong>Text Generation from Knowledge Graphs with Graph Transformers</strong></p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote>
<p> ​    Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. </p>
<p> ​    生成包含复杂含义并且横跨多个句子的文本需要一种对他们文章结构化的表达（文本计划），但是这些表达对于自然生产来说却过于昂贵。</p>
<p>  In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system,and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a signifificant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long distance dependencies, and structural variety.</p>
<p> ​    本文解决了从信息提取系统，特别是知识图谱中生成连续多个句子组成的文本的难题。图形知识对计算机而言是独特的，但由于文本技术无等级的性质，长距离依赖的崩溃以及结构的多样性，图形知识生成技术对于文本生成技术也是一个挑战。</p>
<p> We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints.</p>
<p> 我们介绍一种创新的图转化编码器，这种编码器可以在没有线性和等级限制的前提下，利用这种知识图谱的关系结构。</p>
<p> Incorporated into an encoder-decoder setup,we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientifific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.</p>
<p> 结合编码解码器我们为在科学文本领域应用的图到文本生成器提供了一种端到端的训练系统。自动和人工的测量展现了我们的技术生产出更多含义丰富的文本，这些文本相对于有竞争力的编码解码器展现出更好的文档结构。</p>
</blockquote>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote>
<p>​    Increases in computing power and model capacity have made it possible to generate mostly grammatical sentence-length strings of natural language text. However, generating several sentences related to a topic and which display overall coherence and discourse-relatedness is an open challenge. The difficulties are compounded in domains of interest such as scientific writing.</p>
</blockquote>
<p>计算能力和模型容量的增强和增大使得生成主要是自然语言文本的语法句子长度成为可能。然而，生成一些与主题相关的句子并且显示出整体一致性和语篇相关性是一个公开的挑战。这些困难在像科学写作的兴趣领域中变得更为复杂。</p>
<blockquote>
<p>Additionally, there are strong constraints on document structure, as scientifific communication requires carefully ordered explanations of processes and phenomena.</p>
</blockquote>
<blockquote>
<p>另外，对于文档结构而言有强的限制，作为科学文档需要对过程和现象进行仔细有趣的解释有序的解释。之前研究者们采用手动注释的方法，现在就采用自动注释的方法，但是存在但由于它们的自动性，它们也给生成带来了挑战，如错误注释、结构多样性和表面文本特征(如GR)的显著抽象 语法关系或谓词结构)。</p>
</blockquote>
<blockquote>
<p>​        To effect our study, we use a collection of abstracts from a corpus of scientifific articles (Ammar et al., 2018). We extract entity, coreference, and relation annotations for each abstract with a state-of-the-art information extraction system (Luan et al., 2018), and represent the annotations as a knowledge graph* which collapses co-referential entities. An example of a text and graph are shown in Figure 1. We use these graph/text pairs to train a novel attention-based encoder-decoder model for knowledge-graph-to-text generation. Our model,GraphWriter, extends the successful Transformer for text encoding (Vaswani et al., 2017) to graph structured inputs, building on the recent Graph Attention Network architecture (Velickovic et al.,2018). The result is a powerful, general model for graph encoding which can incorporate global structural information when contextualizing vertices in their local neighborhoods.</p>
</blockquote>
<p>​            为了实施我们的研究，我们使用了科学杂志语料库的摘要集（Ammar等人，2018），我么使用最先进的信息提取系统提取出了每个摘要集的实体，引用以及关系注释（Luan 等人，2018），并且将注释生成为一个知识图谱，该图谱折叠了一些共同引用的实体。一个关于文本和图的例子在图一中展示。</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200818092807792.png" alt=""></p>
<p>我们使用这些图形文本对为了知识图谱到文本的生成器去训练一种创新的基于注意力机制的编码解码器。我们的模型，Graphwriter，将文本编码成功的转换器扩展到图形化的输入。建立在</p>
<p>最近的图注意力网络结构。这个结果是生成一个强大并且通用的模型，这个模型是针对在目标词上下文进行语境化时能够合并全局结构化信息的图形编码建立的。</p>
<blockquote>
<p>The main contributions of this work include:</p>
<ol>
<li><p>We propose a new graph transformer encoder that applies the successful sequence transformer to graph structured inputs.</p>
</li>
<li><p>We show how IE output can be formed as a connected unlabeled graph for use in attention-based encoders.</p>
</li>
<li><p>We provide a large dataset of knowledge graphs paired with scientifific texts for further</p>
<p>study.</p>
</li>
</ol>
</blockquote>
<blockquote>
<p>​    Through detailed automatic and human evaluations, we demonstrate that automatically extracted knowledge can be used for multi-sentence text generation. We further show that structuring and encoding this knowledge as a graph leads to improved generation performance compared to other encoder-decoder setups. Finally, we show that GraphWriter’s transformer-style encoder is more effective than Graph Attention Networks on the knowledge-graph-to-text task.</p>
</blockquote>
<p>​         通过详细的自动和人工评估，我们证明了自动提取的知识可以用于多句文本生成。 我们进一步证明了与其他编解码器设置相比，结构化以及将此类知识当作图形进行编码都产生了更好的生成性能。 最后，我们证明了图形写入器的转换器式编码器比在知识图谱到文本任务上的图形注意力网络更有效率。</p>
<h3 id="3-The-AGENDA-Dataset"><a href="#3-The-AGENDA-Dataset" class="headerlink" title="3.The AGENDA Dataset"></a>3.The AGENDA Dataset</h3><p>我们考虑从自动提取的信息（知识）生成文本的问题。 IE系统可以为各种领域提供高质量的知识，从句子甚至文档边界中综合信息。从知识生成连贯的文本需要一个模型，该模型考虑知识的全局特征以及每个实体的局部特征。这项任务的功能促使我们使用图来表示知识，邻居通过图来定位重要信息，并且路径通过图建立了中间节点之间的远距离节点之间的连接。示例知识图如图1所示。</p>
<p>我们将问题表达如下：给定科学文章的标题和由自动信息提取系统构建的知识图，目标是生成一个摘要，其中a)适用于给定的标题，b)表示该标题的内容。自然语言文本中的知识图。为了评估模型完成此目标的能力，我们引入了Abstract GENeration DAtaset（AGENDA），这是一个与科学摘要配对的知识图谱数据集。我们的数据集包含来自12个顶级AI会议(Ammar et al.，2018)会议过程的语义学者语料库(Semantic Scholar Corpus)的4万篇论文标题和摘要。</p>
<p>对于每个摘要，我们分两步创建一个知识图谱。首先，我们应用Luan等人(2018年)的SciIE系统，一个最新的科学领域信息提取系统。该系统提供科学术语的命名实体识别，其实体类型为”Task”，“Method”,“Metric”,“Material”或”Other-Scientific Term”。该模型还产生共参考注释(co-reference annotations)以及可以在不同实体之间获得的七个关系（Compare，Used-for，Feature-of，Hyponym-of，Evaluate和Conjunction）。例如，在图1中，标记为”SemEval 2011 Task 11”的节点的类型为”Task”,“HMM Models”的类型为” Model”，并且存在”Evaluate-For”关系，该关系表明在任务上已被评估模型。</p>
<p>我们将这些注释形成知识图谱。 我们将共同引用实体折叠到与最长提及相关联的单个节点中（假设这些实体将提供最多信息）。 然后，我们使用关系注释将节点彼此连接，将它们视为图形中的标记边缘。 结果是给定摘要的SciIE注释可能是未连接的图形表示形式。<br>表1中提供了AGENDA数据集的统计信息。我们将AGENDA数据集分为38,720个训练，1000个验证和1000个测试数据点。 我们提供标准化的数据拆分，以方便比较。</p>
<p><img src="https://img-blog.csdnimg.cn/20200209221151600.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FSUE9TUEY=,size_16,color_FFFFFF,t_70" alt=""></p>
<h3 id="4-Model"><a href="#4-Model" class="headerlink" title="4.Model"></a>4.Model</h3><blockquote>
<p>​        Following most work on neural generation we adopt an encoder-decoder architecture, shown in Figure 3, which we call GraphWriter. The input to GraphWriter is a title and a knowledge graph which are encoded respectively with a bidirectional recurrent neural network and a novel Graph Transformer architecture (to be discussed in Section 4.1). At each decoder time step, we attend on encodings of the knowledge graph and document title using the decoder hidden state ht ∈ Rd. The resulting vectors are used to select output wt either from the decoder’s vocabulary or by copying an entity from the knowledge graph. Details of our decoding process are described in Section 4.2. The model is trained end-to-end to minimize the negative log likelihood of the mixed copy and vocabulary probability distribution and the human authored text.</p>
</blockquote>
<p>​        在神经系统生成的大部分工作后，我们生成了一种在图3中展示出来的被称为GraphWriter的编码解码结构。GraphWriter的输入是可以分别用BRNN和新的图形Transformer结构编码的一个标题和一个知识图谱。在每个解码的时间点上，我们使用解码的隐藏状态ht∈Rd对知识图谱和文档标题进行编码。结果向量被用于去从解码器词汇表中选择一个wi输出或者从知识图谱中复制一个实体。解码过程的细节在4.2部分中被描述，这个模型在端到端上被训练去最小化副本、词汇概率分布以及人类创作的文本的负对数似然。</p>
<h3 id="4-1Encoder"><a href="#4-1Encoder" class="headerlink" title="4.1Encoder"></a>4.1Encoder</h3><blockquote>
<p>​    The AGENDA dataset contains a knowledge graph for each datapoint, but our model requires unlabeled, connected graphs as input. To encode knowledge graphs with this model, we restructure each graph as an unlabeled connected graph, preserving label information by the method described below and sketched in Figure 2.</p>
</blockquote>
<p>​        AGENDA数据集对每个数据点都包含一个知识图谱，但是我们的模型需要无标记、相连接的图谱作为输入。为了用这个模型编码知识图谱，我们将每个图谱作为未标记已连接的图谱，如图2所示通过下面描述的方法保存标签信息。</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200823111326465.png" alt=""></p>
<p>[^将未连接已标记的图谱转化成已连接未标记的图谱，作用于基于注意力机制的编码器中。vi是顶点，Rij是关系，G是全局文本节点。]: </p>
<blockquote>
<p><strong>Graph Preparation</strong> </p>
<p>​        We convert each graph to an unlabeled connected bipartite graphs following a similar procedure to Beck et al. (2018). In this process, each labeled edge is replaced with two vertices: one representing the forward direction of the relation and one representing the reverse. These new vertices are then connected to the entity vertices so that the directionality of the former edge is maintained. This restructures the original knowledge graph as an unlabeled directed graph where all vertices correspond to entities and relations in the SciIE annotations without loss of information. To promote information flow between disconnected parts of the graph, we add a global vertex which connects all entity vertices. This global vertex will be used to initialize the decoder, analogously to the fifinal encoder hidden state in a traditional sequence to sequence model. The fifinal result of these restructuring operations is a connected, unlabeled graph <em>G</em> = (<em>V, E</em>), where <em>V</em> is a list of entities, relations, and a global node and<em>E</em> is an adjacency matrix describing the directed edges.</p>
</blockquote>
<h3 id="图形准备"><a href="#图形准备" class="headerlink" title="图形准备"></a>图形准备</h3><p>​            我们按照Beck在2018年写的similiar程序将每个图转换成一个未标记的连通二分图。在这个过程中，每个被标记的边被两个顶点代替：一个代表正向关系，另一个代表正向关系，另一个代表反向关系。为了保持前一条边的方向性，这些新的顶点接着被连接到实体顶点。将原始的知识图谱重组成未标记的有向图，其中所有的顶点都对应SciIE注释中的实体和关系，并且没有损失任何信息。为了提升图中非连通部分之间的信息流动，我们增加了一个可以连接实体顶点的全局顶点。这个全局顶点将会被用于初始化解码器，与传统序列到序列模型之间最终的编码隐藏层类似。这些重组操作的最终结果是一个连通的未标记图G=(V,E),V是实体，关系以及一个全局节点的列表。边是一个描述有向边的邻接矩阵。</p>
<blockquote>
<p><strong>Graph Transformer</strong> </p>
<p>Our model is most similar to the Graph Attention Network (GAT) of Velickovic et al. (2018), which computes the hidden representations of each node in a graph by attending over its neighbors following a self-attention strategy. The use of self-attention in GAT addresses the shortcomings of prior methods based on graph convolutions (Defferrard et al.2016; Kipf and Welling, 2017), but limits vertex updates to information from adjacent nodes. Our model allows for a more global contextualization of each vertex through the use of a transformerstyle architecture. The recently proposed Transformer (Vaswani et al., 2017) addresses the inherent sequential computation shortcoming of recurrent neural networks, enabling effificient and paralleled computation by invoking a self-attention mechanism for global context modeling. These models have shown promising results in a variety of text processing tasks (Radford et al., 2018).</p>
</blockquote>
<h3 id="图转换器"><a href="#图转换器" class="headerlink" title="图转换器"></a>图转换器</h3><p>​        我们的模型与GAT（图形注意网络）是最相似的，它通过自注意力策略处理邻居顶点来计算图形中每个节点的隐藏表示。在GAT中使用的自注意力机制解决了基于图卷积的现有方法的缺点，但是限制顶点从相邻顶点更新信息。我们的模型允许通过使用transformer类型的结构为每一个顶点进行更全局的上下文化。最近提出的Transformer模型中RNN固有的序列计算的缺点。通过为全局文本模型调用自注意力机制启用高效并行的计算。这些模型在多种文本处理任务中展现出了预期的结果。</p>
<blockquote>
<p>​        Our Graph Transformer encoder starts with self-attention of local neighborhoods of vertices; the key difference with GAT is that our model includes additional mechanisms for capturing global context. This additional modeling power allows the Graph Transformer to better articulate how a vertex should be updated given the content of its neighbors, as well as to learn global patterns of graph structure relevant to the model’s objective.</p>
<p>​        Specififically, <em>V</em> is embedded in a dense continuous space by the embedding process described at the end of this section, resulting in matrix <strong>V</strong>0 = [<strong>v*</strong>i<em>]</em>,* <strong>v*</strong>i* <em>∈</em> R<em>d</em> which will serve as input to the graph transformer model shown in Figure 4. Each vertex representation <strong>v*</strong>i* is contextualized by attending over the other vertices to which <em>v**i</em> is connected in <em>G</em>. We use an <em>N</em>-headed self attention setup, where <em>N</em> independent attentions are calculated and concatenated before a residual connection is applied:</p>
</blockquote>
<p>​        我们的图形Transformer编码器编码以顶点的本地令居的自注意力机制为起点，与GAT关键的差异就在于我们的模型包含额外捕捉全局上下文的机制。这个另外的使得图形Transformer能够在考虑邻居节点的前提下如何更新阐明一个顶点，同样也可以学习与模型目标相关的图形结构的全局模式。</p>
<p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20200825093359698.png" alt=""></p>
<p>​        特别地，V就像在这个部分最后描述的嵌入过程中那样被嵌入在一个密集的连续空间中，导致生成矩阵V=[vi],vi∈Rd，矩阵作为如图4所示的图形tranformer模型中的输入。每一个顶点的表示vi都是通过处理与vi在图G中相连的其他所有顶点来进行上下文化。我们使用一个N头的自注意力机制，N头独立的注意力机制，这个N头自注意力机制中在被计算和连接之前需要处理剩余的连接：</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200825094135094.png" alt=""></p>
<blockquote>
<p>Here, || denotes the concatenation of the <em>N</em> attention heads, <em>N**i</em> denotes the neighborhood of <em>v**i</em> in <em>G</em>, <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200825094704284.png" alt=""> , and where <em>a**n</em> are attention mechanisms parameterized per head. In this work,we use attention functions of the following form:</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200825094726513.png" alt=""></p>
<p>Each a learns independent transformations<strong>W***</strong>Q<strong>,</strong>W<strong><em>K</em> <em>∈</em> R*d</strong>×<strong>d* of **q</strong> and <strong>k</strong> respectively, andthe resulting product is normalized across all connected edges. To reduce the tendency of these dot products to impede gradient flflow, we scale them by <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200825095341227.png" alt="">, following Vaswani et al. (2017).The Graph Transformer then augments these multi-headed attention layers with <em>block</em> networks.Each block applies the following transformations:</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200825094942641.png" alt=""></p>
<p>Where FFN(<strong>x</strong>) is a two layer feedforward network with a non-linear transformation <em>f</em> between layersi.e. <em>f</em>(<strong>xW</strong>1 + <em>b</em>1)<strong>W</strong>2 + <em>b</em>2.</p>
</blockquote>
<p>​            在这，||表示N注意力头部的连接，Ni表示在图G中的vi的邻居顶点，<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200825103417862.png" alt="">)，并且a在每个头部注意力机制被参数化。在这个工作中，我们使用下面公式的注意力结构：<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200825103734156.png" alt=""></p>
<p>每个a分别学习q和k的独立变换Wq，Wk∈R，并在所有连接的边上对所得乘积进行归一化。为了减少这些点积阻碍梯度流的趋势，我们按照1/根号d缩放。</p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200826180839842.png" alt=""></p>
<p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200826181025088.png" alt=""></p>
<p><img src="https://img-blog.csdnimg.cn/20200209221457785.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0FSUE9TUEY=,size_16,color_FFFFFF,t_70" alt=""></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>我们研究了从自动信息提取系统的输出中生成多句文本的问题，并表明将知识作为图形进行合并可以提高性能。 我们介绍了GraphWriter，它具有用于图形编码的新注意力模型，并通过与强基准相比的人工和自动评估证明了其实用性。 最后，我们为生成社区提供了一个新资源，即摘要和知识的AGENDA数据集。 未来的工作可能会解决所生成文本中重复和实体覆盖的问题。</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/ARPOSPF/article/details/104241533" target="_blank" rel="noopener">https://blog.csdn.net/ARPOSPF/article/details/104241533</a></p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script>
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        Author:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://weiruohe.github.io" rel="external nofollow noreferrer">Weiruohe</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        Link:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://weiruohe.github.io/2020/08/15/text-knowledge-gragh-transformers/">https://weiruohe.github.io/2020/08/15/text-knowledge-gragh-transformers/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        Reprint policy:
                    </i>
                </span>
                <span class="reprint-info">
                    All articles in this blog are used except for special statements
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    reprint polocy. If reproduced, please indicate source
                    <a href="https://weiruohe.github.io" target="_blank">Weiruohe</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>Copied successfully, please follow the reprint policy of this article</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">more</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/gragh-transformer/">
                                    <span class="chip bg-color">gragh transformer</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="qq,qzone,wechat,weibo,douban" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;Previous</div>
            <div class="card">
                <a href="/2020/08/20/wuenda-s-class-2/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/23.jpg" class="responsive-img" alt="Wuenda&#39;s class(2)">
                        
                        <span class="card-title">Wuenda&#39;s class(2)</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            p1.Train/dev/test sets
Train：训练集，用来训练各种模型
dev：验证集(development set)/Hold-out cross validation set，评估这些模型，通过迭代选出最优模型
test：
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2020-08-20
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/AI/" class="post-category">
                                    AI
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/deep-learning/">
                        <span class="chip bg-color">deep-learning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                Next&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2020/08/12/word-representation-sememes/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/15.jpg" class="responsive-img" alt="Word Representation-Sememes">
                        
                        <span class="card-title">Word Representation-Sememes</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            Abstract
Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed b
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2020-08-12
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            Weiruohe
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Word-Representation/">
                        <span class="chip bg-color">Word Representation</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


<!-- 代码块折行 -->

<style type="text/css">
code[class*="language-"], pre[class*="language-"] { white-space: pre !important; }
</style>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;TOC</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




   

    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;Search</span>
            <input type="search" id="searchInput" name="s" placeholder="Please enter a search keyword"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
$(function () {
    searchFunc("/search.xml", 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script type="text/javascript" src="/js/CFS.Snow.min.js"></script>
    <!-- 点击爆灯效果 -->
    <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
    <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
    <script type="text/javascript" src="/js/fireworks.js"></script>
    <!--动态线条背景-->
    <script type="text/javascript"
        color="122 103 238" opacity='0.7' zIndex="-2" count="200" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
    </script>
    <!-- 天气 -->
    <!-- weather -->
    <!-- weather -->
    <script type="text/javascript">
         WIDGET = {FID: 'knAMQaFanP'}
    </script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script type="text/javascript" src="https://apip.weatherdt.com/float/static/js/r.js?v=1111"></script>
    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
