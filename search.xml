<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Semantic Parsing</title>
      <link href="/2020/12/08/semantic-parsing/"/>
      <url>/2020/12/08/semantic-parsing/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>How Far are We from Effective Context Modeling? An Exploratory Study on Semantic Parsing in Context</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Recently semantic parsing in context has received considerable attention, which is challenging since there are complex contextual phenomena. Previous works verifified their proposed methods in limited scenarios, which motivates us to conduct an exploratory study on context modeling methods under real-world semantic parsing in context. We present a grammar-based decoding semantic parser and adapt typical context modeling methods on top of it. We evaluate 13 context modeling methods on two large complex cross-domain datasets, and our best model achieves state-of-the-art performances on both datasets with signifificant improvements.Furthermore, we summarize the most frequent contextual phenomena, with a fifine-grained analysis on representative models, which may shed light on potential research directions. Our code is available at <a href="https://github.com/microsoft/ContextualSP" target="_blank" rel="noopener">https://github.com/microsoft/ContextualSP</a>.</p></blockquote><p>上下文语义解析在复杂语境环境中具有挑战性。我们提出了一种基于语法的解码语义解析器，并在此基础上提出上下文建模方法。同时对有代表性的模型进行细粒度分析，探索潜在的研究方向。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><img src="Semantic-Parsing.assets/image-20201209092824720.png" style="zoom:80%;"><p>语义解析是将自然语言的句子转化为对应的可执行逻辑形式（eg:SQL),这样用户就不用学习逻辑形式下背后的技术。先前的</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> SQC(语境语义解析) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>compositional Generalization</title>
      <link href="/2020/12/04/compositional-generalization/"/>
      <url>/2020/12/04/compositional-generalization/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>Compositional Generalization by Learning Analytical Expressions</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Compositional generalization is a basic and essential intellective capability of human beings, which allows us to recombine known parts readily. However,existing neural network based models have been proven to be extremely defificient in such a capability. Inspired by work in cognition which argues compositionality can be captured by variable slots with symbolic functions, we present a refreshing view that connects a memory-augmented neural model with analytical expressions, to achieve compositional generalization. Our model consists of two cooperative neural modules, Composer and Solver, fifitting well with the cognitive argument while being able to be trained in an end-to-end manner via a hierarchical reinforcement learning algorithm. Experiments on the well-known benchmark SCAN demonstrate that our model seizes a great ability of compositional generalization, solving all challenges addressed by previous works with 100% accuracies.</p></blockquote><p>人类具有组合泛化的能力，即将已知部分进行重组，但是NN缺乏这种能力。本文发现组合性可以被带有符号函数的变量槽进行捕捉，所以将记忆增强的神经模型与解析表达式连接起来实现组合泛化。模型有两个模块组成Composer、Solver，通过分层强化学习以端到端的方式进行训练。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>如果一个人类学习了”walk”,”jump”,”walk twice”,那么”jump twice”的意思肯定能懂，这依赖于语言的特性——组合性，即复杂表达的含义取决于组成部分的含义以及他们的组合方式。</p><p>Lake and Baroni认为组合泛化对于帮助NN理解域外知识是非常重要的能力，他们使用SCAN数据集（简化版本<strong>C</strong>omm<strong>A</strong>I <strong>N</strong>avigation）进行测试，发现在MT上效果并不好，没有任何基于神经的模型能够在没有额外资源的情况下成功地解决SCAN的所有组成挑战。</p><p>本文提出的模块有Composer，Solve。Composer将非结构化表达转化为结构化表达，Solver通过访问Memery来理解表达式。</p><h3 id="Compositional-Generalization-Assessment"><a href="#Compositional-Generalization-Assessment" class="headerlink" title="Compositional Generalization Assessment"></a>Compositional Generalization Assessment</h3><p>对于组合泛化的学习仍在初期，前人一般使用人工数据集进行评估，本文使用系统性和生产力来进行评估</p><p>系统性：对已知部分进行重组，三个任务：</p><ol><li><em>Add Jump</em>.针对<strong>JUMP</strong>进行划分训练集，测试集。包含”jump”但不全是”jump”的构成测试集，其余构成训练集</li><li><em>Around Right</em>移除训练集中包含”around right”的组成命令。测试从”left”到”right”的泛化能力。</li><li><em>Length</em>输出长度超过24的命令测试中没有出现</li></ol><p>生产力：不仅能重组已知的部分，但也要评估他们是否能有效泛化到超过训练长度的输入，语言允许一组理论上无限可能的句子。</p><p><img src="compositional-Generalization.assets/image-20201208155215342.png" alt="image-20201208155215342"></p><p>Composer将源表达式中可识别的表达式用变量代替，通过Solver进行语义分析得到目标表达式，然后提取Memory中存储的信息替换变量获得常量目标表达式，最后使用得到的结果对Memery进行更新，循环操作直到翻译完成。</p><img src="compositional-Generalization.assets/image-20201208155918942.png" alt="image-20201208155918942" style="zoom: 67%;"><p>合并过程是通过首先枚举当前层的所有可能的父节点，然后选择合并得分最高的节点来实现的。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 组合泛化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>魏若荷_studynote_20201120-20201203</title>
      <link href="/2020/12/03/wei-ruo-he-studynote-20201120-20201203/"/>
      <url>/2020/12/03/wei-ruo-he-studynote-20201120-20201203/</url>
      
        <content type="html"><![CDATA[<h4 id="一、学习内容"><a href="#一、学习内容" class="headerlink" title="一、学习内容"></a>一、学习内容</h4><ol><li><p>研读nlp领域有关KBQA的最新论文</p><p>《<strong>Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering</strong>》</p><blockquote><p>现有的利用知识图谱增强问答模型的工作要么难以有效地建立多跳关系，要么在模型的预测原理上缺乏透明度，本文提出了一种多跳图关系网络，为预训练语言模型配备了多跳关系推理模块，统一了基于路径的推理方法和图神经网络的方法，它可以从知识图谱提取的子图执行多跳，多关系推理。</p></blockquote><p>《<strong>Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases</strong>》</p><blockquote><p>关于知识图谱问答的三种泛化，现有的知识图谱问答一般只能解决i.i.d，但是这篇论文建立了三个层次的内置泛化：i.i.d,compositional,zero-shot,为了增大泛化能力提出GRAILQA数据集以及基于BERT的KBQA模型。</p></blockquote></li><li><p>阅读白硕老师的“闲话语义（9），对“属性”进行更深一步的理解</p></li><li><p>对《<strong>Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering</strong>》进行代码复现。</p></li><li><p>与牛润良师哥讨论常识图谱的下游任务，可能是KBQA or Commonsense Reasoning,准备ppt向白老师进行汇报。</p></li></ol><h4 id="二、下阶段计划"><a href="#二、下阶段计划" class="headerlink" title="二、下阶段计划"></a>二、下阶段计划</h4><ol><li>针对白老师所说的下游任务继续推进常识图谱。</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>QA_KA</title>
      <link href="/2020/11/27/qa-graph/"/>
      <url>/2020/11/27/qa-graph/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> QA </tag>
            
            <tag> NLP </tag>
            
            <tag> KB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>QA_KA</title>
      <link href="/2020/11/27/qa-ka/"/>
      <url>/2020/11/27/qa-ka/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations effificiently,or lack transparency into the model’s prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) with a <em>multi-hop relational reasoning module</em>, named multi-hop graph relation network (MHGRN).It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unififies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets,and interpret its behaviors with case studies1.</p></blockquote><p>现在使用外部知识增强QA的模型努力在对多跳关系进行建模，或者缺乏对模型预测的透明度，我们提出一种有预训练语言模型的知识感知方法。实现了从外部知识图谱提取的子图上进行多跳，多关系的推理。将<strong>基于路径的推理方法和图神经网络</strong>结合起来实现了更好的可解释性，可扩展性。</p><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h3><p>QA问题不仅要对问题和文本进行机器理解，也需要通过外部知识对实体和关系进行关系推理，问题的背景知识没有包含在问题中，但是对于人类却是常识。</p><img src="QA-Graph.assets/image-20201128212656652.png" style="zoom: 67%;"><p>为了利用知识图谱，KG表示了有模型所需的多关系边KGS表示具有多关系边的实体之间的关系知识，供模型获取。the relational path CHILD -&gt;AtLocation n CLASSROOM-&gt;Synonym-&gt; SCHOOLROOM,naturally provides evidence for the answer SCHOOLROOM.</p><p>简单利用知识图谱的方法是建模他们的关系，通过从KB中提取关系路径建立多跳关系，但是这个方法很难升级，因为有容量限制。他们只能考虑一跳来平衡可测量性和容量。</p><img src="QA-Graph.assets/image-20201128213754420.png" style="zoom: 80%;"><p>GNN通过<strong>message passing formulation</strong>信息传送模式来实现可测量，但是缺少透明性。通过聚合邻居节点的信息来进行信息传递但是忽略了关系类型。后来通过特定关系聚合使它可以应用在多关系图中，但是对于不同的邻居节点和关系类型的重要性没有区分，所以对模型行为解释无法提供显示的关系路径。</p><p>我们提出的新模型<strong>MHGRN</strong>(Multi-hop Graph Relation Network)结合了基于路径模型和GNN的优点,保留<strong>message passing formulation,</strong></p><p>提出了<strong>structured relational attention machanism.</strong>主要创新点是multi-hop message.在一个层里允许每个节点直接处理多跳邻居节点,允许多跳关系推理.</p><p>将知识感知的QA有利特征和<strong>MHGRN</strong>进行比较：</p><img src="QA-KA.assets/image-20201130085219002.png" style="zoom:80%;"><h3 id="2-Problem-Formulation-and-Overview"><a href="#2-Problem-Formulation-and-Overview" class="headerlink" title="2 Problem Formulation and Overview"></a>2 Problem Formulation and Overview</h3><p>本文将研究目标限制在<strong>多项式选择QA</strong>问题上，通过KB给出的外部知识和问题q，来选择出集合C中给定的选项。通过对问题和各个选项进行分数计算来选出最有可能的选项。</p><ol><li>将问题和选项合并成向量s = [q:a]，q是question，a是answer</li><li>从外部知识图谱中提取子图G =（<em>V</em>,<em>E</em>,<em>φ</em>)，V是子图中所有与q有关的实体，<em>E</em> ⊆ <em>V</em> × <em>R</em> × <em>V</em>，R是所有预训练中出现的关系类型，映射函数<em>φ</em></li><li>V→T = {Eq,Ea,Eo},将V中的节点i作为输入，Eq：i是问题中提到的实体，Ea：i是选项中提到的实体，Eo表示i是其他实体。</li><li>将statement(q,a)编码成s,图G编码成g，将s和g集中起来计算可能性得分</li></ol><img src="QA-KA.assets/image-20201130094721848.png" style="zoom: 67%;"><h3 id="3-Background-Multi-Relational-Graph-Encoding-Methods"><a href="#3-Background-Multi-Relational-Graph-Encoding-Methods" class="headerlink" title="3 Background: Multi-Relational Graph Encoding Methods"></a>3 <strong>Background: Multi-Relational Graph Encoding Methods</strong></h3><p>对多关系图谱的编码方法有两种策略：<strong>GNN</strong>,path-based models.<strong>GNN</strong>通过在节点之间传递信息来结构化信息，首先将图分解分解成paths，再在paths上加入特征。</p><h4 id="Graph-Encoding-with-GNNs"><a href="#Graph-Encoding-with-GNNs" class="headerlink" title="Graph Encoding with GNNs"></a>Graph Encoding with GNNs</h4><p>将节点特征作为输入，通过信息传递计算他们的相关节点embedding，然后通过池化节点embedding获得紧凑图形表示。</p><p><strong>GNN</strong>的变体：</p><ol><li>通过整合来自直接领居的信息更新node embedding</li><li>通过对每一个边类型定义指定关系的权重矩阵来编码多关系图谱。</li></ol><p>但是GNN只实现了node-level的可伸缩性，在路径级别</p><h4 id="Graph-Encoding-with-Path-Based-Models"><a href="#Graph-Encoding-with-Path-Based-Models" class="headerlink" title="Graph Encoding with Path-Based Models"></a>Graph Encoding with Path-Based Models</h4><p>除了用GNN对图进行建模，将图视为连接实体对的关系路径集合。<strong>RNs</strong>（Relationship Networks）可以适用于QA设置下的多关系图型编码。RN使用MLPs对G中的三元组(One-hop paths，问题指向选项)进行编码，</p><p><img src="QA-KA.assets/image-20201130110340246.png" alt=""></p><p>为了进一步让RN有能力去建模非退化路径。KagNet采用LSTM对所有连接问题和答案实体路径&lt;=4的所有路径进行编码。</p><h3 id="4-Model-Architecture"><a href="#4-Model-Architecture" class="headerlink" title="4.Model  Architecture"></a>4.Model  Architecture</h3><img src="QA-KA.assets/image-20201130100633809.png" style="zoom:80%;"><p>[^learning rate]: <strong>学习率(Learning rate)</strong>作为监督学习以及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值。合适的学习率能够使目标函数在合适的时间内收敛到局部最小值。<br>[^early-stop]: 防止过拟合。将原始的训练数据集划分成训练集和验证集 ，只在训练集上进行训练，并每隔一个周期计算模型在验证集上的误差 3，当模型在验证集上（权重的更新低于某个阈值；预测的错误率低于某个阈值；达到一定的迭代次数），则停止训练。</p><p>什么是节点特征<br>子图匹配怎么匹配<br>statemateQA图是怎么编码<br>text embedding怎么得到的<br>score怎么计算<br>多跳的消息怎么传递</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://baike.baidu.com/item/%E8%8C%83%E5%BC%8F/22773?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/%E8%8C%83%E5%BC%8F/22773?fr=aladdin</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> QA </tag>
            
            <tag> NLP </tag>
            
            <tag> KB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>QA_KB</title>
      <link href="/2020/11/23/qa-kb/"/>
      <url>/2020/11/23/qa-kb/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>现在对KBQA的研究都是假设在问题上的训练分布和测试分布是相同的（i.i.d)。但是在大规模数据集上很难实现：</p><ol><li>真实的用户分布何难捕捉（覆盖面很大，组合也很多）</li></ol><p>[^]: 即使迭代注释了所有的用户问题，没有泛化能力的模型在面对新的out-of distribution questions时依然会失败</p><ol><li>从大空间中随机抽样训练样本效率低下</li></ol><p>所以我们建立三个层次的内置泛化：<strong>i.i.d,compositional,zero-shot</strong>,为了增大泛化能力提出了<strong>GRAILQA</strong>数据集以及基于BERT的KBQA模型</p><p>[^i.i.d]: indenpendently and identically distributed独立同分布（有相同的概率分布且相互独立。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote><p>As the scale and coverage of KBs increase,KBQA is becoming even more important due to the increasing difficulty of writing structured queries like <strong>SPARQL</strong></p></blockquote><p>随着KB规模和覆盖范围的增大，编写像<strong>SPARQL</strong>这样的结构化查询语句变得很难，所以<strong>KBQA</strong>越来越重要 。</p><p><img src="QA-KB.assets/image-20201123095801291.png" alt=""></p><p><strong>KBQA</strong>模型有三个层次的泛化：<strong>i.i.d,compositional,zero-shot</strong>,所以KBQA可以在可见的模式项(relations,classes,function)中进行重新组合,能回答所有涉及模式项的问题，即使在训练中不包含这种特定组合。KBQA在遇到训练中完全没有的关于整个模式甚至整个作用域的问题时，需要在zero-shot进行泛化</p><p>一个理想的数据集应该是大规模，多样化，可以为KBQA捕捉其他的实际挑战，比如实体链接，复杂问题以及语言变异，但是现在的KBQA数据库通常被限制在1 or more维。大多数专注于i.i.d的设置，<strong>GRAPHQ,QALD</strong>可用来测试成分泛化但却不能测试zero-shot泛化,它们的规模都相对较小，对KB本体只有有限的覆盖。<strong>SIMPLEQ</strong>虽然规模很大，但是仅仅限于有限多样性的单关系问题。</p><p>[^language viriation]: 语言变异是指某一个语言现象在实际使用中的话语中的变化。语言现象可以是语音、音位、词汇、语义项目或语法范畴等。例如第二人称单数，在普通话里有“你”（ni）和“您”（nin）两种形式，这两种不同的变化，就是第二人称单数的变异。 –摘自《什么是社会语言学》游汝杰著，上海外语教育出版社，39页</p><p><strong>GRAILQA</strong>支持三种层次的泛化，64331个中包括&gt;=4个关系（<strong>counting,comparatives,superlatives</strong>)的众包问题,数据库覆盖了<strong>FREEBASE COMMONS</strong>的86个领域，我们数据集中的问题涉及的实体是从美国到酒店名的所有受欢迎的实体。</p><p>为了增加多样性和真实性，通过大规模的web mining和众包来收集实体的常见表现形式。</p><blockquote><p><strong>*”Obama”,”President Obama”<em>for *Barack_Obama</em></strong></p></blockquote><p>还提出了一种基于BERT的新KBQA模型，数据库和模型的结合让我们能够测试KBQA的几种挑战<strong>search space pruning and language,ontology alignment</strong>。BERT在实现组合和零射泛化上有很大的作用。</p><h3 id="BACKGROUND"><a href="#BACKGROUND" class="headerlink" title="BACKGROUND"></a>BACKGROUND</h3><p><img src="QA-KB.assets/image-20201124143852828.png" alt=""></p><p>a)从给定的KB到具体的复杂性生成典型的逻辑结构</p><p>b)专家为了典型逻辑形式注释权威问题，实体和文字包含在括号里以便简单替换</p><p>c)通过众包获取高质量以及有差异的注释</p><p>d)从每个由entity grounding构成的逻辑形式生成不同的逻辑形式(?)</p><p>e)采样不同的逻辑形式和注释的组合去生成最终的问题，还可以通过网络挖掘常见的实体表现形式，使用它们使得实体连接更加有真实性</p><h4 id="2-1KB"><a href="#2-1KB" class="headerlink" title="2.1KB"></a>2.1KB</h4><p><img src="QA-KB.assets/image-20201124144742690.png" alt=""></p><p>KB包含两部分：</p><ol><li><strong>ontology</strong>:O ⊆ C × R × C</li><li><strong>relational fact</strong>:M ⊆ E × R × (C ∪ E ∪ L)</li></ol><p>[^C:class R:binary relation E:entity L:litral]: </p><h4 id="2-2Three-Levels-of-Generalization-Definition"><a href="#2-2Three-Levels-of-Generalization-Definition" class="headerlink" title="2.2Three Levels of Generalization:Definition"></a>2.2Three Levels of Generalization:Definition</h4><h3 id="3-Data"><a href="#3-Data" class="headerlink" title="3 Data"></a>3 Data</h3><h4 id="3-1Data-collection"><a href="#3-1Data-collection" class="headerlink" title="3.1Data collection"></a>3.1Data collection</h4><p>3步</p><p>1) generate logical forms from a KB</p><p>2) convert logical forms into canonical questions</p><p>3) paraphrase canonical questions into more natural forms via crowdsourcing</p><h5 id="Canonical-logical-form-generation"><a href="#Canonical-logical-form-generation" class="headerlink" title="Canonical logical form generation"></a>Canonical logical form generation</h5><p>首先遍历KB本体生成仅包含class，relation,function的图形形状的模板，接着落地到兼容实体的节点上，去生成表示含义的逻辑形式，样板和实体生成了canonical logical form。</p><h5 id="Canonical-question-annotation"><a href="#Canonical-question-annotation" class="headerlink" title="Canonical question annotation"></a>Canonical question annotation</h5><h5 id="Crowd-powered-paraphrasing"><a href="#Crowd-powered-paraphrasing" class="headerlink" title="Crowd-powered paraphrasing"></a>Crowd-powered paraphrasing</h5><h5 id="Grouding-and-sampling"><a href="#Grouding-and-sampling" class="headerlink" title="Grouding and sampling"></a>Grouding and sampling</h5><p>标准逻辑形式和兼容的实体生成更多的逻辑形式。同一标准逻辑形式相关的逻辑形式和注释会形成一个pool，每次从每个pool中采样生成一个问题。开始时权值统一，选择logical form or paraphrase时同时除 ρ1，ρp。ρ1 to 2，ρp to 10（diversity），最后随机替换entity form(task3).</p><h4 id="3-2-Data-Analyse"><a href="#3-2-Data-Analyse" class="headerlink" title="3.2 Data Analyse"></a>3.2 Data Analyse</h4><p>Entity linking</p><p>表面形式的基本类型有首字母缩写，姓/名，常识，口语vs书面语，这样挖掘的表面形式用更有代表性，口语化的方法表示KB中的实体，在现有的KBQA数据库中被忽略。</p><h4 id="3-3Logical-Form-in-S-expression"><a href="#3-3Logical-Form-in-S-expression" class="headerlink" title="3.3Logical Form in S-expression"></a>3.3Logical Form in S-expression</h4><p><strong>graph query to S-expression</strong></p><p>更有可读性 </p><h3 id="4-MODEL"><a href="#4-MODEL" class="headerlink" title="4 MODEL"></a>4 MODEL</h3><p>比较和zero-shot泛化相比于i.i.d有两个挑战：<strong>大搜索空间，语言-本体对齐</strong>。</p><ol><li><p>search space:</p><p>i.i.d:ontology observed during training(词汇表主要来自训练数据)</p><p>zero-shot:entire ontology(<em>Qusetion-specific search space pruning</em>)</p></li><li><p>language-ontology alignment:</p><p>zero-shot:学习unseen-schema item为了解决可能出现的相关问题</p><p>组合泛化：生成新的组合而非在训练中记忆的，传统方法是利用数据挖掘建立自然语言到KB语义项的词典，可能会更偏向受欢迎的语义项 </p></li></ol><p>预训练的文本嵌入（BERT）为对齐提供可选择的方案，基于BERT的KBQA模型让我们能测量在非i.i.d泛化中文本嵌入的有效性。</p><p><img src="QA-KB.assets/image-20201125100451071.png" alt=""></p><h4 id="4-1-Model-Overview"><a href="#4-1-Model-Overview" class="headerlink" title="4.1 Model Overview"></a>4.1 Model Overview</h4><p>目标是生成map:input question q=x1,x2,…,x|q| to logical form a=y1,y2,..ya</p><p>基于Seq2Seq2，输出token是在输入的条件下自回归生成的，V是解码词汇表。条件概率定义为：<img src="QA-KB.assets/image-20201125104930255.png" style="zoom:67%;"></p><p>编码和解码器是两个不同的有LSTM的RNN，LSTM根据下面的公式生成token<br>$$<br>h𝑡 = 𝐿𝑆𝑇𝑀(h𝑡-1, g𝑡),<br>$$<br>基于当前的ht和W计算的每个输出token的概率<br>$$<br>𝑝(𝑦𝑡 |𝑦&lt;𝑡<br>, 𝑞) = [𝑆𝑜 𝑓 𝑡𝑚𝑎𝑥 (Wh𝑡)]𝑦𝑡 .<br>$$</p><p>我们使用Seq2seq中的W连接输入输出，从预训练的嵌入中分配语义，促进开放的词汇学习</p><h4 id="4-2BERT-Encoding"><a href="#4-2BERT-Encoding" class="headerlink" title="4.2BERT Encoding"></a>4.2BERT Encoding</h4><p>之前用文本表示学习text-to-SQL的对齐时，将question和所有vocabulary item连接起来作为BERT的输入，但是KBQA中的词汇太多了，超过的BERT的输入长度，vocabulary分成chunk，将chunk和question连接后进行处理。</p><blockquote><p><img src="QA-KB.assets/image-20201125162825742.png" alt="?"></p></blockquote><h4 id="Vocabulary-construction"><a href="#Vocabulary-construction" class="headerlink" title="Vocabulary construction"></a>Vocabulary construction</h4><p>在处理zero-shot泛化时，可以将来自KB本体的所有模式项都包括进来。但是搜索空间太大并且很耗时.为了减少size of V，利用从question中识别的实体作为在KB中的anchor，并且仅选择距离（&gt;=1个）anchor两跳之内的KB items。（<strong>Vocabulary pruning</strong>）我们对每个输入都有一个动态V，所有在S-expression中的被识别实体，函数以及语法常量也包含在其中，同理求出基于BERT的实体表示，表示与它的表面形式相关。</p><h3 id="5-Experiment"><a href="#5-Experiment" class="headerlink" title="5.Experiment"></a>5.Experiment</h3><p><img src="QA-KB.assets/image-20201126111238105.png" alt=""></p><blockquote><ol><li><p>正确率 = 提取出的正确信息条数 /  提取出的信息条数   </p></li><li><p>召回率 = 提取出的正确信息条数 /  样本中的信息条数 </p><p><strong>F-Measure</strong>（综合评价指标，α=1时即为F-1）：<br><img src="QA-KB.assets/20130808110619562" alt=""></p></li></ol></blockquote><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a href="https://baike.baidu.com/item/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/1037587?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B/1037587?fr=aladdin</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> QA </tag>
            
            <tag> NLP </tag>
            
            <tag> KB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>studynote-20201106-20201119</title>
      <link href="/2020/11/19/studynote-20201106-20201119/"/>
      <url>/2020/11/19/studynote-20201106-20201119/</url>
      
        <content type="html"><![CDATA[<h4 id="一、学习内容"><a href="#一、学习内容" class="headerlink" title="一、学习内容"></a>一、学习内容</h4><ol><li><p>研读nlp领域的经典论文</p><p>《<strong>LXMERT: Learning Cross-Modality Encoder Representations from Transformers</strong>》</p><p>《<strong>Improving Language Understanding by Generative Pre-Training</strong>》</p><blockquote><p><strong>model</strong>:Transformer</p><p><strong>data set</strong>:text with long dependency</p><p><strong>target task</strong>:QA,SSA(semantic similarity assessment),ED(entailment Denpendency),TC(text classification)</p></blockquote><p>实现了很强的自然语言理解能力，使用Transformer对长连续文本的不同数据集进行无监督预训练，再在目标task上进行有监督微调，提高处理long-dependency文本的能力.</p></li><li><p>阅读白硕老师的“闲话语义（8），对“模态”进行更深一步的理解</p></li><li><p>使用stanfordnlp中的corenlp模型实现英文的NER</p></li><li><p>与牛润良师哥对HowNet中的动态角色进行初分类</p></li></ol><h4 id="二、困难"><a href="#二、困难" class="headerlink" title="二、困难"></a>二、困难</h4><ol><li>现有的哈工大，斯坦福的中文NER工具最多只能分为三类（Time，Organization，Person）要实现更细粒度的分类需要借助别的方法</li><li>对动态角色进行分类有需要进一步思考和考虑的地方</li></ol><h4 id="三、下阶段计划"><a href="#三、下阶段计划" class="headerlink" title="三、下阶段计划"></a>三、下阶段计划</h4><ol><li>继续研读经典论文</li><li>动态角色分类问题继续进行和完善</li><li>尝试实现中文的NER细分</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LXMERT</title>
      <link href="/2020/11/18/lxmert/"/>
      <url>/2020/11/18/lxmert/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p><strong>LXMERT: Learning Cross-Modality Encoder Representations from Transformers</strong></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections.</p></blockquote><p>为了学习图像-语言推理，除了学习视觉概念和语义之外，两种模态间的一致性和联系是最重要的。</p><p>LXMERT在学习这些联系。</p><p><strong>Transformer</strong>模型包含三个编码层：<strong>object relationship encoder（处理图像）,language encoder（文字）,cross-modality encoder（交叉模态）</strong></p><p><strong>pre-training</strong>:image-sentence pairs,</p><p><strong>task</strong>:masked language modeling, masked object prediction(feature regression and label classifification),cross-modality matching, and image question answering.</p><p>good result on two dataset<strong>:VQA,GQA</strong></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote><p>Despite these inflfluential single modality works, large-scale pretraining and fifine tuning studies for the modality-pair of vision and language are still under-developed.</p></blockquote><p>这个框架是将BERT模型应用于跨模态场景，主要关注单个图像以及描述性句子</p><p>在<strong>Abstract</strong>中提到的五种预训练任务中，与单模态预训练模型不同，交叉模态预训练通过同模态的可见组件或者不同模态的对齐组件推断masked features.最终建立<strong>intra-modality,cross-modality</strong> relationships.</p><h4 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h4><p>首先在VQA,GQA上进行评估，整体精度上达到了最先进的程度。</p><p>为证明通用性，在NLVR^2上使用真实世界的图片进行微调，评估（不使用数据集中的自然图片），最近进行<strong>several analysis</strong>,<strong>ablation stadies</strong>证明模型以及预训练任务的有效性</p><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p><img src="LXMERT.assets/image-20201120183329915.png" alt=""></p><p>模型输入是<strong>concept-level image embedding</strong>,以及<strong>word-level sentence embedding</strong>.</p><h4 id="input-embedding"><a href="#input-embedding" class="headerlink" title="input embedding"></a>input embedding</h4><h5 id="Word-level-Sentence-Embedding"><a href="#Word-level-Sentence-Embedding" class="headerlink" title="Word-level Sentence Embedding"></a>Word-level Sentence Embedding</h5><img src="LXMERT.assets/image-20201122094839310.png" style="zoom: 67%;"><p>对于sentence来说，首先将句子分成单词，再加入每个word在句子中的绝对位置，最后投射到向量里，最后添加到索引感知的单词嵌入中</p><hr><h5 id="Object-Level-Image-Embedding"><a href="#Object-Level-Image-Embedding" class="headerlink" title="Object-Level Image Embedding"></a>Object-Level Image Embedding</h5><p>不同于以往使用<strong>feature map或者直接使用RoI特征</strong>，使用了bounding box coordinates和2048维的ROI特征，加入了位置信息（position info），因为在image embedding和attention layer中对输入的位置是未知的。</p><img src="LXMERT.assets/image-20201122104141569.png" style="zoom:67%;"><h5 id="Encoder层"><a href="#Encoder层" class="headerlink" title="Encoder层"></a>Encoder层</h5><blockquote><p>Attention是在Query向量相关的上下文向量（context vector)集合中检索info，如果Query向量本身就是context vector,那Attention就是Self-attention。attention层先计算query vector和context vector的匹配分数，被<strong>softmax</strong>统一化</p><p><img src="LXMERT.assets/image-20201122161729951.png" alt=""></p></blockquote><h5 id="Single-Modality-Encoder"><a href="#Single-Modality-Encoder" class="headerlink" title="Single-Modality Encoder"></a>Single-Modality Encoder</h5><p><img src="LXMERT.assets/image-20201122190447018.png" alt=""></p><p>**与BERT不同的是，transformer encoder输入不仅是language，而且是vision.每一个encoder层包括Self,FF.FF又包括两个全连接子层。又加入residual connection,layer normalization.</p><p><img src="LXMERT.assets/image-20201122192601559.png" alt=""></p><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a href="https://blog.csdn.net/qq_31050167/article/details/79161077" target="_blank" rel="noopener">https://blog.csdn.net/qq_31050167/article/details/79161077</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Origin PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Pre-training </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>闲话语义</title>
      <link href="/2020/11/18/xian-hua-yu-yi/"/>
      <url>/2020/11/18/xian-hua-yu-yi/</url>
      
        <content type="html"><![CDATA[<h4 id="闲话语义（7）"><a href="#闲话语义（7）" class="headerlink" title="闲话语义（7）"></a>闲话语义（7）</h4><h5 id="1-相谐"><a href="#1-相谐" class="headerlink" title="1.相谐"></a>1.相谐</h5><p>所有语义单位自带螺母螺钉，螺钉是输出类型，语义单位不是态射，输出类型就是语义单位本身，输入类型只有态射类型有，<strong>带入论元的语义单位中输出必须是输入的本身或下位（相谐性条件）</strong>。</p><blockquote><p>“谋杀”事件有两个必选论元：施事和受事者，说“张三被杀了”，则创造出一个实例，不论受试者是谁，都会有一个类型标签“Human”，</p></blockquote><p>事件对论元类型的约束中类型标签间的标配关系是逻辑“或”，&gt;=1个相谐即为相谐。</p><p>在同一个事件框架中的两个论元也需要相谐。eg：（静态类属判断）“是”，（动态类属判断）“成为”，“比….大”,”超过”</p><h5 id="2-必选论元"><a href="#2-必选论元" class="headerlink" title="2.必选论元"></a>2.必选论元</h5><p>补充必选论元，是事件装配的动力，<strong>光杆</strong>是常态。可选论元通过介词是常态。但是两种论元最终都归于槽位。</p><p><strong>为啥要划分两种论元？</strong></p><ol><li><p>事件和实体：构成或关联两种功能。必选论元体现构成功能，关联使用K-V对（可选论元）</p></li><li><p>使用K-V对表示可选论元，方便同一，而且这种方法和变化谓词词尾，添加助动词的效果统一。</p><blockquote><p>eg：&lt;time, 昨天&gt;，和&lt;tense,<em>Past</em>&gt;，直接加昨天就一定表示过去，不用加“了”</p></blockquote></li></ol><h4 id="4-角色名（？）"><a href="#4-角色名（？）" class="headerlink" title="4.角色名（？）"></a>4.角色名（？）</h4><p>事件“罹患”有两个必选论元</p><p><img src="%E9%97%B2%E8%AF%9D%E8%AF%AD%E4%B9%89.assets/image-20201122090743252.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> conceptual record </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT</title>
      <link href="/2020/11/18/gpt/"/>
      <url>/2020/11/18/gpt/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p><strong>Improving Language Understanding by Generative Pre-Training</strong></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p> Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classifification.Although large unlabeled text corpora are abundant, labeled data for learning these specifific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by <em>generative pre-training</em> of a language model on a diverse corpus of unlabeled text, followed by <em>discriminative fifine-tuning</em> on each specifific task.</p></blockquote><p>We introduced a framework for achieving strong natural language understanding with a single task-agnostic model through generative pre-training and discriminative fifine-tuning.</p><p>解决的问题：用来学习specific task的labled data是稀缺的。让针对性的模型充分执行具有挑战性。</p><p>解决方法：generative pre-training,discriminative fifine-tuning(task-aware input transformations)</p><p>结果：improve：8.9%-CR。5.7%-QA,1.5%-TE</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>为了释放在nlp中对监督学习的依赖，从原始文本中进行有效学习很重要，该模型针对从未标记数据中提取语言信息获取更多的注释提出了一种替代方案。</p><p>从未标记的文本中提取字级别的信息有两个主要的挑战：</p><ol><li>无法确定最有效的优化目标</li><li>最有效的传送方法未知</li></ol><blockquote><p><strong>pre-training</strong>:unsupervised-one corpus of unlabeled text</p><p><strong>fine-tunning</strong>:supervised-several dataset with manually annotated traning examples(<strong>target tasks</strong>)</p></blockquote><p>训练步骤：</p><ol><li>使用基于未标记的数据的模型目标去学习神经网络模型的初始参数</li><li>用相关的监督目标来调整参数适应 <strong>target task</strong></li></ol><p>model architecture:<strong>Transformer</strong></p><blockquote><p>more structured memory:hande <strong>long-dependency</strong> in text,在不同tasks中都有很好的跨越表现,同时将结构化文本处理为单个连续的文本序列,这样能在对预训练模型进行最小改变的前提下进行有效的fine-tunning</p></blockquote><p><img src="Pretraining-improving-understand.assets/image-20201118200948172.png" alt=""></p><p>[^计算相似性将两种顺序都进行计算的原因可能是增加鲁棒性]: </p><p>自然语言理解包括很多任务,比如<strong>textual entailment,QA,SSA(semantic similarity assessment),document classification.</strong>实现了无监督的预处理以及在target tasks上有监督的微调,无监督学习从word-level,phrase-level的embedding发展到现在的sentence-level,为了捕捉更高级别的语义信息.无监督的预处理是无监督学习的特例,他是为了找到更好的初始化点,帮助神经网络在不同任务(图像分类,语音识别,实体消歧以及MT)上进行训练.之前借助LSTM进行文本信息捕捉,将预测范围限制的很短,本文使用Transformer模型改进了预测范围.预训练还将</p><p>红框标注出了Transformer模型的位置,token级的输入与Linear层之间.token级的输入是创新点,它实现了更细粒度的fine-tunning(微调).先使用未标记的数据学习模型的初始参数,然后再监督目标下将这些参数适应于特定的目标.目标任务与未标记数据集不需要在同一区域,跨任务的模型表现都很好.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><blockquote><p><strong>model</strong>:Transformer</p><p><strong>data set</strong>:text with long dependency</p><p><strong>target task</strong>:QA,SSA(semantic similarity assessment),ED(entailment Denpendency),TC(text classification)</p></blockquote><p>实现了很强的自然语言理解能力，使用Transformer对长连续文本的不同数据集进行无监督预训练，再在目标task上进行有监督微调，提高处理long-dependency文本的能力.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Origin PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Pre-training </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT</title>
      <link href="/2020/11/18/pretraining-improving-understand/"/>
      <url>/2020/11/18/pretraining-improving-understand/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p><strong>Improving Language Understanding by Generative Pre-Training</strong></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p> Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classifification.Although large unlabeled text corpora are abundant, labeled data for learning these specifific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by <em>generative pre-training</em> of a language model on a diverse corpus of unlabeled text, followed by <em>discriminative fifine-tuning</em> on each specifific task.</p></blockquote><p>We introduced a framework for achieving strong natural language understanding with a single task-agnostic model through generative pre-training and discriminative fifine-tuning.</p><p>解决的问题：用来学习specific task的labled data是稀缺的。让针对性的模型充分执行具有挑战性。</p><p>解决方法：generative pre-training,discriminative fifine-tuning(task-aware input transformations)</p><p>结果：improve：8.9%-CR。5.7%-QA,1.5%-TE</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>为了释放在nlp中对监督学习的依赖，从原始文本中进行有效学习很重要，该模型针对从未标记数据中提取语言信息获取更多的注释提出了一种替代方案。</p><p>从未标记的文本中提取字级别的信息有两个主要的挑战：</p><ol><li>无法确定最有效的优化目标</li><li>最有效的传送方法未知</li></ol><blockquote><p><strong>pre-training</strong>:unsupervised-one corpus of unlabeled text</p><p><strong>fine-tunning</strong>:supervised-several dataset with manually annotated traning examples(<strong>target tasks</strong>)</p></blockquote><p>训练步骤：</p><ol><li>使用基于未标记的数据的模型目标去学习神经网络模型的初始参数</li><li>用相关的监督目标来调整参数适应 <strong>target task</strong></li></ol><p>model architecture:<strong>Transformer</strong></p><blockquote><p>more structured memory:hande <strong>long-dependency</strong> in text,在不同tasks中都有很好的跨越表现,同时将结构化文本处理为单个连续的文本序列,这样能在对预训练模型进行最小改变的前提下进行有效的fine-tunning</p></blockquote><p><img src="Pretraining-improving-understand.assets/image-20201118200948172.png" alt=""></p><p>[^计算相似性将两种顺序都进行计算的原因可能是增加鲁棒性]: </p><p>自然语言理解包括很多任务,比如<strong>textual entailment,QA,SSA(semantic similarity assessment),document classification.</strong>实现了无监督的预处理以及在target tasks上有监督的微调,无监督学习从word-level,phrase-level的embedding发展到现在的sentence-level,为了捕捉更高级别的语义信息.无监督的预处理是无监督学习的特例,他是为了找到更好的初始化点,帮助神经网络在不同任务(图像分类,语音识别,实体消歧以及MT)上进行训练.之前借助LSTM进行文本信息捕捉,将预测范围限制的很短,本文使用Transformer模型改进了预测范围.预训练还将</p><p>红框标注出了Transformer模型的位置,token级的输入与Linear层之间.token级的输入是创新点,它实现了更细粒度的fine-tunning(微调).先使用未标记的数据学习模型的初始参数,然后再监督目标下将这些参数适应于特定的目标.目标任务与未标记数据集不需要在同一区域,跨任务的模型表现都很好.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><blockquote><p><strong>model</strong>:Transformer</p><p><strong>data set</strong>:text with long dependency</p><p><strong>target task</strong>:QA,SSA(semantic similarity assessment),ED(entailment Denpendency),TC(text classification)</p></blockquote><p>实现了很强的自然语言理解能力，使用Transformer对长连续文本的不同数据集进行无监督预训练，再在目标task上进行有监督微调，提高处理long-dependency文本的能力.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Origin PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Pre-training </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Event2Mind-BiRNN</title>
      <link href="/2020/11/18/event2mind-birnn/"/>
      <url>/2020/11/18/event2mind-birnn/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>Event2Mind: Commonsense Inference on Events, Intents, and Reactions</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>We investigate a new commonsense inference task: given an event described in a short free-form text (“X drinks coffee in the morning”), a system reasons about the likely intents (“X wants to stay awake”) and reactions (“X feels alert”) of the event’s participants.</p></blockquote><p>*<em>input: event *</em></p><p><strong>output: intents and reactions</strong></p><p>发表于ACL2018</p><p>构建了一个event数据集，包含一段描述某一行为（事件）的短文本，以及该行为（事件）施加者的意图、行为发生后施加者的变化和被施加者的变化</p><p>用了一个encoder-decoder作为baseline</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://arxiv.org/pdf/1805.06939.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1805.06939.pdf</a></p><p><a href="https://blog.csdn.net/sjh18813050566/article/details/86651089" target="_blank" rel="noopener">https://blog.csdn.net/sjh18813050566/article/details/86651089</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BiRNN </tag>
            
            <tag> Commonsense </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>“卷积+池化+激活+全连接”</title>
      <link href="/2020/11/16/juan-ji-chi-hua-ji-huo-quan-lian-jie/"/>
      <url>/2020/11/16/juan-ji-chi-hua-ji-huo-quan-lian-jie/</url>
      
        <content type="html"><![CDATA[<p><strong>卷积</strong></p><p>用它来进行特征提取</p><p>一个对象对一个系统连续做用一段时间后求系统的状态</p><p>形象化例子：<a href="https://www.zhihu.com/question/22298352/answer/1071892762?utm_source=qq&amp;utm_medium=social&amp;utm_oi=1151265136986722304" target="_blank" rel="noopener">https://www.zhihu.com/question/22298352/answer/1071892762?utm_source=qq&amp;utm_medium=social&amp;utm_oi=1151265136986722304</a></p><p><strong>池化</strong></p><p>对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征,不易出现过拟合</p><p><strong>激活函数</strong></p><p>如果输入变化很小，导致输出结构发生截然不同的结果，这种情况是我们不希望看到的，为了模拟更细微的变化，输入和输出数值不只是0到1，可以是0和1之间的任何数，</p><p><strong>全连接层</strong></p><p>连接所有的特征，将输出值送给分类器（如softmax分类器）</p><p><strong>Reference</strong></p><p><a href="https://blog.csdn.net/yjl9122/article/details/70198357?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">https://blog.csdn.net/yjl9122/article/details/70198357?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> NN </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Basic concept </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AMBert</title>
      <link href="/2020/11/10/ambert/"/>
      <url>/2020/11/10/ambert/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>AMBert</title>
      <link href="/2020/11/10/ambert-fu-ben/"/>
      <url>/2020/11/10/ambert-fu-ben/</url>
      
        <content type="html"><![CDATA[<blockquote><h4 id="1-Explain-Yourself-Leveraging-Language-Models-for-Commonsense-Reasoning"><a href="#1-Explain-Yourself-Leveraging-Language-Models-for-Commonsense-Reasoning" class="headerlink" title="1.Explain Yourself! Leveraging Language Models for Commonsense Reasoning"></a>1.<a href="https://www.semanticscholar.org/paper/874e9318c09c711ecd48a903b3824a3a03e2cd62" target="_blank" rel="noopener">Explain Yourself! Leveraging Language Models for Commonsense Reasoning</a></h4><p>Nazneen Rajani +2 authors R. Socher</p><p>2019, ACL</p></blockquote><p>new dataset:CoS-E（Common sense Explanations) </p><img src="AMBert - 副本.assets/image-20201110164449218.png" style="zoom: 50%;"><p>![](AMBert - 副本.assets/image-20201110164532528.png)</p><p>首先通过CAGE（Commonsense Auto-generated  Explanation）从Cos-E中生成解释,然后将问题Q,答案选择A以及之前生成的tokenE1,…,Ei-1训练出现存的标签E，将Q，{A1,A2,A3},{E0,…,En}输入CSRM（Commonsense Reasoning Model)，推理答案A。</p><p>![](AMBert - 副本.assets/image-20201113093844596.png)</p><p>Reason推理</p><p>吧因为它们可以在推理过程中自动生成，为常识性问题的回答提供额外的上下文</p><p>Rationable是先预测后解释，Reason是先解释后预测（常识推理）</p><blockquote><h4 id="2-Cosmos-QA-Machine-Reading-Comprehension-with-Contextual-Commonsense-Reasoning"><a href="#2-Cosmos-QA-Machine-Reading-Comprehension-with-Contextual-Commonsense-Reasoning" class="headerlink" title="2.Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"></a>2.<a href="https://www.semanticscholar.org/paper/66117f82def0c69a3b9cc77eb3e2694b0245ca86" target="_blank" rel="noopener">Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning</a></h4><p>Lifu Huang +2 authors Yejin Choi</p><p>2019, EMNLP/IJCNLP</p></blockquote><p>dataset： a new dataset with 35588 reading comprehension problems that require reasoning about the causes and effects of events,the likely facts about people and objects in the scene,and hypotheticals and counterfactuals.</p><p>不同于只关注上下文事实和字面意思的阅读理解，该文在Cosmos QA的基础上创建出一种新的神经网络结构，为了理解人类日常叙事的多样化</p><img src="AMBert - 副本.assets/image-20201110190241234.png" style="zoom: 80%;"><p>正确答案在文本的任何地方都没有提到答案，需要进行上下文的推理才能找出正确答案</p><img src="AMBert - 副本.assets/image-20201110193221604.png" alt="image-20201110193221604" style="zoom: 50%;"><p>对问题进行分类</p><p>![](AMBert - 副本.assets/image-20201110192020018.png)</p><ul><li><p>Input:文段：Paragraph，问题：Question,答案：Answer,使用   <strong>[CLS],P,[SEP],Q,[SEP],A,[SEP]</strong>  格式（CLS是分类符），通过计算来自BERT的segment.position embedding以及相关tokens进行初始化，再输入Hidden层计算出 </p><p>[Hcls,Hp,Hq,Ha].</p></li><li><p>Multi-head Attention:</p></li></ul><blockquote><h4 id="3-Abductive-Commonsense-Reasoning"><a href="#3-Abductive-Commonsense-Reasoning" class="headerlink" title="3.Abductive Commonsense Reasoning"></a>3.<a href="https://www.semanticscholar.org/paper/a550f576ff20b8cce98f3ddad0043d3783fbc9b4" target="_blank" rel="noopener">Abductive Commonsense Reasoning</a></h4><p>Chandra Bhagavatula +7 authors Yejin Choi</p><p>2020, ICLR</p></blockquote><p>dataset:ART（Abuductive Reasoning in Narrative Text)</p><p>Abductive Commonsense Reasoning(溯因推理)是对不完全情景的<strong>最合理解释</strong>或假设的推论。</p><p>![](AMBert - 副本.assets/image-20201111093203543.png)</p><p>对于观察到的O1,O2讲述的背后发生的故事进行推理，我们在ART数据集上仅仅在一对解释中进行选择。</p><p>![](AMBert - 副本.assets/image-20201111143357993.png)</p><blockquote><p><em>O</em>1: “Carl went to the store desperately searching for flflour tortillas for a recipe.” </p><p><em>O</em>2: “Carl left the store very frustrated.”</p><p>incorrect <em>h</em>1 : “The cashier was rude” </p><p>correct <em>h</em>2 : “The store had corn tortillas, but not flflour ones.”</p></blockquote><p>线性链和全连接模型针对上例的区别：</p><p>线性链：先看到O1，h1,h2都是看似可能的推断，再看到O2，Carl非常沮丧的离开商店的原因更有可能是h1，则推断错误。</p><p>全连接模型：结合O1中Carl的目标是fiflour tortillas和O2中Carl沮丧的结果，推断出h2更有可能。</p><blockquote><h4 id="4-Does-It-Make-Sense-And-Why-A-Pilot-Study-for-Sense-Making-and-Explanation"><a href="#4-Does-It-Make-Sense-And-Why-A-Pilot-Study-for-Sense-Making-and-Explanation" class="headerlink" title="4.Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation"></a>4.<a href="https://www.semanticscholar.org/paper/e5a400f9663b3d1f6784cd078f5a471e4f17535b" target="_blank" rel="noopener">Does It Make Sense? And Why? A Pilot Study for Sense Making and Explanation</a></h4><p>Cunxiang Wang +3 authors Tian Gao</p><p>2019, ACLP</p></blockquote><img src="AMBert - 副本.assets/image-20201111192410864.png" style="zoom:67%;"><p>首先在两个自然语言陈述中选择哪个是有意义，哪个无意义，然后选择出陈述无意义的原因</p><blockquote><h4 id="5-COMET-Commonsense-Transformers-for-Automatic-Knowledge-Graph-Construction"><a href="#5-COMET-Commonsense-Transformers-for-Automatic-Knowledge-Graph-Construction" class="headerlink" title="5.COMET: Commonsense Transformers for Automatic Knowledge Graph Construction"></a>5.<a href="https://www.semanticscholar.org/paper/f48ae425e2567be2d993efcaaf74c2274fc9d7c5" target="_blank" rel="noopener">COMET: Commonsense Transformers for Automatic Knowledge Graph Construction</a></h4><p>Antoine Bosselut +4 authors Yejin Choi</p><p>2019, ACL</p></blockquote><img src="AMBert - 副本.assets/image-20201112151418083.png" style="zoom: 67%;"><p> 与许多用规范模板存储知识的传统KB相反，常识性KB只存储松散结构的知识开放文本描述。当深层预先训练的语言模型中的隐式知识被转移到常识知识图中生成显式知识时，常识图谱的模型成为可能。</p><p>![](AMBert - 副本.assets/image-20201112160202921.png)</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SpanBert</title>
      <link href="/2020/11/09/spanbert/"/>
      <url>/2020/11/09/spanbert/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>SpanBERT: Improving Pre-training by Representing and Predicting Spans</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>​        We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution.</p></blockquote><p>​    SpanBert相比于Bert更够更好的表示和预测文章的范围。</p><p>我们的方法在Bert的基础上mask了连续随机的范围，而非随机的token， 而且训练范围的表示去预测整个文本的masked span，为不是依赖于单个token。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><img src="SpanBert.assets/image-20201110084120479.png" alt=""></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://arxiv.org/pdf/1907.10529.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1907.10529.pdf</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> pre-training-model </tag>
            
            <tag> SpanBERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K-Bert</title>
      <link href="/2020/11/07/k-bert/"/>
      <url>/2020/11/07/k-bert/</url>
      
        <content type="html"><![CDATA[<p><a href="https://arxiv.org/pdf/1909.07606.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1909.07606.pdf</a></p><h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>K-BERT: Enabling Language Representation with Knowledge Graph</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Bert缺少domain-specific knowledge,专家通过推理来阅读domain text，我们提出基于知识图谱的K-BERT，将三元组嵌入句子中作为domain knowledge。融入太多知识可能会偏离使句子偏离他原本正确的意思，产生KN(knowkedge noise) issue，通过soft-position和visible matrix来进行限制。</p></blockquote><p>[^本体是从客观世界中抽象出来的一个概念模型，这个模型包含了某个学科领域内的基本术语和术语之间的关系.本体是共享概念模型的形式化规范说明。]: </p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>无监督的预处理的LR（Language Representation)模型BERT由于pre-training和fine-tunning之间存在domain-discrepancy,所以在知识驱动的任务中表现不好。</p><p>在domain-specific task中，在open和specific domain上知识的连接太少了，如果选择在specific domain上的预训练模型，耗时又计算量大，所以选择将KG融入LR模型，这样模型有更好的解释性，插入的知识是可解释的。</p><p>我们提出了一种基于知识的LR模型,它在没有HES和KN问题更好的融合了domain knowledge.</p><h3 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a><strong>Structure</strong></h3><p><strong>K-bert：knowledge-layer,Embedding layer,seeing layer,maske-transformer.</strong></p><p><img src="K-Bert.assets/image-20201109085016622.png" alt="i"></p><p>[^Visible matrix 控制每个token的可见区域，矩阵中的aij即i是否可见j，这样避免了原始意思因为KG的插入而改变]: </p><p><img src="K-Bert.assets/image-20201108202155282.png" alt=""></p><p>[^Soft-position embedding对sentence tree中的乱序进行排序，计算self-attention score.]: 如果计算的position number相同但是并没有什么关联，就要使用Mask-Self-Attention<br>[^Segment embedding就是对不同句子进行区分]: </p><p><strong>Masked-Self-Attention</strong></p><img src="K-Bert.assets/image-20201109093911044.png" style="zoom:80%;"><img src="K-Bert.assets/image-20201109093804902.png" style="zoom:67%;"><p>wi与wj无关的话Mij取-∞,softmax为0</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> K-BERT </tag>
            
            <tag> pre-training-model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pre-train-knowledge-graph</title>
      <link href="/2020/11/07/pre-train-knowledge-graph/"/>
      <url>/2020/11/07/pre-train-knowledge-graph/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ssh-win10</title>
      <link href="/2020/11/03/ssh-win10/"/>
      <url>/2020/11/03/ssh-win10/</url>
      
        <content type="html"><![CDATA[<h3 id="ssh远程登录linux系统（win10）"><a href="#ssh远程登录linux系统（win10）" class="headerlink" title="ssh远程登录linux系统（win10）"></a>ssh远程登录linux系统（win10）</h3><ol><li><p>安装openssh</p><p>参考reference1.</p></li><li><p>输入ssh，如果出现下图的执行结果，则安装成功</p><p><img src="ssh-win10.assets/image-20201103170156113.png" alt=""></p></li><li><p>通过ssh对远程linux系统进行访问(密钥加密）。</p><pre><code>ssh -i 私钥位置 host@ip_addr -p port</code></pre></li></ol><ol start="4"><li><p>通过ssh进行tunnel转发</p><pre><code>ssh -L port1:127.0.0.1:port1 -L port2:127.0.0.1:port2 usrname@ip_addr -p port</code></pre><p>`在浏览器上输入localhost:port1进行浏览</p></li></ol><h3 id="Rerference"><a href="#Rerference" class="headerlink" title="Rerference"></a>Rerference</h3><p>1.<a href="https://blog.csdn.net/weixin_41601114/article/details/108052922?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41601114/article/details/108052922?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Commontools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> neo4j </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Extraction-rule-Morphological-Agreement</title>
      <link href="/2020/10/30/extraction-rule-morphological-agreement/"/>
      <url>/2020/10/30/extraction-rule-morphological-agreement/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThroughy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Extraction-rule </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NN-to-LSTM</title>
      <link href="/2020/10/27/nn-to-lstm/"/>
      <url>/2020/10/27/nn-to-lstm/</url>
      
        <content type="html"><![CDATA[<h4 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h4><p><img src="NN-to-LSTM.assets/image-20201027141436148.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT</title>
      <link href="/2020/10/27/bertrt/"/>
      <url>/2020/10/27/bertrt/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BERT</title>
      <link href="/2020/10/27/bert/"/>
      <url>/2020/10/27/bert/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p><strong>《**</strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding**》</p><h3 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1.Abstract"></a>1.Abstract</h3><blockquote><p>​        We introduce a new language representation model called <strong>BERT</strong>, which stands for <strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fifinetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task specifific architecture modififications.</p></blockquote><p>​    <strong>BERT</strong>全称Bidirectional Encoder Representation from Transformer,它是一种在未标记的文本中,用所有layer上下文为共同条件进行深度双向的预训练表示。预训练BERT模型只微调一个额外输出层，没有进行实质性任务特定的模型修改，为多领域（比如问题回答和语言推理）的任务创造一个先进的模型。</p><h3 id="2-Introduction"><a href="#2-Introduction" class="headerlink" title="2.Introduction"></a>2.Introduction</h3><p>​    预训练模型在许多任务中表现很好，such as：</p><p>A：sentence-level（推理和解释<strong>inference和paraphrasing</strong>)</p><p>B:   token-level (NER,QA)</p><p>​    预训练的两个下游任务：基于特征(ELMO)以及微调(OpenAI GPT)参数</p><p>​    fine-tunning微调在Transformer的自注意力层中只能关注前序token的信息，在<strong>QA</strong>这种需要关注上下文的应用中存在问题。还提出了<strong>masked language model</strong>(MLM),它通过随机mask一些输入中的token来随机预测一些基于文本背景的masked token</p><h3 id="3-BERT"><a href="#3-BERT" class="headerlink" title="3.BERT"></a>3.BERT</h3><p><img src="BERTRT.assets/image-20201107100726796.png" alt=""></p><ul><li><p><strong>Bert</strong>模型的框架包括两个部分：<strong>pre-training,fine-tunning</strong>:<br>pre-training - unlabeled data - pre-traning task</p><p>B:fine-tunning - labeled data - downstream task</p></li><li><p><strong>Model architecture</strong>:multi-layer bidirectional Transformer</p><p>num of layer:L,hidden size:H,self-attention head A</p></li><li><p><strong>Input/Output Representation</strong></p></li></ul><p><img src="BERTRT.assets/image-20201107103633037.png" alt=""></p><p>[^Segment Embedding是标记token属于句子A还是句子B的]: </p><h4 id="3-1Pre-training-BERT"><a href="#3-1Pre-training-BERT" class="headerlink" title="3.1Pre-training BERT"></a>3.1Pre-training BERT</h4><h5 id="Task1-Masked-LM"><a href="#Task1-Masked-LM" class="headerlink" title="Task1 Masked LM"></a>Task1 Masked LM</h5><p>随机选择input token中的一些token进行随机mask，接着预测这些token，最后将和maskToken相关的final hidden vector放入output softmax。有一个缺陷是在pre-training和fine-tunning可能有不匹配问，在fine-tunning时[Mask]并没有出现，解决方法是：</p><ol><li>训练数据随机选择15%的token position进行预测</li><li>对于选中的token：80%[MASK],10% random token,10%unchanged</li></ol><h5 id="Task2-NextSentence-Prediction"><a href="#Task2-NextSentence-Prediction" class="headerlink" title="Task2 NextSentence Prediction"></a>Task2 NextSentence Prediction</h5><p><strong>QA</strong>和<strong>NLI</strong>都是基于理解两个句子之间的关系的，这个关系无法在语言模型中被捕捉，为了解决该问题，预训练了</p><p>binarized<em>next sentence prediction</em>task,在每个预训练例子中找到对应的A,B，发现50%的句子B确实是A后继labed(isNext)，其他是来自数据库的随机句子</p><p>label(notnext)</p><h4 id="3-2Fine-tunning-BERT"><a href="#3-2Fine-tunning-BERT" class="headerlink" title="3.2Fine-tunning BERT"></a>3.2Fine-tunning BERT</h4><p>微调是简单的，因为BERT的自注意力机制允许通过合适的交换合适的输入和输出来模拟下游任务。</p><p>文本对应用：独立地对文本对进行<strong>encode+bidirectional cross attention</strong>.</p><p>BERT通过使用自注意机制来进行这两个部分。encode a concatenated text pair with self-attention</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> BERT </tag>
            
            <tag> Pre-training-model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nlp-word-model</title>
      <link href="/2020/10/23/nlp-word-model/"/>
      <url>/2020/10/23/nlp-word-model/</url>
      
        <content type="html"><![CDATA[<h4 id="基于HMM的分词："><a href="#基于HMM的分词：" class="headerlink" title="基于HMM的分词："></a>基于HMM的分词：</h4><p>​    隐含马尔可夫模型（HMM）是将分词作为字在句子中的序列标注任务来实现的。其基本思路是：每个字在构造一个特定词语时都占据着一个特定的位置即词位，一般采用四结构词位：B（词首），M（词中），E（词尾）和S（单独成词）。比如：</p><p>‘中文/分词/是/文本处理/不可或缺/的/一步/！’，</p><p>标注后的形式：</p><p>‘中/B 文/E 分/B 词/E 是/S 文/B 本/M 处/M 理/E 不/B 可/M 或/M 缺/E 的/S 一/B 步/E ！/S’。</p><p>其中，词位序列代表着HMM中不可见的隐藏状态序列，而训练集中的文本则为可见的观测序列。这样就变成了已知观测序列，求未知的隐藏序列的HMM问题。</p><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a href="https://www.jianshu.com/p/e70df0edbb5c" target="_blank" rel="noopener">https://www.jianshu.com/p/e70df0edbb5c</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Model summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>study-note-20201009-20201022</title>
      <link href="/2020/10/22/study-note-20201009-20201022/"/>
      <url>/2020/10/22/study-note-20201009-20201022/</url>
      
        <content type="html"><![CDATA[<h4 id="一、学习内容"><a href="#一、学习内容" class="headerlink" title="一、学习内容"></a>一、学习内容</h4><ol><li><p>研读论文</p><p>CCG方向：<strong>《CCG Supertagging with a Recurrent Neural Network》</strong></p><p>[^Question:the architecture of CCG Supertagging and parsing  can’t consider local context text]: Strategy:Use feeed-forward RNN,distributed word represention                                                                                                       Conclusion:substantial accuracy improvements</p><p>ASR方向：<strong>《END-TO-END ASR: FROM SUPERVISED TO SEMI-SUPERVISED LEARNING WITH MODERN ARCHITECTURES》</strong></p><p>[^由于获取大量被标记的数据成本较高，提出了一种semi-supervised(半监督），用来利用未标记的数据进行训练模型]: 论文中提出一种评价未标记音频特性的方法，生成一种end-to-end 声学模型。</p></li><li><p>《统计学习方法》(李航）</p><p>第6，7，9，10章：推导公式，理解原理</p></li><li><p>运行第一个<strong>allenlp</strong>模型</p></li></ol><h4 id="二、困难"><a href="#二、困难" class="headerlink" title="二、困难"></a>二、困难</h4><ol><li><p>对《统计学习方法》数学公式的推导原理不能理解透彻</p><h5 id="Q-在不等式约束的拉格朗日函数中，什么条件下取到极值？"><a href="#Q-在不等式约束的拉格朗日函数中，什么条件下取到极值？" class="headerlink" title="Q:在不等式约束的拉格朗日函数中，什么条件下取到极值？"></a>Q:在不等式约束的拉格朗日函数中，什么条件下取到极值？</h5><p>C1：假设目标函数是f(x),限定条件是等式h(x)=0,则极值点一定取在切点，假设不在切点那么h(x)向内移或者向外移动会变大或变小，所以不是极值点。</p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201022095334963.png" alt=""></p><p>C2：在限定条件中加一个不等式约束g(x)&lt;=0,f(x)的等值图与h(x)与g(x)的图像如下</p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201022095358786.png" alt=""></p><p>关于f(x)极值点的分布有两种情况:</p><ol><li>f(x)本身的极值点在限制域中</li><li>极值点在h(x)的边界上取到</li></ol><p>第一种情况：h(x)限制无效，则不对它进行约束，目标函数可以看作f(x)+μg(x),对x求偏导得到f’(x)+μg’(x),由图像可知，在极值点f(x)与g(x)的极值符号相反，为了使目标函数的偏导为0，则μ&gt;=0且g(x)=0</p><p>[^取等号是因为有可能g(x)也在该点处取到极值点。]: </p><p>第二种情况：g(x)限制无效，则μ=0且g(x)&lt;=0</p><p><strong>综上μg(x)=0</strong></p></li></ol><h4 id="三、下阶段计划"><a href="#三、下阶段计划" class="headerlink" title="三、下阶段计划"></a>三、下阶段计划</h4><ol><li>论文继续读CCG方向的论文，定期整理总结</li><li>继续研读《统计学习方法》</li><li>一周运行一个allenlp模型</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>study-note-20201009-20201022</title>
      <link href="/2020/10/22/study-note-20201023-20201104/"/>
      <url>/2020/10/22/study-note-20201023-20201104/</url>
      
        <content type="html"><![CDATA[<h4 id="一、学习内容"><a href="#一、学习内容" class="headerlink" title="一、学习内容"></a>一、学习内容</h4><ol><li><p>研读尝试常识图谱相关的论文</p><p>《<strong>Automatic Extraction of Rules Governing Morphological Agreement</strong>》</p><p>《<strong>How to marry a star: probabilistic constraints for meaning in context</strong>》</p></li><li><p>学习neo4j数据库基本指令的用法</p></li><li><p>阅读白硕老师的“闲话语义（1）-（7）”，对语义进行更深一步的理解</p></li></ol><h4 id="二、困难"><a href="#二、困难" class="headerlink" title="二、困难"></a>二、困难</h4><ol><li>对《<strong>How to marry a star: probabilistic constraints for meaning in context</strong>》中有些部分没有理解，后期通过询问师哥或者论文作者来进行进一步理解。</li></ol><h4 id="三、下阶段计划"><a href="#三、下阶段计划" class="headerlink" title="三、下阶段计划"></a>三、下阶段计划</h4><ol><li>继续研读《<strong>How to marry a star: probabilistic constraints for meaning in context</strong></li><li>对常识图谱的模式匹配部分和牛润良师哥合作运行一个demo</li><li>对常识图谱的语义结构（eg:需要提取的relationship类型，模式匹配中Entity的词性）进行设计，在老师的建议下进行优化。</li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Code-allenlp</title>
      <link href="/2020/10/21/code-allenlp/"/>
      <url>/2020/10/21/code-allenlp/</url>
      
        <content type="html"><![CDATA[<h5 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h5><h3 id="text-classification"><a href="#text-classification" class="headerlink" title="text classification"></a>text classification</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201021161233310.png" alt=""></p><h3 id="Defining-input-and-output"><a href="#Defining-input-and-output" class="headerlink" title="Defining input and output"></a>Defining input and output</h3><p>AllenNLP</p><p>one example：one instance:one or more Fields(each Field is  a input or output data) <strong>TextField(in) or LabelField(out)</strong></p><p>Field-&gt;tensor to model</p><h3 id="Read-data"><a href="#Read-data" class="headerlink" title="Read data"></a>Read data</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201021162313203.png" alt=""></p><p>trasfer raw data to instances that match input/output spec</p><h3 id="Make-datasetReader"><a href="#Make-datasetReader" class="headerlink" title="Make datasetReader"></a>Make datasetReader</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201021183158886.png" alt=""></p><p>逐行读取text，将每个line分成单个的token，在根据词汇表中对应的id组成tensor</p><h3 id="Build-model"><a href="#Build-model" class="headerlink" title="Build model"></a>Build model</h3><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201021185926047.png" alt=""></p><ul><li><p>get features from each input word</p></li><li><p>combine word-level features to document-level feature vector</p></li><li><p>classify vector into one label</p><h4 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h4><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201021202338539.png" alt=""></p></li></ul><p>[^batch_size:每次训练时的样本数]: </p><ol><li>对inatances进行编码生成Token IDs（SingleIDTokenIndexr())</li><li>将token通过embedding函数，生成vector</li><li>对vector进行压缩，变成单个向量</li><li>生成label概率分布，对loss进行计算</li></ol><h3 id="Allenlp-structure"><a href="#Allenlp-structure" class="headerlink" title="Allenlp structure"></a>Allenlp structure</h3><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201022081140013.png" alt=""></p><ol><li>is a <strong>Pytorch</strong> Model</li><li>forward() makes output be a <strong>dictionary</strong></li><li>output includes <strong>loss key</strong> to optimize the model</li></ol><h4 id="总流程"><a href="#总流程" class="headerlink" title="总流程"></a>总流程</h4><p><strong>Text -&gt; classifier -&gt; Instances(TextField+LabelField) -&gt; Model(Embedding Vector+Seq2Vec Encoding) -&gt; lables</strong></p><p><strong>Text-&gt;Text-&gt;instance-&gt;TokenIds(vector)-&gt;tensor-&gt;vector</strong></p><h3 id="Training-and-prediction"><a href="#Training-and-prediction" class="headerlink" title="Training and prediction"></a>Training and prediction</h3><p>读取电影数据集</p><pre><code>class ClassificationTsvReader(DatasetReader):    def __init__(self,                 lazy: bool = False,                 tokenizer: Tokenizer = None,                 token_indexers: Dict[str, TokenIndexer] = None,                 max_tokens: int = None):        super().__init__(lazy)        self.tokenizer = tokenizer or WhitespaceTokenizer()        self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}        self.max_tokens = max_tokens    def _read(self, file_path: str) -&gt; Iterable[Instance]:        with open(file_path, 'r') as lines:            for line in lines:                text, sentiment = line.strip().split('\t')                tokens = self.tokenizer.tokenize(text)                if self.max_tokens:                    tokens = tokens[:self.max_tokens]                text_field = TextField(tokens, self.token_indexers)                label_field = LabelField(sentiment)                fields = {'text': text_field, 'label': label_field}                yield Instance(fields)dataset_reader = ClassificationTsvReader(max_tokens=64)instances = dataset_reader.read("quick_start/data/movie_review/train.tsv")for instance in instances[:10]:    print(instance)</code></pre><p><strong>Text-&gt;Instances</strong>，输出是从数据集中读取的前十个instance</p><h4 id="Feeding-instances-to-the-model"><a href="#Feeding-instances-to-the-model" class="headerlink" title="Feeding instances to the model"></a>Feeding instances to the model</h4><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201026144259063.png" style="zoom: 50%;"><pre><code>class SimpleClassifier(Model):    def __init__(self,                 vocab: Vocabulary,                 embedder: TextFieldEmbedder,                 encoder: Seq2VecEncoder):        super().__init__(vocab)        self.embedder = embedder        self.encoder = encoder        num_labels = vocab.get_vocab_size("labels")        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)    def forward(self,                text: Dict[str, torch.Tensor],                label: torch.Tensor) -&gt; Dict[str, torch.Tensor]:        # Shape: (batch_size, num_tokens, embedding_dim)        embedded_text = self.embedder(text)        # Shape: (batch_size, num_tokens)        mask = util.get_text_field_mask(text)        # Shape: (batch_size, encoding_dim)        encoded_text = self.encoder(embedded_text, mask)        # Shape: (batch_size, num_labels)        logits = self.classifier(encoded_text)        # Shape: (batch_size, num_labels)        probs = torch.nn.functional.softmax(logits, dim=-1)        # Shape: (1,)        loss = torch.nn.functional.cross_entropy(logits, label)        return {'loss': loss, 'probs': probs}def run_training_loop():    dataset_reader = ClassificationTsvReader(max_tokens=64)    print("Reading data")    instances = dataset_reader.read("quick_start/data/movie_review/train.tsv")    vocab = build_vocab(instances)    model = build_model(vocab)    outputs = model.forward_on_instances(instances[:4])    print(outputs)def build_vocab(instances: Iterable[Instance]) -&gt; Vocabulary:    print("Building the vocabulary")    return Vocabulary.from_instances(instances)def build_model(vocab: Vocabulary) -&gt; Model:    print("Building the model")    vocab_size = vocab.get_vocab_size("tokens")    embedder = BasicTextFieldEmbedder(        {"tokens": Embedding(embedding_dim=10, num_embeddings=vocab_size)})    encoder = BagOfEmbeddingsEncoder(embedding_dim=10)    return SimpleClassifier(vocab, embedder, encoder)run_training_loop()</code></pre><blockquote><p><strong>self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)</strong></p><p>[^LInear(input_dim,output_dim):输入向量维度（batch_size,input_dim),输出向量维度（batch_size,output_dim)]: </p><p><strong>torch.nn.functional.cross_entropy</strong></p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201026110015728.png" style="zoom: 50%;"><p>nll_loss函数</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201026110459013.png" alt="image-20201026110459013" style="zoom: 67%;"><p>运行结果：<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201026145609293.png" style="zoom: 67%;"></p><p>[^？？？embedding，encoding返回的小数计算]: </p></blockquote><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://guide.allennlp.org/" target="_blank" rel="noopener">https://guide.allennlp.org/</a></p><p><a href="https://blog.csdn.net/poyue8754/article/details/84680609?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">https://blog.csdn.net/poyue8754/article/details/84680609?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p><a href="https://www.cnblogs.com/marsggbo/p/10401215.html" target="_blank" rel="noopener">https://www.cnblogs.com/marsggbo/p/10401215.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code structure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CCG-Parsing-LSTM</title>
      <link href="/2020/10/20/ccg-parsing-lstm/"/>
      <url>/2020/10/20/ccg-parsing-lstm/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CCG-Supertagging-Faster-Parsing</title>
      <link href="/2020/10/20/ccg-supertagging-faster-parsing/"/>
      <url>/2020/10/20/ccg-supertagging-faster-parsing/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CCG-Supertagging-RNN</title>
      <link href="/2020/10/20/ccg-supertagging-rnn/"/>
      <url>/2020/10/20/ccg-supertagging-rnn/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>CCG Supertagging with a Recurrent Neural Network</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Recent work on supertagging using a feed-forward neural network achieved signifificant improvements for </p><p>CCG supertagging and parsing (Lewis and Steedman, 2014).However, their architecture is limited to</p><p>considering local contexts and does not naturally model sequences of arbitrary length.</p></blockquote><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><blockquote><p>And third, in order to reduce computational requirements and feature sparsity, each tagging decision is made without considering any potentially useful contextual information beyond a local context window.</p></blockquote><h3 id="Q"><a href="#Q" class="headerlink" title="Q:"></a>Q:</h3><p>Above all,these sequences tell us the problem:<strong>the architecture can’t consider local contect text and naturally model sequences of arbitrary.</strong></p><p>[^rely too much on POS tagging,there three drawbacks]: POS  tagging是语义标签，NLTK词性标注器，先将句子中的所有word分开，再对每一个word进行词性的标注，生成一个语义词汇对照序列</p><p>1.the effectiveness of supertagging make the accuracy of out of-domain-data down</p><p>2.performance degradation will happen if there are rare or unseen words(features mainly base on raw words and POS tag)</p><p>3.the architecture can’t consider local contect text</p><h3 id="S"><a href="#S" class="headerlink" title="S:"></a>S:</h3><p>RNN ，Distributed word representation,C&amp;C parser as supertagger</p><p>[^Word2vec:Distributed word representation,一种word的低维表示，相比于one-hot，他的速度更快而且包含语义信息，即语义相近的两个词语表示的向量之间的距离也近]: </p><h3 id="C"><a href="#C" class="headerlink" title="C:"></a>C:</h3><p>we obtain substantial accuracy improvements, outperforming the feed-forward setup on both supertagging and parsing.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/wangyangjingjing/article/details/86631058" target="_blank" rel="noopener">https://blog.csdn.net/wangyangjingjing/article/details/86631058</a></p><p><a href="https://blog.csdn.net/AMDS123/article/details/67644298?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">https://blog.csdn.net/AMDS123/article/details/67644298?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThroughy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CCG </tag>
            
            <tag> RNN </tag>
            
            <tag> Supertagging </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CCG-Supertagging-LSTM</title>
      <link href="/2020/10/20/ccg-supertagging-lstm/"/>
      <url>/2020/10/20/ccg-supertagging-lstm/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>Supertagging with LSTMs</strong>》</p><p>LSTM框架</p><img src="CCG-Supertagging-LSTM.assets/image-20201028085923409.png" alt="image-20201028085923409" style="zoom:67%;"><img src="CCG-Supertagging-LSTM.assets/image-20201028090004850.png" style="zoom:67%;"><p>[^LSTM中有三个门电路进行控制，门电路输出为1时门电路打开，反之关闭。]: 相比于Nerual Network,LSTM相当于外接四个电路进行输入，每次输入都对两个输入进行权值的加权运算。最后得出output</p><hr><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>​    In this paper we present new state-of-the-art performance on CCG supertagging and parsing.We analyze the performance of several neural models and demonstrate that while feed-forward architectures can compete with bidirectional LSTMs on POS tagging, models that encode the complete sentence are necessary for the long range syntactic information encoded in supertags.</p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThroughy </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> CCG </tag>
            
            <tag> Supertagging </tag>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CCG-Statistical-Parsing</title>
      <link href="/2020/10/20/ccg-statistical-parsing/"/>
      <url>/2020/10/20/ccg-statistical-parsing/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SpecAugment-Data Augmentation-ASR</title>
      <link href="/2020/10/18/specaugment-data-augmentation-asr/"/>
      <url>/2020/10/18/specaugment-data-augmentation-asr/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>SpecAugment: A Simple Data Augmentation Methodfor Automatic Speech Recognition</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., fifilter bank coef-fificients). The augmentation policy consists of <strong>warping the features, masking blocks of frequency channels, and masking blocks of time steps</strong>. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks.</p></blockquote><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/u010801994/article/details/81914716" target="_blank" rel="noopener">https://blog.csdn.net/u010801994/article/details/81914716</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASR </tag>
            
            <tag> NLP </tag>
            
            <tag> data augmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>End2EndASR-SL2SSL</title>
      <link href="/2020/10/14/end2endasr-sl2ssl.md/"/>
      <url>/2020/10/14/end2endasr-sl2ssl.md/</url>
      
        <content type="html"><![CDATA[<h3 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h3><p><a href="https://arxiv.org/abs/1911.08460" target="_blank" rel="noopener">https://arxiv.org/abs/1911.08460</a></p><h3 id="title"><a href="#title" class="headerlink" title="title"></a>title</h3><p>《END-TO-END ASR: FROM SUPERVISED TO SEMI-SUPERVISED</p><p>LEARNING WITH MODERN ARCHITECTURES》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>We study <strong>pseudo-labeling</strong> for the semi-supervised training of ResNet, Time-Depth Separable ConvNets, and Transformers for speech recognition, with either <strong>CTC or Seq2Seq</strong> loss functions. </p></blockquote><p>[^CTC是针对语音输入和输出序列因为输入音频语速不同导致的无法匹配提出的算法。并且输入输出序列的长度和长度比都在变化。]: eg：CTC对一段语音序列进行映射，比如需要对输入序列进行去重，假如输入是：’heelllo’,直接去重的结果是’helo‘而非’hello‘。 CTC引入空白占位符用于输出对齐。</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201014184329254.png" style="zoom:50%;"><p>CTC特性：</p><ol><li>条件独立：CTC假设每个时间片独立，可以加入语言模型</li><li>单调对齐:在OCR和语音识别,这种约束成立</li><li>多对一映射：lX&gt;lY</li></ol><p>Seq2Seq：Encoder,Decoder,c.每一个box代表一个RNN单元</p><p>[^pseudo-labeling：伪标签]: 学习分为三种，supervised learning,semi-supervised learning,unsupervised learning.因为获取标记样本需要耗费人力和时间，所以对于半监督学习，我们要将标记样本和被标记样本同时使用。</p><ol><li>使用标记数据训练模型（model）</li><li>用1生成的model对未标记的数据进行预测分类生成pseudo-labeling，再使用pseudo-labeling和标记数据在对模型进行训练优化</li></ol><p>[^<strong>end-to-end learning</strong>:可以直接将输入的x,y映射成输出。不用通过中间件，但是需要大量数据集来进行训练，如果数据数量不足，可以借助中间件。]:<br>[^交叉熵：信息论中的一个概念]: 概率越小的事件发生，信息量越大，用I(x0)=−log(p(x0))定义事件X=x0的信息量。用熵来表示所有信息量的期望H(X)=−∑p(xi)log(p(xi))</p><p>相对熵（KL散度）：</p><blockquote><p>In the context of machine learning, DKL(P‖Q) is often called the information gain achieved if P is used instead of Q.</p></blockquote><p>KL散度计算公式：<img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201016171828234.png" style="zoom: 50%;"></p><p>变形得到<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201016171921231.png" alt="image-20201016171921231" style="zoom:50%;"></p><p>后一部分为交叉熵，因为前一部分基本不变所以用交叉熵来评估模型。</p><h3 id="Q"><a href="#Q" class="headerlink" title="Q:"></a>Q:</h3><p>基于Transformer的声学模型只使用监督数据集有很好的表现，但是不同模型间存在差异</p><h3 id="S："><a href="#S：" class="headerlink" title="S："></a>S：</h3><p>提出了几种评价未标记音频特性的方法</p><h3 id="C："><a href="#C：" class="headerlink" title="C："></a>C：</h3><p>我们实现了一种新的先进的端到端声学模型解码，基于标准监督学习的外部语言模型和一个新的先进的半监督训练</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://blog.csdn.net/tsyccnh/article/details/79163834?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase" target="_blank" rel="noopener">https://blog.csdn.net/tsyccnh/article/details/79163834?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.nonecase</a></p><p><a href="https://blog.csdn.net/qq_32241189/article/details/81591456" target="_blank" rel="noopener">https://blog.csdn.net/qq_32241189/article/details/81591456</a></p><p><a href="https://blog.csdn.net/liuxiaoheng1992/article/details/83660557" target="_blank" rel="noopener">https://blog.csdn.net/liuxiaoheng1992/article/details/83660557</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASR </tag>
            
            <tag> NLP </tag>
            
            <tag> semi-supervised </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep Belief Network-ASR.md</title>
      <link href="/2020/10/12/deep-belief-network-asr-md/"/>
      <url>/2020/10/12/deep-belief-network-asr-md/</url>
      
        <content type="html"><![CDATA[<script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASR </tag>
            
            <tag> HMM-DBN </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-LiHang</title>
      <link href="/2020/10/11/notes-lihang/"/>
      <url>/2020/10/11/notes-lihang/</url>
      
        <content type="html"><![CDATA[<h3 id="第六章-logistic-regression-model-logistic-回归模型"><a href="#第六章-logistic-regression-model-logistic-回归模型" class="headerlink" title="第六章 logistic regression model(logistic 回归模型)"></a>第六章 logistic regression model(logistic 回归模型)</h3><blockquote><p>拟牛顿法：<a href="https://blog.csdn.net/itplus/article/details/21896453" target="_blank" rel="noopener">https://blog.csdn.net/itplus/article/details/21896453</a></p></blockquote><h3 id="第九章-EM算法及其推广"><a href="#第九章-EM算法及其推广" class="headerlink" title="第九章 EM算法及其推广"></a>第九章 EM算法及其推广</h3><h4 id="9-1EM算法引入"><a href="#9-1EM算法引入" class="headerlink" title="9.1EM算法引入"></a>9.1EM算法引入</h4><p>EM算法（expectation maximization algorithm)期望极大值算法</p><p><a href="https://blog.csdn.net/u010834867/article/details/90762296" target="_blank" rel="noopener">https://blog.csdn.net/u010834867/article/details/90762296</a></p><p><strong>Jensen不等式</strong></p><p>对于凸函数而言：</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201011101138287.png" alt=""></p><p>[^如果函数为凹函数，则符号的方向反之。]: </p><h3 id="第十章-HMM（Hidden-Markov-model-隐马尔科夫模型"><a href="#第十章-HMM（Hidden-Markov-model-隐马尔科夫模型" class="headerlink" title="第十章 HMM（Hidden Markov model)隐马尔科夫模型"></a>第十章 HMM（Hidden Markov model)隐马尔科夫模型</h3><p><strong>HMM</strong>是统计模型，包含五要素：观测序列，隐状态，初始概率，转移概率（隐含状态之间的概率），输出概率（隐含状态和可见状态之间的概率）</p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201012100629184.png" style="zoom:50%;"><h3 id="第七章-支持向量机"><a href="#第七章-支持向量机" class="headerlink" title="第七章 支持向量机"></a>第七章 支持向量机</h3><h4 id="7-2线性支持向量机与软间隔最大化"><a href="#7-2线性支持向量机与软间隔最大化" class="headerlink" title="7.2线性支持向量机与软间隔最大化"></a>7.2线性支持向量机与软间隔最大化</h4><p>p128 7.53,7.54</p><h5 id="Q-在不等式约束的拉格朗日函数中，什么条件下取到极值？"><a href="#Q-在不等式约束的拉格朗日函数中，什么条件下取到极值？" class="headerlink" title="Q:在不等式约束的拉格朗日函数中，什么条件下取到极值？"></a>Q:在不等式约束的拉格朗日函数中，什么条件下取到极值？</h5><p>C1：假设目标函数是f(x),限定条件是等式h(x)=0,则极值点一定取在切点，假设不在切点那么h(x)向内移或者向外移动会变大或变小，所以不是极值点。</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201021084447071.png" alt="图中为f(x)的等值图以及h(x)的函数图" style="zoom: 50%;"><p>C2：在限定条件中加一个不等式约束g(x)&lt;=0,f(x)的等值图与h(x)与g(x)的图像如下</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201021084837306.png" alt="image-20201021084837306" style="zoom:50%;"><p>关于f(x)极值点的分布有两种情况:</p><ol><li>f(x)本身的极值点在限制域中</li><li>极值点在h(x)的边界上取到</li></ol><p>第一种情况：h(x)限制无效，则不对它进行约束，目标函数可以看作f(x)+μg(x),对x求偏导得到f’(x)+μg’(x),由图像可知，在极值点f(x)与g(x)的极值符号相反，为了使目标函数的偏导为0，则μ&gt;=0且g(x)=0（对μ求偏导得到g(x)</p><p>[^取等号是因为有可能g(x)也在该点处取到极值点。]: </p><p>第二种情况：g(x)限制无效，则μ=0且g(x)&lt;=0</p><p><strong>综上μg(x)=0</strong></p><h5 id="7-2-4合页损失函数"><a href="#7-2-4合页损失函数" class="headerlink" title="7.2.4合页损失函数"></a>7.2.4合页损失函数</h5><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201021093753757.png" style="zoom:50%;"><ol><li><p>0-1损失</p><p>正确分类，损失为0；错误分类，损失为1</p></li><li><p>感知机损失函数</p><p>正确分类，损失为0；错误分离，损失为-y(w·x+b)</p></li><li><p>合页损失函数</p><p>正确分类且函数间隔&gt;=1损失为0，否则损失为1-y(w·x+b)，*<em>合页损失函数对函数有更高的要求                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  *</em></p><p>[^函数间隔是指实例点到超级分离平面的距离，表示该实例点被分类的可信任程度。]: </p></li></ol><h4 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h4><p><a href="https://blog.csdn.net/baidu_38172402/article/details/89090383" target="_blank" rel="noopener">https://blog.csdn.net/baidu_38172402/article/details/89090383</a></p><p><a href="https://www.cnblogs.com/fulcra/p/11065474.html" target="_blank" rel="noopener">https://www.cnblogs.com/fulcra/p/11065474.html</a></p><p><a href="https://blog.csdn.net/weixin_41500849/article/details/80493712?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41500849/article/details/80493712?utm_medium=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p>《统计学习方法》（李航）</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> BookLook </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convs2s-MT</title>
      <link href="/2020/10/09/convs2s-mt/"/>
      <url>/2020/10/09/convs2s-mt/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《<strong>Convolutional Sequence to Sequence Learning</strong>》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>We introduce an architecture based entirely on convolutional neural networks.Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fifixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module.</p></blockquote><p>Compared with RNN,CNN的计算并行性更好，训练的时候可以更好利用GPU而且优化更容易进行，因为非线性参数的数量固定，与输入长度无关。我们使用门控线性单元消除梯度传播</p><p>Compared with RNN,CNN create representation for fixed size,but the context size of the network can be easily made larger by stacking.CNNs do not depend on the previous time step and allow parallelization over every element in a seq.</p><p>[^RNN matain a hidden state of the entire past that prevents parallel computation]: </p><p>Muti-layer CNN create hierarchical representations over the input.</p><p>nearby elements interact at lower layer,distant one at higher.</p><p>Compared with RNN,this structure provides a shorter path to capture long-range dependenceies.</p><p>我们可以在窗口大小为n通过捕捉关系来获得特征表达，CNN的O(n/k),k为kernel’s width,O(n) for RNN</p><h3 id="Q："><a href="#Q：" class="headerlink" title="Q："></a>Q：</h3><p>Bradbury et al. (2016) introduce <strong>recurrent pooling between a succession of convolutional layers</strong>,but doesn’t improve result.</p><p><strong>Gated CNN</strong> restricted the sixe of dataset and must be uesed with count-based model.</p><p>*<em>partially CNN *</em>shows well,but decoder is still recurrent.</p><h3 id="S"><a href="#S" class="headerlink" title="S:"></a>S:</h3><p>an architecture for seq2seq which is entirely convolutional.</p><p>included <strong>gated linear units,residual connection,attetion</strong> <strong>in decoder layer</strong>(control overhead)</p><h3 id="C"><a href="#C" class="headerlink" title="C:"></a>C:</h3><p>CNN模型相比于RNN在更大数据集上提升了一个数量级的速度，相比于RNN更能发现组合结构，依赖于门控以及multi-attention</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201010101323500.png" style="zoom:80%;"><blockquote><p>对CNN的理解</p><p>卷积公式：<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201010162826324.png" style="zoom: 50%;"></p><p>[^系统某一时刻的输出是由多个输入共同作用的结果]: </p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201010163005135.png" style="zoom:50%;"><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201013083349412.png" alt="计算第一个元素" style="zoom:50%;"><p>[^计算第二个元素只需将蓝色方框向右移动一列]: </p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201013085823114.png" style="zoom:67%;"><p>[^上图中间的卷积矩阵通过卷积运算可以看出输入矩阵是从暗变亮还是从亮变暗]: </p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201013090713446.png" style="zoom:67%;"><p>[^卷积矩阵可以用来当作水平或垂直边缘检测]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201013090157757.png" alt="Sobel filter"></p><p>[^中间一行的参数权重变大是为了增大鲁棒性]: </p><p><strong>可以将矩阵的九个参数全都通过神经网络去学习（反向算法），矩阵还可以进行翻转</strong></p><p>卷积核上所有作用点依次作用于原始像素点后（即乘起来），线性叠加的输出结果，即是最终卷积的输出，也是我们想要的结果，我们称为destination pixel.</p><h4 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h4><p>对输入矩阵进行卷积的时候，假设输入矩阵维数为n，卷积矩阵为f，则输出矩阵为n-f+1，f&gt;=1，所以输出会被缩小，这样边缘的特征会丢失。</p><p>引入padding，对输入矩阵四周进行扩充，元素为0.</p><blockquote><p>根据是否padding，将卷积分为Valid,Same。</p><ul><li>Vaild输出矩阵维数为n-f+1.</li><li>Same输出矩阵n+2p-f+1(p为padding的维数)</li></ul></blockquote><p><strong>要使input=output，p=(f-1)/2,即p为基数时才能均匀padding</strong></p><h4 id="Strided-convolution"><a href="#Strided-convolution" class="headerlink" title="Strided convolution"></a>Strided convolution</h4><p>卷积步长是s，即每次卷积后的输出矩阵为<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201013101555904.png" style="zoom:50%;"></p><p>three dimensional CNN</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201013102941304.png" style="zoom:67%;"><p>输出矩阵的通道数可以随着核的数量继续增加</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201013185315195.png" style="zoom: 50%;"><h4 id="Pooling-layer"><a href="#Pooling-layer" class="headerlink" title="Pooling layer"></a>Pooling layer</h4><ul><li><p>Max Pooling</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201013191203733.png" alt="image-20201013191203733"></p><p>如果在过滤器中提取出任何的特征则保留最大值，反之则不是最大值</p></li><li><p>Average pooling</p><p>通过计算方块特征值中的平均数来进行特征提取。效果没有Max pooling好。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201014090917968.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201014091116584.png" alt=""></p><p>POOL层无参数，卷积层参数较少，全连接层参数最多，随着层数增加，激活值减少。</p><p><strong>Why use CNN?</strong></p><p>通过共享参数和使用稀疏网络(防止过拟合)来减少参数。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201014093550570.png" alt=""></p><h4 id="经典网络"><a href="#经典网络" class="headerlink" title="经典网络"></a>经典网络</h4><p><strong>LetNet-5</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201014095914134.png" alt=""></p><p>[^早期pooling只有avg，人们早期使用Sigmoid/tanh,而非Relu]: </p><p><strong>AlexNet</strong></p><h5 id="残差网络（Residual-block"><a href="#残差网络（Residual-block" class="headerlink" title="残差网络（Residual block)"></a>残差网络（Residual block)</h5><p>神经网络的层数太深会导致梯度爆炸或梯度消失。残差网络，可以将从一层获取的激活值迅速反馈给其他层或是更深的网络。</p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201014102558076.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201014103039678.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201014103253269.png" alt=""></p></li></ul></blockquote><h5 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h5><h4 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h4><p>show the current position of the input or output seq.</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://arxiv.org/abs/1705.03122" target="_blank" rel="noopener">https://arxiv.org/abs/1705.03122</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzU0MDQyMzQ2MA==&amp;mid=2247483721&amp;idx=1&amp;sn=2cbebf2ba17c14d98da44ef477cc80dc&amp;chksm=fb382502cc4fac14cb962248d6dbc1cfdb840de189e634edd425ab9c87332b88f27c3f0d720c&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">https://mp.weixin.qq.com/s?__biz=MzU0MDQyMzQ2MA==&amp;mid=2247483721&amp;idx=1&amp;sn=2cbebf2ba17c14d98da44ef477cc80dc&amp;chksm=fb382502cc4fac14cb962248d6dbc1cfdb840de189e634edd425ab9c87332b88f27c3f0d720c&amp;scene=21#wechat_redirect</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MT </tag>
            
            <tag> CNN </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dynamaticonv</title>
      <link href="/2020/10/07/dynamaticonv/"/>
      <url>/2020/10/07/dynamaticonv/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《Pay Less Attention With Lightweight and Dynamic Convolutions](<a href="https://arxiv.org/abs/1901.10430)》" target="_blank" rel="noopener">https://arxiv.org/abs/1901.10430)》</a></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>self-attention在current time step中通过与每个元素比较决定每个文本元素的重要等级。paper中提出一种lightweight convolution以及一种dynamic convolution。</p></blockquote><p>We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic.</p><blockquote><p>separate convolution kernel主要基于当前time-step,操作次数与输入长度线性相关，而且self-attention是二次的。</p></blockquote><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h3><p> RNNs integrate context information by updating a hidden state at every time-step, CNNs summarize a fifixed size context through multiple layers, while as self-attention directly summarizes all context.</p><blockquote><p>RNN通过在每个时间点更新隐藏状态</p></blockquote><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007164744732.png" alt=""></p><p>[^Self-attention computes attention weights by comparing all pairs of elements to each other hile as dynamic convolutions predict separate kernels for each time-step (b).]: </p><p>本文介绍了lightweight convolution,它depth-wise separable,softmax-normalized and share weight over the channel dimension.轻量级卷积相比于标准卷积的权值少几个量级，对上下文重复使用相同的权值忽略当前的时间点。</p><p>Dynamic convolution基于轻量级卷积在每个time-step对不同的卷积核进行预测。但是权值不是在训练后固定不变的，而是被模型动态生成的</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MT </tag>
            
            <tag> CNN </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Scale attention-seq</title>
      <link href="/2020/10/06/multi-scale-attention-seq/"/>
      <url>/2020/10/06/multi-scale-attention-seq/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title"></a>Title</h3><p>《MUSE: PARALLEL MULTI-SCALE ATTENTION FOR SEQUENCE TO SEQUENCE LEARNING》</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><h4 id="Q"><a href="#Q" class="headerlink" title="Q:"></a>Q:</h4><p><strong>attention mechanism alone suffer from dispersed weights not suit long seq</strong></p><p><strong>1.the attention in deep layers tends to over-concentrate on a single token</strong></p><p><strong>2.insufficient use of local info</strong></p><p><strong>3.difficult in representing long seq</strong></p><h4 id="S"><a href="#S" class="headerlink" title="S:"></a>S:</h4><p>parallel multi-scale representation</p><blockquote><p><strong>condition:</strong></p><p>More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unifified semantic space.</p></blockquote><h3 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h3><p>Transformer mainly adept at seq tasks like <strong>MT,text classiification,language modeling.</strong></p><blockquote><p>It is solely based on an attention mechanism that captures global dependencies between input tokens, dispensing with recurrence and convolutions entirely. The key idea of the self-attention mechanism is updating token representations based on a weighted sum of all input representations.</p></blockquote><p>Transformer基于一种自注意力机制，这种机制可以通过递归和卷积来捕捉输入令牌之间的全局依赖，在所有输入表示的权值的基础上更新令牌。</p><p>但Transformer存在缺点，如图1所示，seq的长度与Transformer的表现成反比，因为注意力机制会被过度关注，分散，只有一小部分令牌能被注意力表示。可能会引起insufficient representation of info,hard to understand the source info for model.</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201006193814821.png" alt="图1" style="zoom:67%;"><p>尝试限制注意力只关注长序列的一部分，但是这种方法由于长距离依赖代价太高，也没有显示出seq to seq学习的有效性。是为建立一个具有局部和全局上下文建模的归纳偏差的模块，将self-attention与convolution结合，提出multi-scale attention called <strong>MUSE</strong>.attention用于捕捉依赖，convolution补偿局部信息的不足，并且有扩展性。</p><h3 id="2-MUSE：Parallel-Multi-Scale-Attention"><a href="#2-MUSE：Parallel-Multi-Scale-Attention" class="headerlink" title="2.MUSE：Parallel Multi-Scale Attention"></a>2.MUSE：Parallel Multi-Scale Attention</h3><p>MUSE has encoder-decoder framework,encoder has word embedding as input(x1,x2,…,xn),Muse transfer <strong>X</strong> to <strong>z</strong>,给定 <strong>z</strong>，解码器再生成文本序列<strong>y</strong>。encoder is a stack of N MUSE.decoder与encoder相似，decoder不仅在文本表示中捕捉特征，而且用additional context attention对输出的编码栈进行attention。</p><blockquote><p>MUSE有三个主要部分：<strong>self-attention(global feature),depth-wise separable convolution(local),position-wise feed-forward network(token).</strong></p></blockquote><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007092906860.png" style="zoom: 50%;"><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007093105999.png" alt="MUSE的i-1层来预测i层" style="zoom:67%;"><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007093203776.png" alt="MUSE-simple" style="zoom: 67%;"><h4 id="2-1-Attention-Mechanism"><a href="#2-1-Attention-Mechanism" class="headerlink" title="2.1 Attention Mechanism"></a>2.1 Attention Mechanism</h4><p>给定输入seq X，转换成K,Q,V，再用self-attention得到输出。</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007095521548.png" style="zoom: 67%;"><p>self-attention is dot-production:<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007095833650.png" alt="image-20201007095833650" style="zoom:50%;"></p><h4 id="2-2Convolution"><a href="#2-2Convolution" class="headerlink" title="2.2Convolution"></a>2.2Convolution</h4><p><strong>depth-wise convolution</strong> has 2 transformers:<strong>point-wise projecting,contextual transformation.</strong></p><p>每一个convolution的子模块包含不同大小的单元，用来捕捉不同范围的特征。</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007101651797.png" style="zoom:50%;"><p>select the weight of different convolution cells.</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007103123354.png" style="zoom:50%;"><h5 id="Shared-projection"><a href="#Shared-projection" class="headerlink" title="Shared projection"></a>Shared projection</h5><p>the shared projection project the input feature into the same hidden space.</p><h5 id="2-3Point-wise-feed-forward-network"><a href="#2-3Point-wise-feed-forward-network" class="headerlink" title="2.3Point-wise feed-forward network"></a>2.3Point-wise feed-forward network</h5><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201007104321435.png" style="zoom:50%;"><p>learn token level representation,MUSE将self-attention与feed-forward连接，不同位置线性变化相同，所以位置前馈网路可用于提取token</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MT </tag>
            
            <tag> NLP </tag>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deep transformer-NMT</title>
      <link href="/2020/10/06/deep-transformer-nmt/"/>
      <url>/2020/10/06/deep-transformer-nmt/</url>
      
        <content type="html"><![CDATA[<h3 id="Title"><a href="#Title" class="headerlink" title="Title:"></a>Title:</h3><p>《<a href="https://arxiv.org/abs/2008.07772" target="_blank" rel="noopener">Very Deep Transformers for Neural Machine Translation</a>》</p><h4 id="Q："><a href="#Q：" class="headerlink" title="Q："></a>Q：</h4><p><strong>how to decrease the variance of the output layer in order to train deep Trsnaformer for NMT?</strong></p><h4 id="S"><a href="#S" class="headerlink" title="S:"></a>S:</h4><p><strong>use a initialization named “ADMIN” to remedy the variance problem and stabilize training</strong></p><h4 id="C"><a href="#C" class="headerlink" title="C:"></a>C:</h4><p><strong>outperform their 6-layer baseline,with up to 2.5BLEU improvement.</strong></p><blockquote><p>The code and trained models will be publicly available at: <a href="https://github.com/namisan/exdeep-nmt" target="_blank" rel="noopener">https://github.com/namisan/exdeep-nmt</a>.</p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MT </tag>
            
            <tag> Admin </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Back-Translation</title>
      <link href="/2020/10/06/back-translation/"/>
      <url>/2020/10/06/back-translation/</url>
      
        <content type="html"><![CDATA[<h3 id="title-《Understanding-Back-Translation-at-Scale》"><a href="#title-《Understanding-Back-Translation-at-Scale》" class="headerlink" title="title:《Understanding Back-Translation at Scale》"></a>title:《Understanding Back-Translation at Scale》</h3><h4 id="Q"><a href="#Q" class="headerlink" title="Q:"></a>Q:</h4><p><strong>how to improve neural machine translation with monolingual data?</strong></p><h4 id="S"><a href="#S" class="headerlink" title="S:"></a>S:</h4><p><strong>augment the parallel training corpus with back-tranalations of targetlanguage sentences.</strong></p><h4 id="C"><a href="#C" class="headerlink" title="C:"></a>C:</h4><p><strong>sampling or noisy synthetic data better.</strong></p><p><strong>new state of the art of 35 BLEU  on the WMT’14 English-German test set.</strong></p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><blockquote><p>​        We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. </p></blockquote><p>我们发现在所有资源设置缺失的反向翻译中，通过采样或者噪声束输出的是最有效的，相比于波束或者是贪婪搜索。</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>MT通常依赖于大型平行数据集的统计（paired sentences in both the source and target),但是bitext有限，monolingual data有很多，所以开始利用monolingual data。可以使用language model fusion,back-translation,dual learning来进行优化。</p><p>我们的重点是BT，在semi-supervised setup中双语单语数据在目标语言中都是可用的。</p><p>首先回译在平行数据训练出一个中间系统，，结果是生成一个平行数据库，它的源端是合成的机器翻译，输出是人类书写的真正的文本。这个合成好的加入真正的bitext为了训练最终的系统。</p><p><strong>对基于词的翻译,NMT,unsupervised MT很有作用</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MT </tag>
            
            <tag> Transformer </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Task-Oriented Dialogue as Dataflow Synthesis</title>
      <link href="/2020/09/28/task-oriented-dialogue/"/>
      <url>/2020/09/28/task-oriented-dialogue/</url>
      
        <content type="html"><![CDATA[<h3 id="《Task-Oriented-Dialogue-as-Dataflow-Synthesis》"><a href="#《Task-Oriented-Dialogue-as-Dataflow-Synthesis》" class="headerlink" title="《Task-Oriented Dialogue as Dataflow Synthesis》"></a>《Task-Oriented Dialogue as Dataflow Synthesis》</h3><h5 id="一、对话系统的问题和挑战"><a href="#一、对话系统的问题和挑战" class="headerlink" title="一、对话系统的问题和挑战"></a>一、对话系统的问题和挑战</h5><ol><li><p>短句包括多个指示。</p></li><li><p>人类语言具有长尾性，无法使用高频意图覆盖</p><p>[^长尾性：系统中的个体相差悬殊，缺乏优选]: </p></li><li><p>多轮对话引用修正加大数据收集难度</p></li><li><p>同一句话跨多个领域难处理</p></li></ol><h5 id="二、程序合成"><a href="#二、程序合成" class="headerlink" title="二、程序合成"></a>二、程序合成</h5><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200929085941018.png" style="zoom: 67%;"><p>对函数API进行组合实现更复杂的功能,程序难以标注和解析,所以可以设计一种对话系统专用的机器语言,通过提高与自然语言的相似度降低标注的难度.</p><h5 id="三-Abstract"><a href="#三-Abstract" class="headerlink" title="三   Abstract"></a>三   Abstract</h5><p>将对话状态表示为数据流图的面向任务对话方法,包括引用和修订</p><h5 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h5><p>任务属于交互编程,对话状态用数据流图表示,agent将每次对话翻译成一个程序,扩展流图再描述结果.(自动编写程序)</p><p>作者用一幅有向图表示了完整的执行过程，这幅图叫做<strong>数据流图(Dataflow Graph)</strong>，其中<strong>节点</strong>代表数值或函数（灰色节点代表初始程序，而蓝色节点代表程序的执行结果），<strong>实线</strong>从参数指向使用它的函数，<strong>虚线</strong>从函数指向它的结果。数据流图记录了所提到的实体及其之间的关系。<strong>数据流图中的所有节点都可以被后续的话语引用</strong>。有时，函数的执行结果本身也是一段程序。例如 isCold 的执行结果是 “weatherForecast.temperate &lt; ColdThreshold”，而最终结果是 False。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MT </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MT book</title>
      <link href="/2020/09/20/mt-book/"/>
      <url>/2020/09/20/mt-book/</url>
      
        <content type="html"><![CDATA[<h2 id="统计机器翻译"><a href="#统计机器翻译" class="headerlink" title="统计机器翻译"></a>统计机器翻译</h2><h3 id="3-基于词的机器翻译模型"><a href="#3-基于词的机器翻译模型" class="headerlink" title="3.基于词的机器翻译模型"></a>3.基于词的机器翻译模型</h3><h4 id="3-1-词在翻译中的作用"><a href="#3-1-词在翻译中的作用" class="headerlink" title="3.1 词在翻译中的作用"></a>3.1 词在翻译中的作用</h4><p>先把句子中的每个词对应翻译后重新排序。包括三个步骤：分析、转换、生成。</p><h4 id="3-2构建简单的机器翻译系统"><a href="#3-2构建简单的机器翻译系统" class="headerlink" title="3.2构建简单的机器翻译系统"></a>3.2构建简单的机器翻译系统</h4><blockquote><p>人为翻译：翻译知识的学习（翻译候选），运用知识生成译文（处理常见的单词搭配、主谓一致的问题，生成译文）</p></blockquote><p>机器翻译：</p><p>1.获得每个单词的译文，拼装成句</p><p>翻译词典，对单词进行进行拼装，相当于贯穿路径，计算机生成尽可能多的路径。</p><p>2.评价译文的好坏</p><p>设计统计模型，计算单词的翻译概率，最后得到整句的概率。</p><p>[^训练模型=学习知识]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920190916668.png" alt="机器翻译的过程"></p><p>统计机器翻译的核心：建模、训练和解码</p><h5 id="基本框架"><a href="#基本框架" class="headerlink" title="基本框架"></a>基本框架</h5><blockquote><p>很多思想来自IBM模型</p></blockquote><p>1.训练：从双语平行数据中学习翻译，记为P(t|s),表示把s翻译为t的概率。</p><p>2.解码：为每一个翻译结果打分，最后选择得分最高的输出</p><h6 id="如何从双语平行数据中学习？"><a href="#如何从双语平行数据中学习？" class="headerlink" title="如何从双语平行数据中学习？"></a>如何从双语平行数据中学习？</h6><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920193749247.png" alt=""></p><p>[^X,Y分别表示源语言和目标语言的词汇表。任意x∈X，所有的y∈Y都可能是它的译文P(x&lt;-&gt;y;s,t)在观测到（s,t)的前提下X和y互译的概率。x是句子中t中的词，y属于句子t中的词。分子表示x和y在句子(s,t)中出现的次数，分母是任意x’和任意y‘在(s,t)共同出现的次数。]: </p><p>出现零概率使用平滑算法。</p><h6 id="如何从大量的双语平行数据中学习？"><a href="#如何从大量的双语平行数据中学习？" class="headerlink" title="如何从大量的双语平行数据中学习？"></a>如何从大量的双语平行数据中学习？</h6><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920195634234.png" alt=""></p><p>[^分子分母多了一项累加符号，表示遍历所有的句对。]: </p><p>句子级翻译模型</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920201250924.png" alt=""></p><h5 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h5><p>句子级翻译并不简单，定义新函数g(s,t),g(s,t)与翻译概率正相关。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920201940286.png" alt=""></p><p>词对齐</p><p>用二元组(j,i)来描述词对齐，即原文与译文之间对应词语的位置编号。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920222526109.png" alt=""></p><p>[^g(s,t):句子s中的单词和句子t中的单词翻译概率的乘积。g(s,t)越高，词对齐越准确，s和t之间存在翻译关系的可能性越大。]: </p><h5 id="生成流畅的译文"><a href="#生成流畅的译文" class="headerlink" title="生成流畅的译文"></a>生成流畅的译文</h5><blockquote><p>上文的公式没有考虑词序信息，引入n-gram语言模型（确保流畅翻译结果）。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920223725202.png" alt=""></p><p> Plm(t)和g(s,t)相乘，这样就得到了一个新的g(s,t),同时考虑翻译准确性和流畅度。</p></blockquote><h5 id="解码（Decoding"><a href="#解码（Decoding" class="headerlink" title="解码（Decoding)"></a>解码（Decoding)</h5><blockquote><p>解码系统要找到翻译概率最大的目标译文t，<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920224851214.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920224956633.png" alt=""></p><p>依次计算g(s,t)不可行，源语句s有m个词，每个词有n个可能的翻译候选，则有n的m次方种组合，翻译结果又有m！种排序，至少有<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920230110650.png" style="zoom: 67%;">不同个译文。</p></blockquote><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200920230221099.png" alt=""></p><p>[^机器翻译是NP问题,该算法的思想是贪心算法，系统每次都保存当前最好的结果，然后进一步保存扩展后的所有可能，计算得分后保存最好结果。]: </p><h4 id="3-3基于词的翻译建模"><a href="#3-3基于词的翻译建模" class="headerlink" title="3.3基于词的翻译建模"></a>3.3基于词的翻译建模</h4><blockquote><p>空翻译？调序问题建模？数学模型描述翻译？更复杂的统计模型训练？</p></blockquote><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921083856909.png" alt="IBM模型的统计学基础"></p><p>IBM模型基础是噪声信道，已知s和信道的性质，可以通过P(t|s)得到信源的信息，找到最可能的目标语句的过程被称为解码。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921085322560.png" alt=""></p><p>[^注意翻译的方向改变，P(s|t)而非P(t|s),P(t)是目标语句t出现的可能，P(s)是源句子s出现的可能性。引入P(s|t)和P(t)的原因是单纯使用P(t|s)会得到翻译对应比较好但不合法的语句。]: </p><h6 id="3-3-2三个基本问题"><a href="#3-3-2三个基本问题" class="headerlink" title="3.3.2三个基本问题"></a>3.3.2三个基本问题</h6><blockquote><p>建模：建立P(s|t)和P(t)的模型</p><p>训练：获得P(s|t)和P(t)所需的参数</p><p>解码：完成搜索最优解的过程，完成argmax</p></blockquote><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921092130147.png" alt=""></p><p>[^红的部分表示译文的可能性，蓝色表示译文的流畅程度]: </p><h5 id="词对齐"><a href="#词对齐" class="headerlink" title="词对齐"></a>词对齐</h5><p>源语言句子和目标语言单词之间的对应。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921132515018.png" alt=""></p><p>[^这里的词对齐是非对称词对齐，只对源语言做了约束，目标语言没有，如果没有对齐的目标词，就定义一个t0，引入空对齐的思想，该思想在虚词的翻译中体现较多]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921132654678.png" alt=""></p><h5 id="基于词对齐的翻译模型"><a href="#基于词对齐的翻译模型" class="headerlink" title="基于词对齐的翻译模型"></a>基于词对齐的翻译模型</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921134100659.png" alt="3.18"></p><p>[^引入隐含变量可将困难的问题转化为分步学习问题，aj表示第j个源单词对应到目标语单词的位置。将a视为隐含变量从t生成s就变成从t同时生成s和隐含变量的问题。]: </p><h5 id="因为隐含变量啊很复杂，所以使用链式法则。"><a href="#因为隐含变量啊很复杂，所以使用链式法则。" class="headerlink" title="因为隐含变量啊很复杂，所以使用链式法则。"></a>因为隐含变量啊很复杂，所以使用链式法则。</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921143714782.png" alt="3.19"></p><blockquote></blockquote><p>源语言句子：“在桌子上”，目标语“on the table”</p><h4 id="3-4IBM模型1-2"><a href="#3-4IBM模型1-2" class="headerlink" title="3.4IBM模型1-2"></a>3.4IBM模型1-2</h4><p>[^词对齐的数量随着句子长度呈指数增长，如何遍历所有的对齐a，如何计S算P(m|t)、P(aj|aj−11,sj−11,m,t) 和 P(sj|aj1,sj−11,m,t)，提出了5个IBM模型.]:<br>[^对于每个位置 j，根据译文 t、源文长度 m、已经生成的源语言单词 sj−11和对齐 aj−11，生成第 j 个位置的对齐结果 aj，用 P(aj|aj−11,sj−11,m,t) 表示；]:<br>[^• 对于每个位置 j，根据译文 t、源文长度 m、已经生成的源语言单词 sj−11和对齐 aj1，生成第 j 个位置的源语言单词 sj，用 P(sj|aj1,sj−11,m,t) 表示]: </p><h5 id="3-4-1IBM模型1"><a href="#3-4-1IBM模型1" class="headerlink" title="3.4.1IBM模型1"></a>3.4.1IBM模型1</h5><blockquote><p>IBM1模型对公式3.19进行简化。</p><ul><li><p>假设P(m|t)为常数 ε，源语言的长度的生成概率服从均匀分布，</p></li><li><p>对齐概率仅依赖于译文长度l，每个词对齐连接的概率服从均匀分布<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921202647088.png" alt=""></p></li><li><p>源语单词sj的生成概率及依赖于对齐的译文单词<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200921202949264.png" alt=""></p><p>将三式和公式3.19代入公式3.18中</p><p>![3.24](<a href="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922085836651.pn" target="_blank" rel="noopener">https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922085836651.pn</a></p></li></ul></blockquote><h5 id="3-4-2IBM模型2"><a href="#3-4-2IBM模型2" class="headerlink" title="3.4.2IBM模型2"></a>3.4.2IBM模型2</h5><p>IBM模型1简化了问题，但假设太强，但如果词对齐的生成概率服从均匀分布，模型会忽略目标语言单词的位置信息，IBM2考虑了这一点，生成位置aj与j、m、l有关。<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922093259937.png" alt="3.26"></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922093712006.png" alt=""></p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20200922102502158.png" alt="3.28"></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922102636925.png" alt="3.29"></p><p>[^这样要执行O((l+1)^m)如果将计算式改为3.29，则计算复杂度降为O((l+1)m,先乘后加改为先加后乘。]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922102927802.png" alt="IBM1和IBM2模型的最终公式"></p><h6 id="3-4-4训练"><a href="#3-4-4训练" class="headerlink" title="3.4.4训练"></a>3.4.4训练</h6><p>完成建模与解码，接下来要进行设置参数，最重要的。训练的过程就是在给定数据集上调整参数得到最优解。可以把优化目标视为对似然函数的优化。可以描述为对目标函数Pθ(s|t)的优化过程。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922143442385.png" alt=""></p><h6 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h6><p>解决这类问题的常用方法是把带约束的目标函数转化为不带约束的目标函数。用到的是拉格朗日乘数法，</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922145932623.png" alt=""></p><p>调整为无约束的函数后通过求偏导，再更新一组参数。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922151830537.png" alt="">这种方法叫期望最大化，简称EM方法</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922152802112.png" alt=""></p><h4 id="3-5IBM模型及隐马尔可夫模型"><a href="#3-5IBM模型及隐马尔可夫模型" class="headerlink" title="3.5IBM模型及隐马尔可夫模型"></a>3.5IBM模型及隐马尔可夫模型</h4><p>[^采用更细致的建模方法，引入产出率、单词的抽象，隐马尔科夫模型和IBM有一定的联系但是从另一个视角看翻译问题。]: </p><p>IBM1-2的基础上把奴痛的源语言单词看作相互独立的单元进行词对齐和翻译，这里提出另一个模型。首先确定每个目标语言单词生成源语言单词的个数，再决定译文中的每个单词生成的源语言单词，形成了一个列表。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922155322609.png" alt=""></p><p>[^蓝色：生成元语言单词的个数eg:scientists有两个含义“科学家”和”们“，红色用来存储可以用Π来表示元语言单词存储的位置eg：{Π1}={Π11=1，Π12=2}]:<br>[^s、t、m 和 l 分别表示源语言句子、目标语译文、源语言单词数量以及译文单词数量。φ、τ 和 π 分别记录产出率、生成的源语言单词以及它们在源文中的位置。]: </p><p>​        一组 τ 和 π（记为 &lt; τ,π &gt;）可以决定一个对齐 a 和一个源语句子 s。相反的，一个对齐 a 和一个源语句子 s 可以对应多组 &lt; τ,π &gt;，这里把不同的 &lt; τ, π &gt; 对应到的相同的源语句子 s 和对齐 a 记为 &lt; s,a &gt;。因此计算 P(s,a|t) 时需要把每个可能结果的概率加起来，如下：<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922193831542.png" style="zoom: 50%;">，&lt;s,a&gt;包含<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922194124464.png" style="zoom:50%;">个不同的二元组&lt; τ,π &gt;</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200922194713534.png" alt="图3.31"></p><p>s、t、m 和 l 分别表示源语言句子、目标语译文、源语言单词数量以及译文单词数量。φ、τ 和 π 分别记录产出率、生成的源语言单词以及它们在源文中的位置。</p><p>[^红色：目标单词产出率建模，即 φi的概率。绿色：词汇翻译建模 。黄色：[1, l] 的目标语言单词生成的源语言单词的扭曲度。灰色：i = 0 时的扭曲度建模，即空标记 t0生成的源语言单词在源语言句子中位置的概率。]</p><h4 id="IBM模型3"><a href="#IBM模型3" class="headerlink" title="IBM模型3"></a>IBM模型3</h4><p>​        IBM模型3通过一些假设对图3.31进行化简</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923084514173.png" style="zoom: 50%;"><p>在已经放置了k个空对齐源语言单词的时候,应该 φ0-k个空位置,如果第i个位置为空,<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923090956081.png" alt="image-20200923090956081" style="zoom: 50%;">,,对于上式就有<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923091112876.png" alt="image-20200923091112876" style="zoom: 33%;"></p><p>在IBM模型3中假设在所有的非空对齐源语言单词都被生成出来后.这些单词都以p1概率随机产生”槽”来放置空对齐单词.这样φ0就服从了一个二项分布.</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923091727669.png" alt="image-20200923091727669" style="zoom: 50%;">,可得到P(s|t)的表达式<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923092414928.png" style="zoom: 50%;"></p><h4 id="IBM模型4"><a href="#IBM模型4" class="headerlink" title="IBM模型4"></a>IBM模型4</h4><p>IBM1-3中都不能很好地处理一个目标语言对应多个源语言单词的情况,因为相同目标语言对应多个源语言那么这些源语言单词往往构成短语或搭配.,但是模型1-3把这些源语言单词堪称独立的单元</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923095840159.png" alt=""></p><p>[^概念单元或概念(concept)是具有独立语法的一组词语,源语言和目标语言的cept不一定相等,在IBM词框架下,每个cept只能由一个目标语组成[i]表示位置⊙i表示位置为[i]的目标语对应源语言位置的平均值]: </p><p>模型4对3进行修改,主要把扭曲度分解为两类参数,[i]对应源语言单词列表(τ[i]) 中的第一个单词 (τ[i]1）<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923102229923.png" alt="image-20200923102229923" style="zoom:50%;"></p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923101722374.png" alt="image-20200923101722374" style="zoom:50%;"><p>[^这里的函数A,B分别把目标语言和源语言的单词影射到单词的词类,这样可以减少数据的稀疏程度也可以减少参数空间的大小]: </p><p>IBM4模型的过程就是将t[i]生成的第一个源语言单词代表整个t[i]生成的单词列表,先把第一个源语言单词放在合适的位置,再根据相对距离放置其他单词,这样就保证了同一目标单词生成的源语言单词之间可以互相影响</p><h4 id="IBM模型5"><a href="#IBM模型5" class="headerlink" title="IBM模型5"></a>IBM模型5</h4><p>模型34都有可能将一部分概率分配给一些不存在的句子(缺陷Deficiency),因为没有这样的约束,如果已经放置了某个源语言单词的位置不能防止其他词放入同样的位置.,模型5针对这个问题增加了约束,即预先判断目标位置是否为空</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923104437724.png" alt=""></p><h5 id="3-5-5隐马尔科夫模型-Hidden-Markov-Model，HMM"><a href="#3-5-5隐马尔科夫模型-Hidden-Markov-Model，HMM" class="headerlink" title="3.5.5隐马尔科夫模型(Hidden Markov Model，HMM)"></a>3.5.5隐马尔科夫模型(Hidden Markov Model，HMM)</h5><p>隐马尔科夫模型是针对IBM2模型的改进版.描述一个系统,隐含状态的转移和可见状态的概率.</p><p>HMM包括三个问题:</p><ol><li>估计：给定模型根据可见状态链计算该结果概率。（前后向算法）</li><li>参数学习：给定隐含状态数量，根据多个可见状态链估计模型的参数（EM算法）</li><li>解码问题：给定模型和可见状态链，计算最可能出现的状态序列。(Viterbi Algorithm,动态规划）</li></ol><blockquote></blockquote><p>HMM词对齐模型，词语与词语之间并不是毫无联系的，对其概率取决于对齐位置的差异。不是所有的模型使用EM算法都能找到全局最优解，IBM模型1是一个凹凸函数，理论上使用EM方法是能找到全局最优解的。IBM1是凸函数，理论上使用EM方法能找到全局最优解，IBM1训练结果与参数初始化过程无关</p><h3 id="基于短语和句法的机器翻译模型"><a href="#基于短语和句法的机器翻译模型" class="headerlink" title="基于短语和句法的机器翻译模型"></a>基于短语和句法的机器翻译模型</h3><h4 id="4-1翻译中的结构信息"><a href="#4-1翻译中的结构信息" class="headerlink" title="4.1翻译中的结构信息"></a>4.1翻译中的结构信息</h4><p>相较于基于单词的机器翻译模型，如果在翻译红茶时很大概率会翻译成“red tea”而非”black tea”,如果机器翻译系统能记住这样的搭配就可以做得更好。</p><p>使用短语的优点在于引入更大颗粒度的翻译单元给建模增加了灵活性，同时增大了翻译假设空间</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923164311326.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200923164340632.png" alt="image-20200923164340632"></p><h4 id="4-2基于短语的翻译模型"><a href="#4-2基于短语的翻译模型" class="headerlink" title="4.2基于短语的翻译模型"></a>4.2基于短语的翻译模型</h4><p>定义4.2.1对于一个句子W=w1w2w3…wn,任意子串w1…wj（i&lt;=j且0&lt;=i,j&lt;=n)都是句子W的一个短语</p><p>定义4.2.2如果一个句子W=w1w2….wn可以被分为m个子串，则称W由m个短语组成，记W=p1..pm,pi是W的一个短语，p1,,pm被称为句子W的一个切分</p><p>定义4.2.3双语短语</p><p>源语和目标语句对(s,t),s和t中的任意一个短语都可以构成一个双语短语对，简称短语对，&lt;-&gt;表示互译关系</p><p>定义4.2.4基于短语的翻译推导</p><p>{aj}表示{tj}中每个短语对应到源语言短语的编号，构成了s到t的基于短语的翻译推导</p><p>有四个基本问题</p><ol><li>用统计模型描述每个翻译推导的好坏——翻译的统计建模问题</li><li>获得双语短语对</li><li>对调序问题建模</li><li>找到输入句子s的最佳译文</li></ol><h5 id="4-2-2数学建模及判别式模型"><a href="#4-2-2数学建模及判别式模型" class="headerlink" title="4.2.2数学建模及判别式模型"></a>4.2.2数学建模及判别式模型</h5><p>统计学习的目标是找到生成概率最大的译文即t=argmax(P(t|s)),可以把翻译推导d当作从s到t翻译的隐含结构。P(t|s)=ΣP(d,t|s),因为空间D中d数量太大枚举很费时间，所以可以选择最好的n个翻译推导来代表整个空间D，<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924085306350.png" style="zoom: 50%;"></p><p>另一种方法是用P(d,t|s)的最大值代表整个翻译推导的概率和，翻译的目标可以被重新定义为<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924085750248.png" style="zoom: 50%;"></p><h5 id="对数线性模型"><a href="#对数线性模型" class="headerlink" title="对数线性模型"></a>对数线性模型</h5><p>how to denifite P(d,t|s)?可以使用判别式模型对P(d,t|s)进行描述。<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924091009590.png" style="zoom:80%;"></p><p>[^每个特征用函数 hi(d,t,s) 表示。每个特征都对应一个权重 λi，这样值越大得分越高，最大的好处在于它可以更灵活的引入特征。]: </p><h5 id="搭建模型"><a href="#搭建模型" class="headerlink" title="搭建模型"></a>搭建模型</h5><p>设计特征函数+获得最好的特征权重{ λi}</p><h5 id="4-2-3短语抽取"><a href="#4-2-3短语抽取" class="headerlink" title="4.2.3短语抽取"></a>4.2.3短语抽取</h5><blockquote><p>获得短语翻译的常用方法，图中任意一个矩阵框都是双语短语，为了提高效率获得有效的短语对</p></blockquote><p>定义4.2.5与词对齐一致的双语短语</p><p>对于源语言句子 s 和目标语句子 t，存在 s 和 t 之间的词对齐。如果有 (s,t) 中的双语短语 (s,t)，且 s 中所有单词仅对齐到t 中的单词，同时t 中所有单词仅对齐到s 中的单词，那么称 (s,t) 与是与词对齐一致的（兼容的）双语短语。</p><h6 id="获取词对齐"><a href="#获取词对齐" class="headerlink" title="获取词对齐"></a>获取词对齐</h6><p>IBM模型获得的模型是不对称的，为了获得对称的，先前向后反向再取交并集</p><p>词对齐质量可以用词对齐错误率AER评价</p><h5 id="度量双语短语质量"><a href="#度量双语短语质量" class="headerlink" title="度量双语短语质量"></a>度量双语短语质量</h5><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924101902928.png" alt="image-20200924101902928" style="zoom:50%;"><p>[^分母是短语s在S中出现的次数，count(s,t)在（S,T）中被抽取出来的次数。]: </p><p>这样对于低频短语不合理，但是可以把短语拆解为单词，简介度量，可以使用词汇话翻译概率，词对齐信息本身就包含了<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924162210374.png" alt=""></p><h5 id="调序"><a href="#调序" class="headerlink" title="调序"></a>调序</h5><p>为了使得译文质量提高，在双语短语之外还需要进行调序，调序包括基于距离、基于方向(WSD)以及基于分类。</p><blockquote><p>基于距离：度量当前结果与顺序翻译之间的差距</p><p>基于方向的调序模型:顺序(M)：monotone,源与目标的顺序相同,与前一个短语交换位置（S）：swap,对应的短语顺序在目标语中是相反的,非连续翻译（D）：discontinuous,两个短语之间有其他的短语</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924180756803.png" style="zoom: 50%;"><p>基于分类的调序</p><p>使用最大熵、SVM等分类器输出调序概率</p></blockquote><h5 id="4-2-6-最小错误率训练（Minimum-Error-Rate-Training-WERT"><a href="#4-2-6-最小错误率训练（Minimum-Error-Rate-Training-WERT" class="headerlink" title="4.2.6 最小错误率训练（Minimum Error Rate Training,WERT)"></a>4.2.6 最小错误率训练（Minimum Error Rate Training,WERT)</h5><blockquote><p>在统计机器翻译中，短语抽取和翻译概率被看作是模型训练，特征权重的训练，被称作权重调优。</p></blockquote><p>[^假设翻译结果相对于标准答案的错误是可度量的，通过降低错误数量的方式来找到最优的特征权重]: </p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924200101033.png" style="zoom: 50%;"><p>[^所有样本的参考译文集合为R={r1,r2,..,rN},通过调整不同特征的权重λ = {λi}，让错误率最小Error(),Error可以是WER,PER,BLEU,NIST。]: </p><p>BLEU本身是不可微分函数，无法梯度下降，可以使用线搜索</p><p>横坐标为所有的M个特征函数，纵坐标为权重可能的取值，遍历所有的特征权重取值的组合有<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924201100968.png" style="zoom:33%;">种。假设计算BLEU的时间开销为B，那么遍历所有的路径时间复杂度为<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924201220445.png" alt="image-20200924201220445" style="zoom:33%;">,对全搜索的改进可以是局部搜索，循环处理每个特征，每一次只调整一个特征值，找到使BLEU达到最大的权重，这种方法被称为格搜索。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200924200645158.png" alt=""></p><p>[^BLEU(Bilingual Evaluation Understudy双语评估替补）]: </p><p>假设对于每个输入的句子，翻译模型生成了两个推导(d1,d2),score(d)表示成关于第i个特征值的权重λi的线性函数</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200925134420414.png" alt=""></p><p>在交叉点右边d1是最优的翻译结果，左侧是d2是最优的结果，只需要比较左侧右侧BLEU谁高就好</p><h5 id="4-2-7栈解码"><a href="#4-2-7栈解码" class="headerlink" title="4.2.7栈解码"></a>4.2.7栈解码</h5><p>解码的目的是根据模型和输入找到模型得分最高的推导。机器翻译被证明是一个NP问题，无法简单暴力搜索，介绍一种基于栈的从左到右的方法。</p><h5 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h5><p>在句子变长时，有可能空间爆炸，可以使用束剪枝，即思路一：在每次翻译扩展时最多生成k个新的翻译假设，对束宽度进行限制，也称为直方图剪枝。思路二：每次扩展只保留与最优翻译假设得分相差在δ 之内的翻译假设。即阈值剪枝</p><h5 id="假设重组"><a href="#假设重组" class="headerlink" title="假设重组"></a>假设重组</h5><p>对翻译假设进行的重新组合，把代表同一个译文的不同翻译假设融合为一个翻译假设，两个不同的翻译假设需要舍弃分数较低的</p><h5 id="解码中的栈结构"><a href="#解码中的栈结构" class="headerlink" title="解码中的栈结构"></a>解码中的栈结构</h5><p>因为质量较差的翻译假设在早期出现时，会被剪枝，这样可以减少搜索空间，但是删除的翻译假设可能又被重新搜索出来，过早地删除翻译假设也可能导致无法搜索到最优。</p><p>所以我们可以引入栈结构，对翻译假设进行整理，当放入栈的翻译假设超过一定阈值可以删除模型得分低的，每个栈代表覆盖源语言单词数量相同的翻译假设，翻译相同数量的单词所对应的翻译假设</p><blockquote><p>翻译相同数量的单词所对应的翻译假设一般是 “可比的”，因此</p><p>在同一个栈里对它们进行剪枝带来的风险较小。</p></blockquote><h4 id="基于层次短语的模型"><a href="#基于层次短语的模型" class="headerlink" title="基于层次短语的模型"></a>基于层次短语的模型</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200925151135776.png" alt="">单词间的距离有时需要距离很远的搭配，但是如果使用长短语来解决长距离依赖问题。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200925151748605.png" alt=""></p><p>[^左右连接的方框表示翻译模板的源语言和目标语言中的变量被同步替换。]: </p><h5 id="同步上下文无关法（Synchronous-Context-free-Grammar，SCFG"><a href="#同步上下文无关法（Synchronous-Context-free-Grammar，SCFG" class="headerlink" title="同步上下文无关法（Synchronous Context-free Grammar，SCFG)"></a>同步上下文无关法（Synchronous Context-free Grammar，SCFG)</h5><p>很好的解决短语系统对翻译中长距离调序建模不足的问题。</p><h5 id="胶水规则"><a href="#胶水规则" class="headerlink" title="胶水规则"></a>胶水规则</h5><p>在实际系统中需要把两个局部翻译拼接到一起。需要引入胶水规则<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926085151863.png" style="zoom: 33%;"></p><p>引入一个新的非终止符S，S只能和X顺序拼接，或者S由X生成，胶水规则将句子都划分为若干部分，每个部分都被归纳为X</p><h5 id="处理流程"><a href="#处理流程" class="headerlink" title="处理流程"></a>处理流程</h5><p>双语数据中学习同步翻译语法，进行翻译特征的学习）规则+特征即为翻译模型，从目标语言数据中学习语言模型，将二者一起送入解码器。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926090100559.png" alt=""></p><h5 id="4-3-2层次短语规则抽取"><a href="#4-3-2层次短语规则抽取" class="headerlink" title="4.3.2层次短语规则抽取"></a>4.3.2层次短语规则抽取</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926091954366.png" alt=""></p><p>通过挖槽的方法来进行大量的层次短语抽取，但是我们需要进行限制，包括：抽取的规则最多可以跨越 10 个词； 规则的（源语言端）变量个数不能超过 2； 规则的（源语言端）变量不能连续出现</p><h5 id="4-3-3翻译模型及特征"><a href="#4-3-3翻译模型及特征" class="headerlink" title="4.3.3翻译模型及特征"></a>4.3.3翻译模型及特征</h5><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926092550184.png" style="zoom:80%;"><p>对每一条翻译规则有:</p><ul><li><p>(h1-2)短语翻译概率</p></li><li><p>(h3-4)词汇话翻译概率</p></li><li><p>(h5)翻译规则数量</p></li><li><p>(h7)短语规则数量</p><p>最终模型得分为：</p></li></ul><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926093110615.png" alt=""></p><p>[^log表示语言模型得分，|t|表示译文长度]: </p><h5 id="4-3-4CKY解码"><a href="#4-3-4CKY解码" class="headerlink" title="4.3.4CKY解码"></a>4.3.4CKY解码</h5><p>CKY方法可以看作是基于二叉规则的一种分析方法，从小范围开始不断扩大，最终完成整个字符串的分析，跨度是表示从一个起始位置到一个结束位置中间的部分。</p><blockquote><p>CKY方法跨度由小到大的次序执行，是一种自上而下的分析。对每个跨度要检查：</p><ul><li><p>是否有形如 A→a 的规则可以匹配</p></li><li><p>是否有形如 A→BC 的规则可以匹配。</p><p>[^第一种匹配字符串即可第二种需要将当前跨度分为两部分，左边归纳为B右边为C]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926095656136.png" alt="CKY算法伪代码"></p><p>还有两个实践问题：</p><p>1.剪枝：CKY中每个跨度的推导呈指数关系，存储有困难所以使用束剪枝，只保留top-k个推导，局部结果只保留最好的k个，k=1为贪心</p><p>2.n-best:整个句子的翻译结果保存在最大跨度所对应的结构，去除排名前n即可</p></li></ul></blockquote><h5 id="4-3-5立方剪枝"><a href="#4-3-5立方剪枝" class="headerlink" title="4.3.5立方剪枝"></a>4.3.5立方剪枝</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926101629420.png" alt=""></p><p>[^立方剪枝假设如果空间某个点的得分较高，那么它周围的点得分也较高。]: </p><h4 id="4-4基于语言学句法的模型"><a href="#4-4基于语言学句法的模型" class="headerlink" title="4.4基于语言学句法的模型"></a>4.4基于语言学句法的模型</h4><blockquote><p>在翻译中使用树结构可以更有效的对句子结构进行抽象，在句子中距离较远的在树结构可以很近，层次短语没有语言学句法标记，而且不允许对多个结构进行调序。</p></blockquote><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926110026491.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200926110052141.png" alt=""></p><h5 id="4-4-2基于树结构的文法"><a href="#4-4-2基于树结构的文法" class="headerlink" title="4.4.2基于树结构的文法"></a>4.4.2基于树结构的文法</h5><p>有两种规则：树到树、树到串</p><h6 id="树到树翻译规则"><a href="#树到树翻译规则" class="headerlink" title="树到树翻译规则"></a>树到树翻译规则</h6><blockquote><p>（Nt,Ns,Ts,Tt,Is,It,R)</p><p>Ns,Nt源、目标非终结符集合</p><p>Ts,Tt源、目标终结符集合</p><p>Is∈Ns,It∈Nt，其实非终结符集合</p><p>R是规则集合，r∈R，<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200927091618035.png" style="zoom: 50%;"></p><p>[^αh∈Ns，βh∈Nt，αr，βr表示由源语言(目标）终结符和非终结符组成的树结构，~是αr和βr叶子非终结符1-1对应关系]: </p></blockquote><h6 id="基于树结构的翻译推导"><a href="#基于树结构的翻译推导" class="headerlink" title="基于树结构的翻译推导"></a>基于树结构的翻译推导</h6><p>树到树：同步树替换文法规则</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200927092736753.png" alt=""></p><h6 id="树到串翻译规则"><a href="#树到串翻译规则" class="headerlink" title="树到串翻译规则"></a>树到串翻译规则</h6><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200927093010384.png" style="zoom:50%;"><h5 id="4-4-3树到串翻译规则抽取"><a href="#4-4-3树到串翻译规则抽取" class="headerlink" title="4.4.3树到串翻译规则抽取"></a>4.4.3树到串翻译规则抽取</h5><p>文法归纳和解码，文法归纳学习规则以及特征，解码利用得到的文法进行分析获得概率最高的翻译推导。介绍树到串的经典方法——<strong>GHKM</strong></p><h6 id="树的切割与最小规则"><a href="#树的切割与最小规则" class="headerlink" title="树的切割与最小规则"></a>树的切割与最小规则</h6><p>GHKM input：源语言句子及句法树、目标语言句子、源与目标之间的词对齐</p><blockquote><p>定义4.4.2Span目标语言第一个单词与最后一个单词所构成的范围，4.4.3Comlement Span除了祖先和子孙外其他节点Span的并集</p><p>4.4.4对于Span与Complement Span不相交，节点node可信，说明和其他部份无歧义</p><p>4.4.5一个树片段根节点和叶子结点的非终结系节点都是可信节点，那么树片段合格</p></blockquote><p>所有可信节点可以看作是边缘集合，边缘集合定义了哪些可以被切割，切割后得到的片段不能再被分割，被称为最小规则</p><h6 id="组合规则"><a href="#组合规则" class="headerlink" title="组合规则"></a>组合规则</h6><p>同时处理多个变量的调序，需要规则同时处理多个变量时需要更大的规则，称为composed-m规则（m个最小规则）</p><h6 id="SPMT规则"><a href="#SPMT规则" class="headerlink" title="SPMT规则"></a>SPMT规则</h6><p>对任意一个与词对齐兼容的短语可以找到包含它的最小翻译规则。</p><h6 id="句法树二叉化"><a href="#句法树二叉化" class="headerlink" title="句法树二叉化"></a>句法树二叉化</h6><p>句法树是描述句子结构的工具，但是过于扁平抽取困难，可以进行二叉化，树二叉化可以帮助规则抽取到更细颗粒度的规则</p><h5 id="4-2-4树到树翻译规则抽取"><a href="#4-2-4树到树翻译规则抽取" class="headerlink" title="4.2.4树到树翻译规则抽取"></a>4.2.4树到树翻译规则抽取</h5><p>可以基于节点对齐进行规则抽取，这样可以避免词对齐的错误：先利用外部节点对其工具或的对应关系，再将每个对其的节点看作是树片段的根节点进行规则抽取。也可以使用<strong>对齐矩阵</strong></p><h5 id="4-4-6基于超图的推导空间表示"><a href="#4-4-6基于超图的推导空间表示" class="headerlink" title="4.4.6基于超图的推导空间表示"></a>4.4.6基于超图的推导空间表示</h5><p>在句法分析中，上下文无关文法（CFG）可以被组织成一个叫有向超图的结构。或称为<strong>超图</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200927155206085.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200927155513360.png" alt=""></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> BookLook </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MT </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>code structure</title>
      <link href="/2020/08/29/common-tools/"/>
      <url>/2020/08/29/common-tools/</url>
      
        <content type="html"><![CDATA[<p><strong>nlp经典论文集</strong></p><p><a href="http://www.marekrei.com/blog/74-summaries-of-machine-learning-and-nlp-research/" target="_blank" rel="noopener">http://www.marekrei.com/blog/74-summaries-of-machine-learning-and-nlp-research/</a></p><p>prerequisite</p><p>MLemail</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727181706415.png" alt="image-20200727181706415"></p><p>bert相比于Transformer+Ngram</p><p>利用上下文相邻词的搭配信息计算出具有最大概率的句子。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727181533041.png" alt="image-20200727181533041"></p><p>该表格是人为训练得到的词组附加信息</p><p>代码部分：</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200802103545203.png" alt="image-20200802103545203"></p><blockquote><p>data_preprocessing:运行模型前要安装的包</p><p>LICNSE:协议</p><p>gitgnore：上传GitHub时可忽略</p><p>modles：存放模型</p><p>机器学习流程：训练（用有标记的数据训练）、测试（用有标记的数据检测正确率）、预测（没有答案，没有标记）</p></blockquote><p>huggingface/transformers   NLP模型</p><p>confi设置</p><p>modeling模型</p><p>Tokenization符号化（按词分开按字分开）</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200802105517955.png" alt="image-20200802105517955"></p><p>seq2seq翻译问题序列到序列finetune后训练 预训练</p><p>run_squard预测答案的所在的区间</p><p>做任务看example</p><p>_tf</p><p>wmseq.model</p><p>InputExample:ba’yi’ge’li’zi’zhuan’hua’wie’t</p><p>Bert</p><p>BIES</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200802110057407.png" alt="image-20200802110057407"></p><p>word2id</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727183526167.png" alt="image-20200727183526167"></p><p>加载tokenazation</p><p>ymcil/Chinese-BERT-wwm#bert模型</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727183644598.png" alt="image-20200727183644598"></p><p>快速加载：只需要填词就可以</p><p>都会存在hpara</p><p>分类器只能告诉类别，不能加限制</p><p>解码器限制输出是合法输出BIE/BE，此刻输出</p><p>CRF概率模型B-F 50</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727184205047.png" alt="image-20200727184205047"></p><p>PYTORCH</p><p>forward数据流动</p><p>——init——()</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/0V~FRU9G3J4SI%7D~4%V0JGCP.png" alt="img"></p><p>forward call(tf)</p><p>![img](C:/Users/89582/Documents/Tencent Files/895824013/Image/C2C/J@QW_9L~)TBJX7F4K1Z)$D4.png)</p><p>input——id attention_mask:qkv</p><p>QKV:</p><p>Masked Self-attention:</p><p>attenton_mask：控制每个时间点看到的词语</p><p>bert</p><p>tansformer;enconder,decoder,enconder-deconder</p><p>信息流处理</p><p>模型尽可能小获取更多的信息transformer</p><p>istm模型遗忘</p><p>kv——memory</p><p>viterbi</p><p>crf参考了上一个的概率以及当前输入的-综合算出概率</p><p>概率图模型<img src="https://bkimg.cdn.bcebos.com/pic/7dd98d1001e93901e04c0c6e7cec54e736d19610?x-bce-process=image/watermark,image_d2F0ZXIvYmFpa2U4MA==,g_7,xp_5,yp_5" alt="img"></p><p>神经网络的前身是概率图模型</p><p>.class Word kVMN:</p><p>。batchsize越大越好permute</p><p>matmul</p><p>clamp</p><p>exp自然范围</p><p>stack对函数</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> code structure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>neo4j</title>
      <link href="/2020/08/29/neo4j/"/>
      <url>/2020/08/29/neo4j/</url>
      
        <content type="html"><![CDATA[<h4 id="一、Basic-concepts"><a href="#一、Basic-concepts" class="headerlink" title="一、Basic concepts"></a>一、Basic concepts</h4><h5 id="Example-graph："><a href="#Example-graph：" class="headerlink" title="Example graph："></a>Example graph：</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200829155730420.png" alt=""></p><h5 id="1-Nodes：represent-entities"><a href="#1-Nodes：represent-entities" class="headerlink" title="1.Nodes：represent entities"></a>1.Nodes：represent entities</h5><h5 id="2-Labels：group-nodes-into-sets-by-a-certain-label"><a href="#2-Labels：group-nodes-into-sets-by-a-certain-label" class="headerlink" title="2.Labels：group nodes into sets by a certain label"></a>2.Labels：group nodes into sets by a certain label</h5><p>[^one node can get 0 to more lables]: </p><h5 id="3-Relationship："><a href="#3-Relationship：" class="headerlink" title="3.Relationship："></a>3.Relationship：</h5><blockquote><h5 id="connects-two-nodes-orgabize-nodes-into-structure-allow-a-graph-to-resemble-a-list-tree-map-compound-entity"><a href="#connects-two-nodes-orgabize-nodes-into-structure-allow-a-graph-to-resemble-a-list-tree-map-compound-entity" class="headerlink" title="connects two nodes,orgabize nodes into structure,allow a graph to resemble a list,tree,map,compound entity"></a>connects two nodes,orgabize nodes into structure,allow a graph to resemble a list,tree,map,compound entity</h5></blockquote><h5 id="4-Relationship-types"><a href="#4-Relationship-types" class="headerlink" title="4.Relationship types"></a>4.Relationship types</h5><p>a relationship must have exactly one type.And type can have value with some items </p><h5 id="5-Properties-属性"><a href="#5-Properties-属性" class="headerlink" title="5.Properties(属性)"></a>5.Properties(属性)</h5><p>name-value pairs</p><p>[^eg:node Person has two properties:name and born.]: </p><p>the value part of th eproperty can hold different data types:number string,bool(refer to “Cypel manual”)</p><h5 id="6-Traversals-and-paths"><a href="#6-Traversals-and-paths" class="headerlink" title="6.Traversals and paths"></a>6.Traversals and paths</h5><blockquote><p>Traversing a graph means visiting nodes by following relationships according to some rules.</p></blockquote><p>The traversal result could be returned as a path with the length one:</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200829160814113.png" alt=""></p><p>The shorstest path has length zero.(a single node and no relationship)</p><h5 id="7-Schema（indexs-and-constraint"><a href="#7-Schema（indexs-and-constraint" class="headerlink" title="7.Schema（indexs  and constraint)"></a>7.Schema（indexs  and constraint)</h5><blockquote><p>Neo4j is schema optional.</p></blockquote><p>​    7.1.index（increase performance)</p><p>​    7.2.constraint:Constraints are used to make sure that the data adheres to the rules of the domain. </p><h5 id="Naming-rules-and-recommendations"><a href="#Naming-rules-and-recommendations" class="headerlink" title="Naming rules and recommendations"></a>Naming rules and recommendations</h5><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200829162406979.png" alt=""></p><h5 id="8-Cyper"><a href="#8-Cyper" class="headerlink" title="8.Cyper"></a>8.Cyper</h5><h5 id="9-Pattern-模式：描述所需数据的形状"><a href="#9-Pattern-模式：描述所需数据的形状" class="headerlink" title="9.Pattern(模式：描述所需数据的形状)"></a>9.Pattern(模式：描述所需数据的形状)</h5><blockquote><p>A single node or relationship typically encodes very little information, but a pattern of nodes and relationships can encode arbitrarily complex ideas.</p><p>Cypher, Neo4j’s query language, is strongly based on patterns. Specifically, patterns are used to match desired graph structures. Once a matching structure has been found or created, Neo4j can use it for further processing.</p></blockquote><p>[^simple pattern：only a single relationship,such as:a Person LIVES_IN a City]:<br>[^Complex pattern: match instance where a Person LIVES_IN a Country]: (:Person) -[:LIVES_IN]-&gt; (:City) -[:PART_OF]-&gt; (:Country)//图标和箭头用来可视化图表</p><h5 id="10-Node-syntax"><a href="#10-Node-syntax" class="headerlink" title="10.Node syntax"></a>10.Node syntax</h5><p>(matrix:Movie {title: “The Matrix”, released: 1997})</p><blockquote><p>()represent a node(anonymous,uncharacterrized node)</p><p>(matrix):定义一种变量，只在单个语句中有意义</p><p>:Movie is pattern</p><p>{title: “The Matrix”, released: 1997} : properties</p></blockquote><h5 id="11-Relationship-syntax"><a href="#11-Relationship-syntax" class="headerlink" title="11.Relationship syntax"></a>11.Relationship syntax</h5><pre><code>-[role:ACTED_IN {roles: ["Neo"]}]-&gt;</code></pre><p>[^ACTED_IN表示关系的类型（类似于节点的标签]: </p><blockquote><p>(–):无定向关系</p><p>在一段加入–&gt;,&lt;–表示定向关系</p></blockquote><h5 id="12-Pattern-syntax"><a href="#12-Pattern-syntax" class="headerlink" title="12.Pattern syntax"></a>12.Pattern syntax</h5><pre><code>(keanu:Person:Actor {name: "Keanu Reeves"} )-[role:ACTED_IN {roles: ["Neo"] } ]-&gt;(matrix:Movie {title: "The Matrix"} )</code></pre><p>[^[]存储关系属性，（）存储节点属性]: </p><h5 id="13-Pattern-variables"><a href="#13-Pattern-variables" class="headerlink" title="13.Pattern variables"></a>13.Pattern variables</h5><blockquote><p>To increase modularity and reduce repetition.Cypher允许将模式分配给变量，允许检查匹配路径等其他表达式中。</p></blockquote><pre><code>acted_in = (:Person)-[:ACTED_IN]-&gt;(:Movie)</code></pre><blockquote><p>[^nodes(path),relationship(path),length(path):access details of a path]: </p></blockquote><h4 id="二、Patterns-in-practice"><a href="#二、Patterns-in-practice" class="headerlink" title="二、Patterns in practice"></a>二、Patterns in practice</h4><h5 id="1-Creating-data"><a href="#1-Creating-data" class="headerlink" title="1.Creating data"></a>1.Creating data</h5><pre><code>CREATE (:Movie { title:"The Matrix",released:1997 })</code></pre><p>CREATE语句直接创建指定的模式 </p><p>if 要返回创建的数据，定义一个变量</p><pre><code>CREATE (p:Person { name:"Keanu Reeves", born:1964 })RETURN p</code></pre><blockquote><p> To create more complex structures,we can separate the elements with commas or use multiple CREATE statements</p><pre><code>CREATE (a:Person { name:"Tom Hanks",  born:1956 })-[r:ACTED_IN { roles: ["Forrest"]}]-&gt;(m:Movie { title:"Forrest Gump",released:1994 })CREATE (d:Person { name:"Robert Zemeckis", born:1951 })-[:DIRECTED]-&gt;(m)RETURN a,d,r,m</code></pre><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200830082920404.png" alt=""></p></blockquote><h5 id="2-Matching-patterns"><a href="#2-Matching-patterns" class="headerlink" title="2.Matching patterns"></a>2.Matching patterns</h5><p>[^A MATCH statement will search for the patterns we specify and return one row per successful pattern match.we can start looking for all nodes with the Movie label:]: </p><pre><code>MATCH (m:Movie)RETURN m</code></pre><pre><code>MATCH (p:Person { name:"Keanu Reeves" })RETURN p</code></pre><pre><code>MATCH (p:Person { name:"Tom Hanks" })-[r:ACTED_IN]-&gt;(m:Movie)RETURN m.title, r.roles</code></pre><p>[^可以通过identifer.property访问属性]: </p><h5 id="3-Attaching-structures"><a href="#3-Attaching-structures" class="headerlink" title="3.Attaching structures"></a>3.Attaching structures</h5><p>[^To extand the graph with new info]: </p><pre><code>MATCH (p:Person { name:"Tom Hanks" })CREATE (m:Movie { title:"Cloud Atlas",released:2012 })CREATE (p)-[r:ACTED_IN { roles: ['Zachry']}]-&gt;(m)RETURN p,r,m</code></pre><h5 id="4-Completing-patterns"><a href="#4-Completing-patterns" class="headerlink" title="4.Completing patterns"></a>4.Completing patterns</h5><blockquote><p>Whenever we get data from external systems or are not sure if certain info  already exists in the graph,can use MERGE in this function.</p></blockquote><pre><code>MERGE (m:Movie { title:"Cloud Atlas" })ON CREATE SET m.released = 2012RETURN m</code></pre><pre><code>MATCH (m:Movie { title:"Cloud Atlas" })MATCH (p:Person { name:"Tom Hanks" })MERGE (p)-[r:ACTED_IN]-&gt;(m)ON CREATE SET r.roles =['Zachry']RETURN p,r,m</code></pre><p>[^MERGE 还可确保创建不重复的关系，前提是要传递给它两个将要被连接的节点。如果只传递一个节点给MERGE 语句，就会在该节点的直接领居顶点中寻找，不存在则直接创建。]: </p><pre><code>CREATE (y:Year { year:2014 })MERGE (y)&lt;-[:IN_YEAR]-(m10:Month { month:10 })MERGE (y)&lt;-[:IN_YEAR]-(m11:Month { month:11 })RETURN y,m10,m11</code></pre><h4 id="三、Getting-the-correct-results"><a href="#三、Getting-the-correct-results" class="headerlink" title="三、Getting the correct results"></a>三、Getting the correct results</h4><h5 id=""><a href="#" class="headerlink" title=""></a><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200830104247709.png" alt="image-20200830104247709"></h5><h5 id="1-Filtering-results-筛选结果"><a href="#1-Filtering-results-筛选结果" class="headerlink" title="1.Filtering results(筛选结果)"></a>1.Filtering results(筛选结果)</h5><blockquote><p>用WHERE子句，用AND,OR,XOR and NOT构成了条件语句。</p><p>eg：</p><pre><code>MATCH (m:Movie)WHERE m.title = "The Matrix"RETURN m</code></pre><p>equals：</p><pre><code>MATCH (m:Movie { title: "The Matrix" })RETURN m</code></pre></blockquote><p>WHERE语句还可以用于数值比较，匹配正则公式以及检查列表中是否存在特定值，搭配AND,OR,XOR,NOT</p><pre><code>MATCH (p:Person)-[r:ACTED_IN]-&gt;(m:Movie)WHERE p.name =~ "K.+" OR m.released &gt; 2000 OR "Neo" IN r.rolesRETURN p,r,m</code></pre><pre><code>MATCH (p:Person)-[:ACTED_IN]-&gt;(m)WHERE NOT (p)-[:DIRECTED]-&gt;()RETURN p,m</code></pre><h5 id="2-Returning-results"><a href="#2-Returning-results" class="headerlink" title="2.Returning results"></a>2.Returning results</h5><p><strong>literal value</strong>:arrays[1,2,3],map{name:”wrh”,born:2000,movies:[“Forrest”,…]},properties of any node:n.name,length(array),toInteger(“12”), substring(“2014-07-01”,0,4) and coalesce(p.nickname,”n/a”)</p><blockquote><p>起别名expression AS alias</p><p>RETURN之后使用DISTINCT返回唯一结果</p></blockquote><h5 id="3-Aggregating-information-聚合信息"><a href="#3-Aggregating-information-聚合信息" class="headerlink" title="3.Aggregating information(聚合信息)"></a>3.Aggregating information(聚合信息)</h5><blockquote><p>count,sum,avg,min,max函数</p></blockquote><p>[^聚合时要跳过NULL值，如果想聚合唯一的DISTINCT值，可以使用count（DISTINCT role）]: </p><p>eg:the following statement  finds out how often an actor and adirctoe work together</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200830161224020.png" alt=""></p><h5 id="4-Ordering-and-pagination-排序和分类"><a href="#4-Ordering-and-pagination-排序和分类" class="headerlink" title="4.Ordering and pagination(排序和分类)"></a>4.Ordering and pagination(排序和分类)</h5><p>Ordering is using： </p><pre><code> ORDER BY expression [ASC|DESC]//ASC正序，DESC逆序</code></pre><p>if we return person.name,we can still ORDER BY person.age,because it return the person reference </p><p>Pagination is done using SKIP{offset} and LIMIT {count} clauses</p><blockquote><p>For instance to find the most prolific actors we could do:</p><pre><code>MATCH (a:Person)-[:ACTED_IN]-&gt;(m:Movie)RETURN a, count(*) AS appearancesORDER BY appearances DESC LIMIT 10;</code></pre></blockquote><h5 id="5-Collecting-aggregation"><a href="#5-Collecting-aggregation" class="headerlink" title="5.Collecting aggregation"></a>5.Collecting aggregation</h5><p>use collect() in the parent-child strutures</p><p>The following statement could be used to retrieve the cast of each movie in our database:</p><pre><code>MATCH (m:Movie)&lt;-[:ACTED_IN]-(a:Person)RETURN m.title AS movie, collect(a.name) AS cast, count(*) AS actors</code></pre><pre><code>+-----------------------------------------+| movie | cast | actors |+-----------------------------------------+| "Forrest Gump" | ["Tom Hanks"] | 1 || "Cloud Atlas" | ["Tom Hanks"] | 1 |+-----------------------------------------+2 rows</code></pre><h5 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h5><p><a href="http://www.neo4j.com" target="_blank" rel="noopener">www.neo4j.com</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Commontools </category>
          
      </categories>
      
      
        <tags>
            
            <tag> database </tag>
            
            <tag> neo4j </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>juice recipe</title>
      <link href="/2020/08/28/juice-recipe/"/>
      <url>/2020/08/28/juice-recipe/</url>
      
        <content type="html"><![CDATA[<h4 id="一、果昔"><a href="#一、果昔" class="headerlink" title="一、果昔"></a>一、果昔</h4><p>[^Tips:if you want to make juice stickier,you can add some avocado(牛油果)]: </p><p>1.avocado+apple+mint(薄荷)</p><p>taste:</p><p>2.mango+pineapple(1:1)</p><p>taste:</p><p>3.papaya+red grapes</p><p>taste:</p><p>4.mango+peach+pear/orange</p><h4 id="二、果汁"><a href="#二、果汁" class="headerlink" title="二、果汁"></a>二、果汁</h4><p>1.mango+pear</p><p>T:</p><p>2.peach+pear</p><p>T：</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> food </category>
          
      </categories>
      
      
        <tags>
            
            <tag> juice </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Wuenda&#39;s class(2)</title>
      <link href="/2020/08/20/wuenda-s-class-2/"/>
      <url>/2020/08/20/wuenda-s-class-2/</url>
      
        <content type="html"><![CDATA[<h3 id="p1-Train-dev-test-sets"><a href="#p1-Train-dev-test-sets" class="headerlink" title="p1.Train/dev/test sets"></a>p1.Train/dev/test sets</h3><blockquote><p>Train：训练集，用来训练各种模型</p><p>dev：验证集(development set)/Hold-out cross validation set，评估这些模型，通过迭代选出最优模型</p><p>test：测试集，需要对最终选定的神经模型进行最优估计（可选）</p><p>[^小数据集Train:dev:test=6:2:2 数据集较大（eg：100万data验证集和测试集可能只达到0.25%）]: </p><p>dev,test选择的数据来源需相同，才能有更好的效果</p></blockquote><h3 id="p2Bias-Variance偏差-方差"><a href="#p2Bias-Variance偏差-方差" class="headerlink" title="p2Bias/Variance偏差/方差"></a>p2Bias/Variance偏差/方差</h3><p>high bias:underfitting</p><p>high variance: overfitting</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200820092101036.png" alt=""></p><p>[^Train set error 小，dev set error大说明过拟合方差大 ；Train set error 大，dev set error小说明欠拟合偏差大]: </p><h3 id="p3机器学习基础"><a href="#p3机器学习基础" class="headerlink" title="p3机器学习基础"></a>p3机器学习基础</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200820095910120.png" alt=""></p><h4 id="P4Regulation"><a href="#P4Regulation" class="headerlink" title="P4Regulation"></a>P4Regulation</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200820101622245.png" alt=""></p><h4 id="P5How-does-regularization-prevent-overfitting"><a href="#P5How-does-regularization-prevent-overfitting" class="headerlink" title="P5How does regularization prevent overfitting"></a>P5How does regularization prevent overfitting</h4><p><img src="http://www.ai-start.com/dl2017/images/1a6c47e64293cde6a0facb3872701db2.png" alt=""></p><p>if正则化参数设置的足够大，权重矩阵被设置为接近为零，基本上消除了隐藏的影响。这样就会从过拟合的状态接近高偏差的状态。</p><p>但会存在一个中间值，即“Just Right”状态。消除或减少许多隐藏单元的影响，神经网络会越来越接近逻辑回归，所有隐藏单元依然存在，但是神经网络变简单了。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822091610507.png" alt=""></p><p>如果正则化参数变得很大，参数W很小，z也会相对变小，此时忽略b的影响，z会相对变小，实际上，z的取值范围很小，这个激活函数，也就是曲线函数tanh会相对呈线性，整个神经网络会计算离线性函数近的值，这个线性函数非常简单，并不是一个极复杂的高度非线性函数，不会发生过拟合。</p><h3 id="Dropout-regularization（主要应用在cv）"><a href="#Dropout-regularization（主要应用在cv）" class="headerlink" title="Dropout regularization（主要应用在cv）"></a>Dropout regularization（主要应用在cv）</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822091810868.png" alt=""></p><p>[^keep_prob:保留每一个节点的概率。]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822092105257.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822092632822.png" alt=""></p><p>如果某些层需要去除过拟合，那么keep_prob就需要设置的大一些。</p><h3 id="p9归一化输入"><a href="#p9归一化输入" class="headerlink" title="p9归一化输入"></a>p9归一化输入</h3><blockquote><p>一种加速神经网络训练速度的方法。</p></blockquote><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822093306498.png" alt=""></p><p>右图比左图的代价函数更容易找到min值，有图需要梯度下降迭代寻找。</p><h3 id="p10Vanishing-exploding-gradients"><a href="#p10Vanishing-exploding-gradients" class="headerlink" title="p10Vanishing/exploding gradients"></a>p10Vanishing/exploding gradients</h3><p>Vanishing gradients：接近于输出层的隐藏层由于其梯度相对正常，权值更新也相对正常。越靠近输入层，由于gradients消失，会导致靠近输入层权值更新缓慢或者停止。此时就导致训练时只等价于后面几层的浅层网络的学习。</p><h5 id="p9-1-换用Relu、LeakyRelu、Elu等激活函数"><a href="#p9-1-换用Relu、LeakyRelu、Elu等激活函数" class="headerlink" title="p9.1 换用Relu、LeakyRelu、Elu等激活函数"></a>p9.1 换用Relu、LeakyRelu、Elu等激活函数</h5><p>ReLu：让激活函数的导数为1</p><p>LeakyReLu：包含了ReLu的几乎所有有点，同时解决了ReLu中0区间带来的影响</p><p>ELU：和LeakyReLu一样，都是为了解决0区间问题，相对于来，elu计算更耗时一些（为什么）</p><p>具体可以看<a href="#activation">关于各种激活函数的解析与讨论</a></p><h5 id="p9-2-BatchNormalization"><a href="#p9-2-BatchNormalization" class="headerlink" title="p9.2 BatchNormalization"></a>p9.2 BatchNormalization</h5><p>BN本质上是解决传播过程中的梯度问题，具体待补充完善，查看<a href="...">BN</a></p><h5 id="p9-3-ResNet残差结构"><a href="#p9-3-ResNet残差结构" class="headerlink" title="p9.3 ResNet残差结构"></a>p9.3 ResNet残差结构</h5><p>具体待补充完善，查看<a href="...">ResNet</a></p><h5 id="p9-4-LSTM结构"><a href="#p9-4-LSTM结构" class="headerlink" title="p9.4 LSTM结构"></a>p9.4 LSTM结构</h5><p>LSTM不太容易发生梯度消失，主要原因在于LSTM内部复杂的“门（gates）”</p><h5 id="p9-5-预训练加finetunning"><a href="#p9-5-预训练加finetunning" class="headerlink" title="p9.5 预训练加finetunning"></a>p9.5 预训练加finetunning</h5><p>此方法来自Hinton在06年发表的论文上，其基本思想是每次训练一层隐藏层节点，将上一层隐藏层的输出作为输入，而本层的输出作为下一层的输入，这就是逐层预训练。</p><p>训练完成后，再对整个网络进行“微调（fine-tunning）”。</p><p>此方法相当于是找全局最优，然后整合起来寻找全局最优，但是现在基本都是直接拿imagenet的预训练模型直接进行finetunning。</p><h3 id="4-5-梯度剪切、正则"><a href="#4-5-梯度剪切、正则" class="headerlink" title="4.5 梯度剪切、正则"></a>4.5 梯度剪切、正则</h3><p>这个方案主要是针对梯度爆炸提出的，其思想是设值一个剪切阈值，如果更新梯度时，梯度超过了这个阈值，那么就将其强制限制在这个范围之内。这样可以防止梯度爆炸。</p><p>另一种防止梯度爆炸的手段是采用权重正则化，正则化主要是通过对网络权重做正则来限制过拟合，但是根据正则项在损失函数中的形式：</p><p>可以看出，如果发生梯度爆炸，那么权值的范数就会变的非常大，反过来，通过限制正则化项的大小，也可以在一定程度上限制梯度爆炸的发生。</p><h3 id="p32softmax回归"><a href="#p32softmax回归" class="headerlink" title="p32softmax回归"></a>p32softmax回归</h3><p>可以将输入数据分为多类。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822102947253.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822103103845.png" alt=""></p><p>[^直线边界代表决策边界]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822103500570.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822104116682.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822104242011.png" alt="image-20200822104242011"></p><h3 id="Class-5-sequence-data-nlp-Natural-Language-Processing"><a href="#Class-5-sequence-data-nlp-Natural-Language-Processing" class="headerlink" title="Class 5 sequence data-nlp(Natural Language Processing)"></a>Class 5 sequence data-nlp(Natural Language Processing)</h3><h4 id="p2Notation"><a href="#p2Notation" class="headerlink" title="p2Notation"></a>p2Notation</h4><h5 id="Reprsenting-words"><a href="#Reprsenting-words" class="headerlink" title="Reprsenting words"></a>Reprsenting words</h5><p>one-hot编码（UNK表示未标识）</p><h4 id="p3Recurrent-Neural-Network"><a href="#p3Recurrent-Neural-Network" class="headerlink" title="p3Recurrent Neural Network"></a>p3Recurrent Neural Network</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200822110650134.png" alt=""></p><p>[^X&lt;1&gt;,….，X <t>表示对目标序列的索引]</t></p><h3 id="RNN-Recurrent-Neural-Networks"><a href="#RNN-Recurrent-Neural-Networks" class="headerlink" title="RNN(Recurrent Neural Networks)"></a>RNN(Recurrent Neural Networks)</h3><blockquote><p>传统神经网络有两个缺点：</p><p>1.输入和输出序列的长度必须一致。</p><p>2.不同位置的特征无法</p></blockquote><p>RNN只是用了之前的信息来预测当时的信息</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> VideoClass </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word Representation-Sememes</title>
      <link href="/2020/08/12/word-representation-sememes/"/>
      <url>/2020/08/12/word-representation-sememes/</url>
      
        <content type="html"><![CDATA[<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed by several sememes.</p></blockquote><p>义原是人为标记的，标记义原后最终形成语义常识知识库。word representation learning(WRL)就是把词语映射到低维空间中。本文提出了三种义原编码模型来学习义原、意识、词语的表示，再结合attention机制来发现词语意识。</p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><p>ono-hot:数据稀疏，忽视词语关系</p><p>提出分布式表示，所有词语投射到低维语义环境，将每个词都考虑为向量。</p><p>随着文本语料呈指数增长，模型效率很重要，所以提出的CBOW和Skip-Gram两种模型。</p><blockquote><p>这两种模型都是通过最大化词和上下文的预测概率，进一步在word affinity matrix上，利用矩阵分解来学习词表示。但没有考虑一词多义，提出一种对每个单词进行non-parametric的Skip-Gram模型。提出了用来联合学习词语、语义和近义词表示的自编码器。</p></blockquote><h3 id="Word-Sense-Disambiguation-and-Representation-learning"><a href="#Word-Sense-Disambiguation-and-Representation-learning" class="headerlink" title="Word Sense Disambiguation and Representation learning"></a>Word Sense Disambiguation and Representation learning</h3><p>WSD:在确定的上下文中计算上的识别出词语的词义和意识。</p><p>[^WSD:supervised and  kowledge-based methods]: </p><h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p><strong>framework:SE-WRL</strong>(Sememe-Encoded WRL义原编码词语表示学习)</p><p>[^该框架为词语语义消歧和表示学习任务，考虑了义原信息。]: </p><h3 id="Knowledege"><a href="#Knowledege" class="headerlink" title="Knowledege"></a>Knowledege</h3><p><strong>Semems,Sesens and words in Hownet</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200813104751621.png" alt="image-20200813104751621"></p><p>[^第一层表示词语“苹果”，第二层表示苹果的两个语义“电脑”与”水果“。第三层表示第一个语义有三个义原“电脑、携带和特定牌子。]: </p><h3 id="Conventional-Skip-Gram-Model"><a href="#Conventional-Skip-Gram-Model" class="headerlink" title="Conventional Skip-Gram Model"></a>Conventional Skip-Gram Model</h3><p>[^义原、语义、词语集合为X,S,W]: </p><p>对于每一条纯文本序列中的目标字w，C(w)代表它的上下词语集合</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200813110339351.png" alt="image-20200813110339351"></p><h3 id="SE-WRL-model"><a href="#SE-WRL-model" class="headerlink" title="SE-WRL model"></a>SE-WRL model</h3><p>SE-WRL model的三种应用义原信息的不同策略，包括SSA,SAC,SAT.</p><h5 id="1-SSA-Simple-Sememe-Aggregation-Model-简单义原聚集模型"><a href="#1-SSA-Simple-Sememe-Aggregation-Model-简单义原聚集模型" class="headerlink" title="1.SSA(Simple Sememe Aggregation Model)简单义原聚集模型"></a>1.SSA(Simple Sememe Aggregation Model)简单义原聚集模型</h5><p>SSA把所有词语的语义的义原一同考虑进来，用目标词的所有上下文的义原嵌入的平均值来表示。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200813111950211.png" alt="image-20200813111950211"></p><h5 id="2-SAC-Sememe-Attention-over-Context-Model-基于上下文的义原注意力模型"><a href="#2-SAC-Sememe-Attention-over-Context-Model-基于上下文的义原注意力模型" class="headerlink" title="2.SAC(Sememe Attention over Context Model)基于上下文的义原注意力模型"></a>2.SAC(Sememe Attention over Context Model)基于上下文的义原注意力模型</h5><p>SSA模型用聚集的义原嵌入来代替目标词词嵌入，用义原信息编码来进行词表示学习。</p><p>但是不能处理大多数词的多义词现象。SAC利用注意力机制，根据当前词自动选择上下文合适的语义。</p><h5 id="3-SAT-Sememe-Attention-over-Target-Model-基于目标词的义原注意力模型"><a href="#3-SAT-Sememe-Attention-over-Target-Model-基于目标词的义原注意力模型" class="headerlink" title="3.SAT(Sememe Attention over Target Model)基于目标词的义原注意力模型"></a>3.SAT(Sememe Attention over Target Model)基于目标词的义原注意力模型</h5><p>与SAC模型不同，SAT为上下文词语学习原始的词嵌入，但是为目标词学习义原嵌入。</p><h3 id="Word-Similarity词汇相似度"><a href="#Word-Similarity词汇相似度" class="headerlink" title="Word Similarity词汇相似度"></a>Word Similarity词汇相似度</h3><p>通过比较在给定的数据集上，通过词语表示学习模型计算出的词对相似度来衡量词语表示的质量。词语表示学习模型在语义空间中，根据词语的距离来计算词语相似度。</p><h3 id="Word-Analogy词语类推"><a href="#Word-Analogy词语类推" class="headerlink" title="Word Analogy词语类推"></a>Word Analogy词语类推</h3><p>词语类比推理是用来评价模型词语表示学习质量的任务。</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>利用义原信息来表示每一个词各种各样的语义，提出了可以自动地上下文中选取合适的语义的义原注意力。</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>File：<a href="https://arxiv.org/pdf/1504.00548.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1504.00548.pdf</a></p><p>Translation：<a href="https://www.cnblogs.com/fengyubo/p/9365824.html" target="_blank" rel="noopener">https://www.cnblogs.com/fengyubo/p/9365824.html</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> WRL - WSD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sequence-wuenda</title>
      <link href="/2020/08/01/sequence-wuenda/"/>
      <url>/2020/08/01/sequence-wuenda/</url>
      
        <content type="html"><![CDATA[<h3 id="1-2数学符号"><a href="#1-2数学符号" class="headerlink" title="1.2数学符号"></a>1.2数学符号</h3><ol><li><p>创建词表（UNK表示未知词）2.represent word(ono-hot)</p><p>[^use x&lt;1&gt;,x&lt;2&gt;,…,x<tx>,tx表示sequence length]: </tx></p></li></ol><h3 id="1-3RNN-Recurrent-Netrual-Network"><a href="#1-3RNN-Recurrent-Netrual-Network" class="headerlink" title="1.3RNN(Recurrent Netrual Network)"></a>1.3RNN(Recurrent Netrual Network)</h3><h4 id="Why-not-choose-standard-Network"><a href="#Why-not-choose-standard-Network" class="headerlink" title="Why not choose standard Network?"></a>Why not choose standard Network?</h4><p><strong>-Reason:</strong></p><ol><li>Input and output’s length is differrent.(use pad is not good)</li><li>can’t share features in the different positions of text.</li></ol><h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><blockquote><p>RNN计算输出只考虑了之前的输入，没有考虑之后的输入。</p></blockquote><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200801172800273.png" alt="image-20200801172800273"></p><p>[^a&lt;0&gt;一般是空向量，Wya表示乘a类型的向量，计算出y类型的向量。]: </p><h4 id="Simplified-RNN-notation"><a href="#Simplified-RNN-notation" class="headerlink" title="Simplified RNN notation"></a>Simplified RNN notation</h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200801174401617.png" alt="image-20200801174401617"></p><h3 id="1-4通过时间的方向传播"><a href="#1-4通过时间的方向传播" class="headerlink" title="1-4通过时间的方向传播"></a>1-4通过时间的方向传播</h3><p>正相传递与反向传递（更新参数）</p><h3 id="1-5Examples-of-RNN"><a href="#1-5Examples-of-RNN" class="headerlink" title="1-5Examples of RNN"></a>1-5Examples of RNN</h3><blockquote><ol><li>one to one:标准类型</li><li>one to many：音乐分类/序列输入</li><li>many to one：情感</li><li>many to many（Tx=Ty)：NER</li><li>many to many(Tx不等于Ty)：机器翻译</li></ol></blockquote><h4 id="1-6语言模型和序列生成"><a href="#1-6语言模型和序列生成" class="headerlink" title="1-6语言模型和序列生成"></a>1-6语言模型和序列生成</h4><h4 id="Training-set-large-corpus-of-language"><a href="#Training-set-large-corpus-of-language" class="headerlink" title="Training set:large corpus of language"></a>Training set:large corpus of language</h4><p>语言模型会告诉你下一个出现的词语的概率。先将序列的词汇标记</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200801194059269.png" alt="image-20200801194059269"></p><blockquote><p>使用条件概率计算整个句子的概率。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200801194531781.png" alt="image-20200801194531781"></p></blockquote><h4 id="1-7对新序列采样"><a href="#1-7对新序列采样" class="headerlink" title="1-7对新序列采样"></a>1-7对新序列采样</h4><p>基于字符的语言模型相比于基于词汇的语言模型而言，不太能关注到文本的上下文关系，而且计算的时间也较长。</p><h3 id="1-8-Vanishing-gradientes-with-RNNs"><a href="#1-8-Vanishing-gradientes-with-RNNs" class="headerlink" title="1-8 Vanishing gradientes with RNNs"></a>1-8 Vanishing gradientes with RNNs</h3><p>RNN不擅长处理长期依赖的问题，反向传播较为困难。提出了GRU来解决这个问题</p><p><strong>GRU</strong></p><p>加入新的变量具有记忆能力，即记忆细胞，c<t>记录记忆细胞的值，GRU门记录</t></p><p>决定了哪个向量与更新记忆细胞有关。</p><p>Γu表示GRU门</p><h3 id="1-10长短期记忆"><a href="#1-10长短期记忆" class="headerlink" title="1-10长短期记忆"></a>1-10长短期记忆</h3><p>[^Γu更新门，Γf遗忘门，Γo输出门]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200802221536662.png" alt="image-20200802221536662"></p><h3 id="1-11双向神经网络"><a href="#1-11双向神经网络" class="headerlink" title="1-11双向神经网络"></a>1-11双向神经网络</h3><p><strong>BRNN</strong></p><p>构成无向图</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200802222930856.png" alt="image-20200802222930856"></p><p>前向的激活值a&lt;3&gt;与反向的激活值共同决定y&lt;3&gt;</p><h3 id="1-12深层循环神经网络"><a href="#1-12深层循环神经网络" class="headerlink" title="1-12深层循环神经网络"></a>1-12深层循环神经网络</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200802224119713.png" alt="image-20200802224119713"></p><h3 id="NLP-and-word-representation"><a href="#NLP-and-word-representation" class="headerlink" title="NLP and word representation"></a>NLP and word representation</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200802230012280.png" alt="image-20200802230012280"></p><h4 id="Visualizing-word-embedding"><a href="#Visualizing-word-embedding" class="headerlink" title="Visualizing word embedding"></a>Visualizing word embedding</h4><p>降维观察</p><h4 id="使用词嵌入"><a href="#使用词嵌入" class="headerlink" title="使用词嵌入"></a>使用词嵌入</h4><p>1.在大量文本集中使用词向量表示文本（或下载预训练的嵌入模型）</p><p>2.使用one-hot表示词向量</p><p>3在含有少量标签的数据集中继续训练</p><h3 id="2-3词嵌入的特性"><a href="#2-3词嵌入的特性" class="headerlink" title="2.3词嵌入的特性"></a>2.3词嵌入的特性</h3><h4 id="Analogies-using-word-vectors"><a href="#Analogies-using-word-vectors" class="headerlink" title="Analogies using word vectors"></a>Analogies using word vectors</h4><p>t-SNE:300D-&gt;2D</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200804100405784.png" alt="image-20200804100405784"></p><p>因为women-man，king-queen相差的都是gender</p><h3 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h3><p><strong>skip-grams:</strong>抽取上下文、选择目标词</p><p>分别在上下文的一定范围内选择代表词，构造监督学习问题</p><blockquote><p>分级softmax：哈夫曼思想</p><p>上下文采样：1.根据语料库均匀随机的采样（无关词出现的频率太高）</p><p>2.使用启发式找到常出现并且含有有效信息的词</p></blockquote><h3 id="2-7负采样（Negative-sample"><a href="#2-7负采样（Negative-sample" class="headerlink" title="2.7负采样（Negative sample)"></a>2.7负采样（Negative sample)</h3><p>用content预测word,结果为1/0（正样本/负样本）</p><p>k=5-20 small data set</p><p>k=2-5 big data set(k:skip-window)</p><p>Glove算法</p><p>Xij表示i-j在上下文出现的次数</p><h3 id="sentiment-classification"><a href="#sentiment-classification" class="headerlink" title="sentiment classification"></a>sentiment classification</h3><p>情绪分类需要借助RNN来判断good和not good的区别</p><p>消除偏差时需要</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> VedioLearning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> sequence model </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch official code</title>
      <link href="/2020/07/20/pytorch-official-code/"/>
      <url>/2020/07/20/pytorch-official-code/</url>
      
        <content type="html"><![CDATA[<h3 id="1-numpy"><a href="#1-numpy" class="headerlink" title="1.numpy"></a>1.numpy</h3><p>[^numpy提供n维数组对象，是科学计算的通用框架，不涉及图，深度学习、梯度。但可借助网络手动实现向前向后传递。]: </p><pre><code># -*- coding: utf-8 -*-import numpy as np# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random input and output datax = np.random.randn(N, D_in)y = np.random.randn(N, D_out)# Randomly initialize weightsw1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)learning_rate = 1e-6for t in range(500):    # Forward pass: compute predicted y    h = x.dot(w1)    h_relu = np.maximum(h, 0)    y_pred = h_relu.dot(w2)    # Compute and print loss    loss = np.square(y_pred - y).sum()    print(t, loss)    # Backprop to compute gradients of w1 and w2 with respect to loss    grad_y_pred = 2.0 * (y_pred - y)    grad_w2 = h_relu.T.dot(grad_y_pred)    grad_h_relu = grad_y_pred.dot(w2.T)    grad_h = grad_h_relu.copy()    grad_h[h &lt; 0] = 0    grad_w1 = x.T.dot(grad_h)    # Update weights    w1 -= learning_rate * grad_w1    w2 -= learning_rate * grad_w2</code></pre><p>1.1numpy.random.randn(d0,d1,…,dn)</p><blockquote><p>#rand 函数给定维度生成[0,1)之间的数据，包含0，不包含1</p><p>dn表示每个维度</p><p>返回值为维度为d0<em>d1</em>….*dn的矩阵</p></blockquote><p>1.2range(start,stop,step)函数</p><blockquote><p>start:计数开始点（默认0）</p><p>stop:技术结束点（不包括stop）</p><p>step：步长（默认1）</p></blockquote><p>1.3numpy.dot(arr1,arr2)//numpy.dot(matrix1,matrix2)</p><blockquote><p>求解两数组的内积/矩阵积</p></blockquote><p>1.4numpy.maximum(x,y)</p><blockquote><p>求x与y较大者</p></blockquote><p>1.5numpy.max(a,axis=None,out=None,keepdims=False)</p><blockquote><p>求序列的最值，最少接收一个参数，axis（=0为列向，=1为行向量）</p></blockquote><p>1.6matrix.T(m)</p><blockquote><p>求矩阵的转置</p></blockquote><p>1.7numpy.square(num)</p><blockquote><p>求num的平方</p></blockquote><h3 id="2-Tensor"><a href="#2-Tensor" class="headerlink" title="2.Tensor"></a>2.Tensor</h3><p>[^Tensor是张量，即任意维度的向量，pytorch可以利用GPU加速数字计算，要在GPU上运行pytorch Tensor，需要将其转换为新的数据类型]: </p><pre><code># -*- coding: utf-8 -*-import torchdtype = torch.floatdevice = torch.device("cpu")# device = torch.device("cuda:0") # Uncomment this to run on GPU# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random input and output datax = torch.randn(N, D_in, device=device, dtype=dtype)y = torch.randn(N, D_out, device=device, dtype=dtype)# Randomly initialize weightsw1 = torch.randn(D_in, H, device=device, dtype=dtype)w2 = torch.randn(H, D_out, device=device, dtype=dtype)learning_rate = 1e-6for t in range(500):    # Forward pass: compute predicted y    h = x.mm(w1)    h_relu = h.clamp(min=0)    y_pred = h_relu.mm(w2)    # Compute and print loss    loss = (y_pred - y).pow(2).sum().item()    if t % 100 == 99:        print(t, loss)    # Backprop to compute gradients of w1 and w2 with respect to loss    grad_y_pred = 2.0 * (y_pred - y)    grad_w2 = h_relu.t().mm(grad_y_pred)    grad_h_relu = grad_y_pred.mm(w2.t())    grad_h = grad_h_relu.clone()    grad_h[h &lt; 0] = 0    grad_w1 = x.t().mm(grad_h)    # Update weights using gradient descent    w1 -= learning_rate * grad_w1    w2 -= learning_rate * grad_w2</code></pre><p>2.1torch.device(‘cpu’/‘cuda’)</p><blockquote><p>将torch.Tensor分配到的设备的对象</p></blockquote><p>2.2torch.mm(input,mat2,out=None)</p><blockquote><p>对矩阵input和mat2执行矩阵乘法，返回结果矩阵</p></blockquote><p>2.3torch.clamp(input,min,max,out=None)-&gt;Tensor</p><blockquote><p>input:输入张量；min：限制范围下限；max：上限；out：输出张量</p></blockquote><h3 id="3-Autograd"><a href="#3-Autograd" class="headerlink" title="3.Autograd"></a>3.Autograd</h3><p>[^针对大型网络而言，手动实现前向和后向传递非常麻烦，使用autograd自动计算神经网络中的反向传递。pytorch中的autograd软件包完全提供了此功能，前向传递定义一个计算图；节点为张量，边为输入张量产生输出张量的函数，接着就可以通过该图进行反向传播，可以轻松计算梯度。if x:Tensor,x.requires_grad=True,x.grad是另一个Tensor。]: </p><blockquote><p>不需要手动通过网络实现反向传递，使用Pytorch Tensor和autograd来实现两层网络。</p></blockquote><pre><code># -*- coding: utf-8 -*-import torchdtype = torch.floatdevice = torch.device("cpu")# device = torch.device("cuda:0") # Uncomment this to run on GPU# N is batch size; D_in is input dimension;# H is hidden dimension; D_out is output dimension.N, D_in, H, D_out = 64, 1000, 100, 10# Create random Tensors to hold input and outputs.# Setting requires_grad=False indicates that we do not need to compute gradients# with respect to these Tensors during the backward pass.x = torch.randn(N, D_in, device=device, dtype=dtype)y = torch.randn(N, D_out, device=device, dtype=dtype)# Create random Tensors for weights.# Setting requires_grad=True indicates that we want to compute gradients with# respect to these Tensors during the backward pass.w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)learning_rate = 1e-6for t in range(500):    # Forward pass: compute predicted y using operations on Tensors; these    # are exactly the same operations we used to compute the forward pass using    # Tensors, but we do not need to keep references to intermediate values since    # we are not implementing the backward pass by hand.    y_pred = x.mm(w1).clamp(min=0).mm(w2)    # Compute and print loss using operations on Tensors.    # Now loss is a Tensor of shape (1,)    # loss.item() gets the scalar value held in the loss.    loss = (y_pred - y).pow(2).sum()    if t % 100 == 99:        print(t, loss.item())    # Use autograd to compute the backward pass. This call will compute the    # gradient of loss with respect to all Tensors with requires_grad=True.    # After this call w1.grad and w2.grad will be Tensors holding the gradient    # of the loss with respect to w1 and w2 respectively.    loss.backward()    # Manually update weights using gradient descent. Wrap in torch.no_grad()    # because weights have requires_grad=True, but we don't need to track this    # in autograd.    # An alternative way is to operate on weight.data and weight.grad.data.    # Recall that tensor.data gives a tensor that shares the storage with    # tensor, but doesn't track history.    # You can also use torch.optim.SGD to achieve this.    with torch.no_grad():        w1 -= learning_rate * w1.grad        w2 -= learning_rate * w2.grad        # Manually zero the gradients after updating weights        w1.grad.zero_()        w2.grad.zero_()</code></pre><p>3.1</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Code </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention is all your need</title>
      <link href="/2020/07/15/attention-is-all-your-need/"/>
      <url>/2020/07/15/attention-is-all-your-need/</url>
      
        <content type="html"><![CDATA[<p>Source：<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1706.03762.pdf</a></p><h3 id="Expanding-knowledge"><a href="#Expanding-knowledge" class="headerlink" title="Expanding  knowledge"></a><strong>Expanding  knowledge</strong></h3><blockquote><p>1.RNN(Recurrent Neural Network)循环神经网络：为了能更好的处理前后相关的sequence信息提出了RNN。假设该网络在输入为xt，隐藏层为st，输出为ot，前提下，st的值不仅取决于xt，还取决于st-1</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201009195053782.png" alt=""></p><p>提出RNN的原因是CNN和人工翻译都假设输入输出的相互独立的，但RNN可以解决输入输出不独立的情况，将神经网络加入记忆。再使用激活函数进行非线性映射过滤无用的信息，只记住重要的信息，使用softmax来进行预测下一个词出现的概率，预测时要带权重矩阵。</p><p>[^与CNN相同，RNN每个cell都共享（U,V,W),极大降低计算量]: </p><p>双向RNN是在RNN的基础上预测当前状态需要考虑前后的信息。</p><p>2.CNN(Conventional Neural Network)卷积神经网络</p><p>3.LSTM长短时记忆（Long Short Time Memory)：</p><p>1.处理和预测时间序列中间隔和延迟相对较长的重要事件（在词汇预测中如果关联词相差较远，RNN就会出现“梯度消失”的问题</p><p>2.三种门：遗忘门（丢弃的信息）、输入门（新加入的信息）、输出门（输出的信息）</p></blockquote><p><strong>attention机制</strong></p><p>与RNN,CNN不同，完全采用的是attention机制，具有更强的的并行性、节约了训练的时间</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>seq翻译模型主要基于复杂的recurrent or convolution 神经网络，它包括一个解码器一个编码器，表现最好的模型也通过注意力机制连接了解码编码器。</p><p><strong>S</strong>:Transformer仅仅基于注意力机制，与CNN,RNN均无关，发现Transformer具有更好的并行性，使用了更少的时间去训练。</p><p><strong>C</strong>:28.4BLEU on WMT2014 English-to-German translation task,我们发现Transformer也可很好的推广到其他任务，like English constituency parsing both with large and limited training data.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Recurrent model通常沿着输入输出的标记位置进行计算，将position与当前时间对齐生成隐藏状态ht，作为ht-1的函数以及position t的输入，对于长序列的处理至关重要，因为内存有限制，最近通过factorization tricks and conditional computation提高了计算效率，后者提高了模型的表现，但顺序计算的限制依旧存在。</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>基础是Extended Neural GPU,ByteNet,ConvS2S,它们中两个词汇之间的依赖关系与两者之间的距离反相关，这样如果两个词汇的距离太远它们的依赖性就很难体现。Transformer模型针对这种缺陷提出了Multi-Head Attention，Transformer模型没有使用RNN,CNN，它全部使用了attention机制对整个机制进行监控。</p><h3 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200716234419464.png" alt="image-20200716234419464"></p><p><strong>Encoder: 6 identical layers,Each layer has 2 sub-layers.One is Multi-Head Attenion,The other is a feed-forward network.Between the two sub-layers,there is a residual connection followed by layer normalization(dmodel=512).output is **<br>$$<br>LayerNorm(x+Sublater(x))<br>$$<br>**Decoder:compared with encoder,the decoder has 3 sub-layers,the 3rd layer perform multi-head attention over the output of the encoder stack.The attention ensures the prediction for position i depend only on the pre-position.</strong></p><blockquote><p><strong>basic knowledge</strong></p><ol><li><p>Embedding：引用one-hot方法词向量会很高维而且稀疏，使用Emedding更能找出词向量的相似性，这样就可以进行降维操作。计算嵌入矩阵前首先确定潜在因子，将个别单词用潜在因子组成的向量进行表示，其他单词可以用矩阵中向量的索引表示，探索具有相似性的词语，利用降维技术对词语进行相似性可视化。</p></li><li><p>positional encoding</p><p>对位置不敏感的模型（模型的输出不随着文本数据顺序的改变而改变）分为两类，Sinusoidal Positional Encoding（相对）和Learned Positional Encoding.（绝对）</p><p>[^绝对是对不同位置随机初始化一个position embedding,相对位置向量：用正余弦函数分别表示绝对位置，然后用乘积表示绝对位置，complex embedding使用复数域上连续函数来编码词在不同位置的表示]:</p></li><li><p>AR模型(Auto-regressive)and MA(Moving-average)模型</p><ul><li>AR自回归模型</li></ul><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201008110340470.png" style="zoom: 50%;"><p>[^本身前面的数据影响后面的数据（自相关），ut表示白噪声，在时间序列中数值随机波动，θ自回归系数]: </p><ul><li><p>MA移动平均模型</p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201008110842915.png" alt="image-20201008110842915" style="zoom: 50%;"></li></ul><p>[^对时间序列的白噪声序列进行加权和，得到移动平均方程]: </p></li></ol></blockquote><p><strong>Attention</strong></p><p>the output is computed as a weighted sum of th values,where the weight assigned to each value is computed by a function of query with the corrsponding key.</p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20201009100048322.png" alt=""></p><h2 id="1-Transformer-模型直观认识"><a href="#1-Transformer-模型直观认识" class="headerlink" title="1 Transformer 模型直观认识"></a>1 Transformer 模型直观认识</h2><p>首先来说一下transformer和LSTM的最大区别, 就是LSTM的训练是<strong>迭代</strong>的, 是一个接一个字的来, 当前这个字过完LSTM单元, 才可以进下一个字, 而transformer的训练是<strong>并行</strong>的, 就是所有字是全部同时训练的, 这样就大大加快了计算效率, transformer使用了位置嵌入 (𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔) 来理解语言的顺序, 使用自注意力机制和全连接层来进行计算, 这些后面都会详细讲解.</p><p>transformer模型主要分为两大部分, 分别是编码器和解码器, <strong>编码器</strong>负责把自然语言序列映射成为隐藏层(下图中第2步用九宫格比喻的部分), 含有自然语言序列的数学表达. 然后<strong>解码器</strong>把隐藏层再映射为自然语言序列, 从而使我们可以解决各种问题, 如情感分类, 命名实体识别, 语义关系抽取, 摘要生成, 机器翻译等等, 下面我们以机器翻译为例简单说一下下图的每一步都做了什么:</p><ol><li>输入自然语言序列到编码器: Why do we work?(为什么要工作);</li><li>编码器输出到隐藏层, 再输入到解码器;</li><li>输入 &lt;𝑠𝑡𝑎𝑟𝑡&gt; (起始)符号到解码器;</li><li>得到第一个字”为”;</li><li>将得到的第一个字”为”落下来再输入到解码器;</li><li>得到第二个字”什”;</li><li>将得到的第二字再落下来, 直到解码器输出 &lt;𝑒𝑛𝑑&gt; (终止符), 即序列生成完成</li></ol><h2 id="2-Transformer-Block结构图"><a href="#2-Transformer-Block结构图" class="headerlink" title="2 Transformer Block结构图"></a>2 Transformer Block结构图</h2><p><img src="https://img-blog.csdnimg.cn/20200630151025713.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODM4NjQz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>编码器的输入X是一个自然语言序列，它的维度是[batch size,sequence length]，batch size是一次训练句子的个数，sequence length是句子的长度。它通过查阅字向量表或者其他方式得到每个字embedding dimension维的嵌入向量。</p><p>上面是一个transformer block，实际网络中可以堆叠多个block。</p><p>我们通过编码器输出的X h i d d e n X_{hidden}<em>X*</em>h<strong>i</strong>d<strong>d</strong>e*<em>n</em>就是隐藏层，下面详细介绍每个模块。</p><h3 id="2-1-𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙-𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔"><a href="#2-1-𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙-𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔" class="headerlink" title="2.1 𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔"></a>2.1 𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑒𝑛𝑐𝑜𝑑𝑖𝑛𝑔</h3><blockquote><p>加position encoding是因为trandformer与LSTM不同，不是循环生成的</p></blockquote><p>由于transformer模型没有循环神经网络的迭代操作, 所以我们必须提供每个字的位置信息给transformer, 才能识别出<strong>语言中的顺序关系</strong>.</p><p>注意, 我们<strong>一般以字为单位训练transformer模型</strong>, 数的线性变换来提供给模型位置信息:<br><strong>位置嵌入在e m b e d d i n g  d i m e n s i o n embedding \ dimension*e\</strong>m*<em>b*<em>e*</em>d*<em>d*</em>i*<em>n*</em>g\</em> *d*<em>i*</em>m*<em>e*</em>n*<em>s*</em>i*<em>o*</em>n*维度上随着维度序号增大, 周期变化会越来越慢**, .<br><img src="https://img-blog.csdnimg.cn/20200630152716607.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODM4NjQz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200630152723649.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODM4NjQz,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h3 id="2-2-𝑠𝑒𝑙𝑓-𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛-𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚"><a href="#2-2-𝑠𝑒𝑙𝑓-𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛-𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚" class="headerlink" title="2.2 𝑠𝑒𝑙𝑓 𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚"></a>2.2 𝑠𝑒𝑙𝑓 𝑎𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 𝑚𝑒𝑐ℎ𝑎𝑛𝑖𝑠𝑚</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201026162217543.png" alt="image-20201026162217543"><br><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201026162336219.png" alt=""></p><p><strong>Attention Mask</strong><br><img src="https://img-blog.csdnimg.cn/20200630152942384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0ODM4NjQz,size_16,color_FFFFFF,t_70" alt=""><br><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201026162415835.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20201026162449863.png" alt=""></p><h4 id="自己总结的图"><a href="#自己总结的图" class="headerlink" title="自己总结的图"></a>自己总结的图</h4><p><img src="Attention-is-all-your-need.assets/QQ%E5%9B%BE%E7%89%8720201027081151-1603757541161.jpg" alt=""></p><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h4><p>BERT（<strong>B</strong>idirectional <strong>E</strong>ncoder <strong>R</strong>epresentations from <strong>T</strong>ransformers）</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>1.<a href="https://blog.csdn.net/FrankieHello/article/details/80883147?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">https://blog.csdn.net/FrankieHello/article/details/80883147?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p>2.</p><p><a href="https://blog.csdn.net/qq_39521554/article/details/79337929?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param" target="_blank" rel="noopener">https://blog.csdn.net/qq_39521554/article/details/79337929?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.channel_param</a></p><p>3.<a href="https://blog.csdn.net/qq_35799003/article/details/84780289" target="_blank" rel="noopener">https://blog.csdn.net/qq_35799003/article/details/84780289</a></p><p>4.<a href="https://blog.csdn.net/qq_39422642/article/details/78676567" target="_blank" rel="noopener">https://blog.csdn.net/qq_39422642/article/details/78676567</a></p><p>5.<a href="https://www.bilibili.com/video/BV1Mt411J734" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Mt411J734</a><br><a href="https://github.com/aespresso/a_journey_into_math_of_ml" target="_blank" rel="noopener">https://github.com/aespresso/a_journey_into_math_of_ml</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> Origin PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MT </tag>
            
            <tag> Transformer </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MRC for NER</title>
      <link href="/2020/07/10/mrc-for-ner/"/>
      <url>/2020/07/10/mrc-for-ner/</url>
      
        <content type="html"><![CDATA[<h3 id="title"><a href="#title" class="headerlink" title="title"></a>title</h3><p><strong>A Unified MRC Framework for Named Entity Recognition</strong></p><blockquote><p><strong>Abstract:</strong>The task of named entity recognition (NER) is normally divided into nested NER and flat NER depending on whether named entities are nested or not. Models are usually separately developed for the two tasks, since sequence labeling models are only able to assign a single label to a particular token, which is unsuitable<br>fornested NER where a token may be assigned several labels.</p></blockquote><h3 id="concepts"><a href="#concepts" class="headerlink" title="concepts"></a>concepts</h3><ul><li><p>实体重叠</p><p>可能会出现实体重叠的问题，即一个句子“席慕容散文集是我最喜欢的书”。在这个句子中“席慕容”和“席慕容散文集”都是实体，并且有重叠的部分。但传统做法无法解决此类问题，因为一个token只属于一个tag。</p></li></ul><p><strong>Qusetion</strong></p><p>​        本文针对实体重叠问题提出了一种统一的框架，可以分别处理falt and nested NER task.</p><p><strong>Strategy</strong></p><p>​        与传统的序列标记问题不同的是，作者采用了MRC（machine reading comprehension)来进行任务完成。即提取两个实体需要提问两次得到答案。</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -NER -实体重叠 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>deep learning-Wuenda</title>
      <link href="/2020/07/07/deep-learning-wuenda/"/>
      <url>/2020/07/07/deep-learning-wuenda/</url>
      
        <content type="html"><![CDATA[<h3 id="4-1多功能"><a href="#4-1多功能" class="headerlink" title="4-1多功能"></a>4-1多功能</h3><h3 id=""><a href="#" class="headerlink" title=""></a><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200709194136749.png" alt=""></h3><p><strong>note</strong></p><ul><li><p>n=number of features</p></li><li><p>x(i)=input of ith training example</p></li><li><p>x(i)j=value of j feature in ith training example</p><p><strong>假设函数</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200709195313024.png" alt=""></p><p>[^默认x0=1]: </p></li></ul><p>  <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200709200300679.png" alt=""></p><p>特征缩放</p><p>​        -多个特征值所在的范围都在相近的范围内，此时梯度下降算法就会更快地收敛，特征值在-1/3-1/3之间较好，过大过小都不好</p><p><strong>均值归一化</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200709201212057.png" alt=""></p><p>代价函数：<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200724215558011.png" alt=""></p><h3 id="4-5多项式回归"><a href="#4-5多项式回归" class="headerlink" title="4-5多项式回归"></a><strong>4-5多项式回归</strong></h3><p>在多个参数的代价函数中，要分别对各个参数求偏导，分别设为零。</p><p>if 斯塔为实数而非向量，那么转化为二次函数的求最值</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200724215748186.png" alt=""></p><h6 id="Gradient-Descent-vs-Normal-Equation"><a href="#Gradient-Descent-vs-Normal-Equation" class="headerlink" title="Gradient Descent vs Normal Equation"></a><strong>Gradient Descent vs Normal Equation</strong></h6><p>Gradient Descent需要不断修正学习率α，需要迭代，在n（特征量的数量）很大的情况下也能很好的工作。</p><p>Normal Equation不需要修正学习率，需要计算XtX,在n很大的情况下时间复杂度为n³，费时</p><blockquote><p>如果矩阵不可逆（singular/degenerate)，使用pinv（X’<em>X)</em>X’*Y也可求出矩阵的逆</p><p>也可以使用正则化或者删除一些特征向量</p></blockquote><p><strong>overfitting</strong>(高方差)：模型过多数据集过少</p><p>泛化能力：训练得到的模型适应新训练集的能力。</p><p>出现过拟合解决的方法：</p><ul><li><p>Reduce number of features.</p><p> -保留舍弃</p><p> -选择适当的模型（model  selection algorithm)</p></li><li><p>正则化(Regulation)</p><p>-线性回归的正则化</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711173321699.png" alt=""></p></li></ul><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711175549803.png" alt=""></p><p>​            <img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711175628518.png" alt=""></p><p>[^if X不可逆则选用prinv函数]: </p><p>​        -logistic回归的正则化</p><p>​        多项式的参数的代价函数，会导致模型过于复杂，出现过拟合</p><p>​            对各个参数分别求偏导</p><h3 id="8-1神经网络（Neural-Network"><a href="#8-1神经网络（Neural-Network" class="headerlink" title="8-1神经网络（Neural Network)"></a>8-1神经网络（Neural Network)</h3><h5 id="p43-8-1"><a href="#p43-8-1" class="headerlink" title="p43 8-1"></a><strong>p43 8-1</strong></h5><p>非线性回归</p><p>n过大时计算量过大</p><p>神经元与大脑</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200711184448515.png" alt=""></p><p>[^上标表示与第几层有关，下标表示与第几个神经元有关]: </p><p>神经元层分为三个：输入层、隐藏层（好多层）、输出层</p><p>XOR：异或：不同为一</p><p>NXOR：同或:相同为一</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713160945825.png" alt=""></p><p>（NOT x1)AND(NOT x2)</p><p>可能有很多隐藏层</p><p>每层对输入进行不同的处理，最后送入输出层</p><p>多元分类</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713162834051.png" alt=""></p><p>L表示层数</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713163115839.png" alt=""></p><p>[^h\theta(x)是k维向量，(h\theta(x))$i_d$表示神经网络输出向量的第i个元素]: </p><p><strong>Gradient computation</strong></p><p>1.前向传播</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200713164907850.png" alt=""></p><p>[^a(1)为第一层的激活值，g为sigmod激活函数，计算第二层的激活函数]: </p><hr><p><strong>Backporpagation algorithm</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200719182147968.png" alt=""></p><p>反向传播就是反过来计算误差值</p><p>总结神经网络</p><p><strong>1.确定架构</strong></p><p>就是选择神经元之间的连接方法</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200719212007267.png" alt=""></p><p><strong>2.训练神经网络</strong></p><ul><li>确定初始化权值</li><li>使用正向传播算法为每个x（i）计算出h(x(i))</li><li>使用代码计算出代价J（Θ）</li><li>使用反向传播计算出每个偏导数</li><li>使用梯度检测确定反向传播计算的偏导数和用数值计算的估计值之间的误差，最后确定算法是正确的</li></ul><p>使用最优化算法来确定代价函数的最小值</p><h4 id="梯度代价函数"><a href="#梯度代价函数" class="headerlink" title="梯度代价函数"></a><strong>梯度代价函数</strong></h4><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200720084214076.png" alt=""></p><p>通过求偏导数无限接近于代价较小的点</p><h3 id="10-2评估假设"><a href="#10-2评估假设" class="headerlink" title="10-2评估假设"></a>10-2评估假设</h3><p>将数据集70%用于训练集，30%用于测试集。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200720084824398.png" alt=""></p><h4 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a><strong>Model Selection</strong></h4><p><strong>Evaluating your hypothesis</strong></p><p>6:2:2(训练集：交叉验证集：测试集)</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200720090804228.png" alt=""></p><p><strong>Model selection</strong></p><p>用验证集或者交叉验证集来选择模型，评估泛化误差（防止过拟合）</p><p>ps：与前面的验证集不同，前面的验证集只能选择合适的模型，无法评估泛化能力。</p><p><strong>10-4偏差与方差</strong>（欠拟合与过拟合）</p><blockquote><p>偏差：预测值的期望与真实值之间的差距，偏差越大，越偏离真实数据</p><p>方差：描述预测值的变化范围，离散程度</p></blockquote><p>训练集和交叉验证集的代价计算公式</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721103256057.png" alt=""></p><p>两种误差的变化趋势</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721103150199.png" alt=""></p><p><strong>10-5结合正则化</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721103958207.png" alt=""></p><p>选择使代价最小的λ值</p><h3 id="10-6学习曲线"><a href="#10-6学习曲线" class="headerlink" title="10-6学习曲线"></a>10-6学习曲线</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721113346897.png" alt=""></p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20200721162202446.png" alt=""></p><h3 id="11-1确定执行的优先级"><a href="#11-1确定执行的优先级" class="headerlink" title="11-1确定执行的优先级"></a>11-1确定执行的优先级</h3><p><strong>example</strong>：垃圾邮件分类器</p><h3 id="11-2误差分析"><a href="#11-2误差分析" class="headerlink" title="11-2误差分析"></a>11-2误差分析</h3><blockquote><ol><li>建立一个简单的模型</li><li>画出学习曲线以及分析</li><li>误差分析（找出需要特殊处理的样本）在交叉验证集上进行</li></ol></blockquote><h3 id="11-3不对称分类的误差分析"><a href="#11-3不对称分类的误差分析" class="headerlink" title="11-3不对称分类的误差分析"></a>11-3不对称分类的误差分析</h3><p>Cancer classification example</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200721223025519.png" alt=""></p><p>precision表示交叉部分占所有预测值的比例</p><p>recall表示交叉部分占总的正例的比例</p><h3 id="11-5机器学习数据"><a href="#11-5机器学习数据" class="headerlink" title="11-5机器学习数据"></a>11-5机器学习数据</h3><h3 id="12-1优化数据"><a href="#12-1优化数据" class="headerlink" title="12-1优化数据"></a>12-1优化数据</h3><h4 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h4><ol><li><p>logistic回归</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200722153941516.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200722154423535.png" alt=""></p></li></ol><blockquote><p>SVM：Support Vector Machine,向量机：用supervised learning对数据进行二元分类的广义分类器</p></blockquote><h4 id="12-3大间隔分类器"><a href="#12-3大间隔分类器" class="headerlink" title="12-3大间隔分类器"></a>12-3大间隔分类器</h4><blockquote><p><strong>向量内积</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200722161211108.png" alt="">        </p></blockquote><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200723113904597.png" alt=""></p><p>为了达到向量机优化算法的目的，要使参数θ的取值范围尽可能小，为了保持p*θ&gt;=1（正样本）恒成立，p的取值应该尽可能大，p为样本点在向量参数θ上的投影，显然右图的投影长度更长。（即正负样本与分类线之间的距离更大）</p><h3 id="12-核函数"><a href="#12-核函数" class="headerlink" title="12-*核函数"></a>12-*核函数</h3><h5 id="非线性函数的决策边界"><a href="#非线性函数的决策边界" class="headerlink" title="非线性函数的决策边界"></a>非线性函数的决策边界</h5><p>[^要确定计算边界要构造一个很复杂的多项式函数θ，但是过于复杂，就提出核函数]: </p><p><strong>高斯核函数</strong></p><p>[^||x-l(1)||是欧式距离]: </p><p>1.选择标记点</p><p>2.用高斯核函数计算f值</p><p>3.代入预测函数计算结果是1or0</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200723163128297.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200723162655307.png" alt=""></p><p>** how to choose landmarks**：直接将训练样本点的位置作为标记点的位置</p><p>计算fi=similarity(x,l^i)</p><p><strong>SVM parameters</strong></p><h5 id="SVM应用"><a href="#SVM应用" class="headerlink" title="*SVM应用 *"></a><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200723214650858.png" alt="">*<em>SVM应用 *</em></h5><p>1.选择内核参数C和内核</p><p>[^没有内核就是线性内函数，通过计算参数不等式来确定取值]: </p><h3 id="13-1无监督学习"><a href="#13-1无监督学习" class="headerlink" title="13-1无监督学习"></a>13-1无监督学习</h3><blockquote><p>提出聚类算法-分类无标签的数据集</p></blockquote><h3 id="13-2K-means算法"><a href="#13-2K-means算法" class="headerlink" title="13-2K-means算法"></a>13-2K-means算法</h3><p><strong>cluster assignment</strong></p><ol><li>簇分配：随机生成两个聚类中心</li><li>根据距离聚类中心更近的原则进行分类</li><li>移动聚类中心：计算所有红类点的坐标均值，将聚类中心移动到该点上，继续进行2操作直至聚类中心不在移动</li></ol><p><strong>non-separated clusters</strong></p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.bilibili.com/video/BV164411b7dx?p=76" target="_blank" rel="noopener">https://www.bilibili.com/video/BV164411b7dx?p=76</a></p><h3 id="13-3优化函数"><a href="#13-3优化函数" class="headerlink" title="13-3优化函数"></a>13-3优化函数</h3><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727092604110.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727092711287.png" alt=""></p><h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>为了避局部最优化，初始选择多次局部最优化。具体过程如下：</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727093439675.png" alt=""></p><h3 id="13-5选择聚类数量"><a href="#13-5选择聚类数量" class="headerlink" title="13-5选择聚类数量"></a>13-5选择聚类数量</h3><p>[^最常见就是手动选择。需要结合自己的经验，还可以借助]: </p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727094948688.png" alt=""></p><h3 id="14-1目标Ⅰ：数据压缩"><a href="#14-1目标Ⅰ：数据压缩" class="headerlink" title="14-1目标Ⅰ：数据压缩"></a>14-1目标Ⅰ：数据压缩</h3><blockquote><p>节省内存或者硬盘空间，加快算法计算速度。</p></blockquote><p><strong>Data Compreesion</strong></p><p>降维可以使用投影的方法</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727100241361.png" alt=""></p><h3 id="14-2目标Ⅱ：可视化数据"><a href="#14-2目标Ⅱ：可视化数据" class="headerlink" title="14-2目标Ⅱ：可视化数据"></a>14-2目标Ⅱ：可视化数据</h3><p><strong>Data Visualization</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727100835002.png" alt=""></p><p><strong>Reduce 50D to 2D</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727100913558.png" alt=""></p><h3 id="14-3-4主成分分析问题规划（PCA"><a href="#14-3-4主成分分析问题规划（PCA" class="headerlink" title="14-3-4主成分分析问题规划（PCA)"></a>14-3-4主成分分析问题规划（PCA)</h3><p>[^PCA：寻找一个低维平面，使得数据投影在直线上的垂直距离平方和达到最小值。如下图所示，如果选择品红色的直线而非大红色的直线，如果选择品红色直线，数据点需要移动很长的距离才能到达投影直线。]: </p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20200727104627377.png" alt="image-20200727104627377"></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727105537284.png" alt="image-20200727105537284"></p><p>PCA选择计算的是垂直距离，linear regression选择的是与直线的正交距离</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727155808024.png" alt="image-20200727155808024"></p><p>[^svd：奇异值分解]: </p><p>$$<br>[U,S,V] = svd(Sigma)</p><p>sigma is a n*n matrix.<br>$$</p><p>取U矩阵的前k列，与矩阵X相乘得到矩阵Z</p><h4 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200727160229473.png" alt="image-20200727160229473"></h4><h3 id="14-5主成分数量选择"><a href="#14-5主成分数量选择" class="headerlink" title="14-5主成分数量选择"></a>14-5主成分数量选择</h3><p><strong>choose k(number of principal components</strong></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> VideoClass </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MNER-Multimodal Entity Span Detection</title>
      <link href="/2020/07/06/improving-multimodal-named-entity-recognition-via-entity-spandetection-with-unified-multimodal-transformer/"/>
      <url>/2020/07/06/improving-multimodal-named-entity-recognition-via-entity-spandetection-with-unified-multimodal-transformer/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>Named-entity recognition</strong> (NER) (also known as <strong>entity identification</strong>, <strong>entity chunking</strong> and <strong>entity extraction</strong>) is a subtask of <a href="https://encyclopedia.thefreedictionary.com/Information+extraction" target="_blank" rel="noopener">information extraction</a> that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.</p></blockquote><h4 id="A-example-of-NER"><a href="#A-example-of-NER" class="headerlink" title="A example of NER:"></a>A example of NER:</h4><table><thead><tr><th>Jim bought 300 shares of Acme Corp. in 2006.</th></tr></thead><tbody><tr><td>[Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.</td></tr></tbody></table><p><strong>level</strong>:ACL2020</p><p><strong>author:</strong>Jianfei Yu</p><p><strong>keywords:</strong>MNER,Entity Span Detection</p><h2 id="Qusetions"><a href="#Qusetions" class="headerlink" title="##Qusetions"></a>##Qusetions</h2><blockquote><h3 id="MNER-drawbacks"><a href="#MNER-drawbacks" class="headerlink" title="MNER drawbacks"></a>MNER drawbacks</h3></blockquote><ul><li><p>the words are insensitive to the visual context</p><p>现有的方法侧重模态间交互进行建模，因为单词的隐藏层表示仍然基于文本上下文，对视觉上下文不敏感。</p><p>忽略了合并视觉信息的误差。关联的图片信息只包括句子中的一两个实体，不涉及其他实体，这样会使其他实体无法识别。</p></li><li><p>most of the words ignore the bias brought by the visual context</p></li></ul><h4 id=""><a href="#" class="headerlink" title=""></a></h4><h3 id="Strategy"><a href="#Strategy" class="headerlink" title="Strategy:"></a>Strategy:</h3><p>1.main strategy:多通道交互模块（MMI）：standard Transformer layer+cross-model attention mechanism</p><p>2.auxiliary task:leverage purely text-based entity span detection</p><p>Consequence:</p><p>achieves the new state-of-the-artperformance on two benchmark datasets.</p><h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><p> Overall Architecture of Our Unified Multimodal Transformer.</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200708184644333.png" alt="image-20200708184644333"></p><h3 id="Transformer模型"><a href="#Transformer模型" class="headerlink" title="Transformer模型"></a>Transformer模型</h3><h5 id="采用encoder-decoder模型。与Attention相似"><a href="#采用encoder-decoder模型。与Attention相似" class="headerlink" title="采用encoder-decoder模型。与Attention相似"></a>采用encoder-decoder模型。与Attention相似</h5><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20200708094459080.png" alt="image-20200708094459080"></p><p><strong>基本内部结构</strong>如图所示，进入Encoder层前先将单词进行Emebedding操作，self-attention操作后送入前馈神经网络，也可并行进行self-attention和前馈神经网络。</p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20200708095052382.png" alt="image-20200708095052382"></p><h4 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a><strong>BERT</strong></h4><ul><li><p>全称Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder</p></li><li><p><strong>创新点：</strong>pre-train：Masked LM+Next Sentence Prediction</p><p><strong>MLM（Masked LM)</strong>可以理解为完形填空，作者会随机mask每一个句子中15%的词，用其上下文来做预测，例如：<code>my dog is hairy → my dog is [MASK]</code></p><p>此处将hairy进行了mask处理，然后采用非监督学习的方法预测mask位置的词是什么，但是该方法有一个问题，因为是mask15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过，为了解决这个问题，作者做了如下的处理：</p><ul><li>80%的时间是采用[mask]，my dog is hairy → my dog is [MASK]</li><li>10%的时间是随机取一个词来代替mask的词，my dog is hairy -&gt; my dog is apple</li><li>10%的时间保持不变，my dog is hairy -&gt; my dog is hairy</li></ul><p>那么为啥要以一定的概率使用随机词呢？这是因为transformer要保持对每个输入token分布式的表征，否则Transformer很可能会记住这个[MASK]就是”hairy”。至于使用随机词带来的负面影响，文章中解释说,所有其他的token(即非”hairy”的token)共享15%*10% = 1.5%的概率，其影响是可以忽略不计的。Transformer全局的可视，又增加了信息的获取，但是不让模型获取全量信息。<br>注意：</p><ul><li>有参数dupe_factor决定数据duplicate的次数。</li><li>其中，create_instance_from_document函数，是构造了一个sentence-pair的样本。对每一句，先生成[CLS]+A+[SEP]+B+[SEP]，有长（0.9）有短（0.1），再加上mask，然后做成样本类object。</li><li>create_masked_lm_predictions函数返回的tokens是已经被遮挡词替换之后的tokens</li><li>masked_lm_labels则是遮挡词对应位置真实的label。</li></ul><h4 id="Next-Sentence-Prediction"><a href="#Next-Sentence-Prediction" class="headerlink" title="Next Sentence Prediction"></a>Next Sentence Prediction</h4><p>选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。<br>个人理解：</p><ul><li>Bert先是用Mask来提高视野范围的信息获取量，增加duplicate再随机Mask，这样跟RNN类方法依次训练预测没什么区别了除了mask不同位置外；</li><li>全局视野极大地降低了学习的难度，然后再用A+B/C来作为样本，这样每条样本都有50%的概率看到一半左右的噪声；</li><li>但直接学习Mask A+B/C是没法学习的，因为不知道哪些是噪声，所以又加上next_sentence预测任务，与MLM同时进行训练，这样用next来辅助模型对噪声/非噪声的辨识，用MLM来完成语义的大部分的学习。</li></ul></li></ul><h4 id="positional-Encoding"><a href="#positional-Encoding" class="headerlink" title="positional Encoding"></a><strong>positional Encoding</strong></h4><p>Transformer中缺少一种解释单词顺序的方法，positional Encoding维度和embedding一样，可以通过它计算出任意两个词之间的距离，最终将它和Embedding相加输入下一层即可</p><h4 id="self-attention"><a href="#self-attention" class="headerlink" title="self-attention"></a><strong>self-attention</strong></h4><ol><li>定义三个向量：Query,Key,Value(三个矩阵是embedding向量与三个随机矩阵相乘的结果，eg：维度（64，128)，注意第二个维度与embedding向量的维度相同</li><li>scores=Q*K将结果除以1提到的第一个维度的开方得到的是softmax</li></ol><p>该词代表的是每个词对于当前位置的词的相关性大小。将value和softmax相乘得到的各个结果进行相加得到的结果即为self-attention在当前节点的值</p><p><strong>Resnet</strong></p><p>​    ResNet是一种残差网络,网络越深，获取的信息越多，特征也越丰富。但是根据实验表明，随着网络的加深，优化效果反而越差，测试数据和训练数据的准确率反而降低了。这是由于网络的加深会造成梯度爆炸和梯度消失的问题。</p><p><img src="C:/Users/89582/AppData/Roaming/Typora/typora-user-images/image-20200709111035572.png" alt="image-20200709111035572"></p><p>​                                                           Multimodal Interaction (MMI) Module.</p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> PaperLookThrough </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> MNER - NER(命名实体识别) </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PPT Useful Plugin</title>
      <link href="/2020/05/11/ruan-jian-gong-ju/ppt/ppt-useful-plugin/"/>
      <url>/2020/05/11/ruan-jian-gong-ju/ppt/ppt-useful-plugin/</url>
      
        <content type="html"><![CDATA[<h2 id="1、PPT美化大师"><a href="#1、PPT美化大师" class="headerlink" title="1、PPT美化大师"></a>1、<a href="http://meihua.docer.com/" target="_blank" rel="noopener">PPT美化大师</a></h2><p>“让制作专业精美PPT变得简单”“让不会做PPT的人，也能做好PPT”作为一款由wps的开发公司金山软件开发的PPT插件，自然来头不小，也不负大师之名。</p><h3 id="1-1、内容规划-生成模板"><a href="#1-1、内容规划-生成模板" class="headerlink" title="1.1、内容规划 生成模板"></a>1.1、内容规划 生成模板</h3><p>在美化大师工具栏选择新建一个PPT，选择美化大师中的“<font color="red">内容规划</font>”，输入PPT所需的大标题和一二级目录标题，并选择相应合适的“风格”，即可自动生成一份PPT模板。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092132264.png" alt=""></p><p>对模板背景不满意，还可以对其进行更换，选择自己喜欢的模板。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092147169.png" alt=""></p><h3 id="1-2、模板在线选择"><a href="#1-2、模板在线选择" class="headerlink" title="1.2、模板在线选择"></a>1.2、模板在线选择</h3><p>在美化大师工具栏下选择<font color="red">资源广场</font>，里面有海量免费和收费的高质量PPT模板，可以在线购买，直接导入PPT，类似于wps的在线模板</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092201791.png" alt=""></p><h3 id="1-3、图片、图形、幻灯片素材"><a href="#1-3、图片、图形、幻灯片素材" class="headerlink" title="1.3、图片、图形、幻灯片素材"></a>1.3、图片、图形、幻灯片素材</h3><p>还在为找不好看的图片发愁么，美化大师里面提供大量各行各业的图片素材，而且几乎都是没有背景的，可以一键运用</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092216002.png" alt=""></p><h3 id="1-4、导出各类格式（拼图、全图、图片、视频）"><a href="#1-4、导出各类格式（拼图、全图、图片、视频）" class="headerlink" title="1.4、导出各类格式（拼图、全图、图片、视频）"></a>1.4、导出各类格式（拼图、全图、图片、视频）</h3><p>美化大师提供PPT拼图，可以讲PPT以拼图形式直接呈现出来，一目了然。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092234474.png" alt=""></p><h3 id="1-5、批量删除（动画、切换页-备注）"><a href="#1-5、批量删除（动画、切换页-备注）" class="headerlink" title="1.5、批量删除（动画、切换页 备注）"></a>1.5、批量删除（动画、切换页 备注）</h3><p>当我们制作了一份PPT，里面有动画但是临时有演示要求说不需要动画，我们就可以这样一件删除所有动画，另外，还可以删除所有页切换或者备注</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092248221.png" alt=""></p><h3 id="1-6、收藏喜欢的幻灯片、图形、图片"><a href="#1-6、收藏喜欢的幻灯片、图形、图片" class="headerlink" title="1.6、收藏喜欢的幻灯片、图形、图片"></a>1.6、收藏喜欢的幻灯片、图形、图片</h3><p>网上下载了一份很漂亮的PPT模板，里面有些好看的图标素材、图片素材、或者幻灯片想要保存以便日后再用，美化大师提供了在线收藏的功能，对他们进行收藏，就不用保存到自己电脑本地要用时还得找好久或者误删了。</p><h2 id="2、onekey（OK）插件"><a href="#2、onekey（OK）插件" class="headerlink" title="2、onekey（OK）插件"></a>2、onekey（OK）插件</h2><p>onekey是由一位大师@只为设计开发完成的（收下我的小膝盖），从只有简单的几个功能，发展到现在已经有一百四五十个功能。功能涵盖形状、调色、图片、演示、辅助等方面。在图片形状处理方面尤为突出，这里主要为大家简单介绍一下，更多强大到可怕的功能，官方有详细的教程需要大家自己摸索了，</p><h3 id="2-1、一键转图"><a href="#2-1、一键转图" class="headerlink" title="2.1、一键转图"></a>2.1、一键转图</h3><p>OK插件的一键转图功能非常方便实用，如果我们要讲一个做好的图表、不能嵌入PPT的字体、形状组合、处理过的图片转成一张图片，通常的操作是保存到电脑为图片再插入，过程较为繁琐，有了这个功能，可以在PPT里面原位直接转为图片，方便实用。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092325222.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092338325.png" alt=""></p><h3 id="2-2、强大的图片处理"><a href="#2-2、强大的图片处理" class="headerlink" title="2.2、强大的图片处理"></a>2.2、强大的图片处理</h3><p>ok插件强大的图片处理功能好用到爆，提供了正片叠底、滤色、柔光、反相，图片色相、图片马赛克、图片分割、形状吸附到路径、形状取图片像素、多页统一、特殊选中等等功能。例：一键虚化、一键马赛克</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092354557.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092410789.png" alt=""></p><h3 id="2-3、强大的形状处理"><a href="#2-3、强大的形状处理" class="headerlink" title="2.3、强大的形状处理"></a>2.3、强大的形状处理</h3><p>OK插件提供了强大的形状处理功能，覆盖导入、去除、复制、文本等等，例如可以导入EMF(一种PPT支持的矢量图片文件格式，在PPT中可以通过取消组合来得到矢量形状)、一键拆分段落</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092430143.png" alt=""></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092442500.png" alt=""></p><p>另外，OK插件还有颜色处理、三维处理等高大上功能，总之，OK插件是一款非常强大的PPT插件，官方也出了很多相关教程，是一款非常专业好用的PPT神器。</p><h2 id="3、口袋动画PA"><a href="#3、口袋动画PA" class="headerlink" title="3、口袋动画PA"></a>3、<a href="http://www.papocket.com/" target="_blank" rel="noopener">口袋动画PA</a></h2><p>口袋动画(Pocket Animation,简称PA)是由大安工作室(作者:安少)独立开发出来的一款PowerPoint动画插件，顾名思义就是简化PPT动画设计过程、完善PPT动画相关功能。下面举例介绍几种常用功能</p><h3 id="3-1、动画删除"><a href="#3-1、动画删除" class="headerlink" title="3.1、动画删除"></a>3.1、动画删除</h3><p>PA插件可以一键去除对象动画、幻灯片动画、整个PPT文档动画，这对于做了很多动画后领导临时要求全部删除动画的人是福音，简直节省效率神器</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092458078.png" alt=""></p><h3 id="3-2、动画序列"><a href="#3-2、动画序列" class="headerlink" title="3.2、动画序列"></a>3.2、动画序列</h3><p>对于经常要设置相同动画序列来说，这个功能超级实用，例如可以批量设置动画延迟时间，包括固定延迟、随机延迟、公式延迟等，再也不用一个一个区重复操作了。快速制作动画，就试试PA插件的这个功能吧</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092515964.png" alt=""></p><h3 id="3-3、颜色替换"><a href="#3-3、颜色替换" class="headerlink" title="3.3、颜色替换"></a>3.3、颜色替换</h3><p>PA的颜色替换功能可谓是十足的福利啊，可以将整个PPT的颜色由一种全部替换为另外一种，一键更改PPT的配色，简单又实用，无论是表格、图表，还是文本线条，它都能一键更改，再也不用一个一个改配色了，有它足以。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092530555.png" alt=""></p><h3 id="3-4、动画库"><a href="#3-4、动画库" class="headerlink" title="3.4、动画库"></a>3.4、动画库</h3><p>PA提供了一些大神为我们预先设计好的动画效果，我们可以在制作PPT的时候随时调用，省去了大量制作动画的时间。对于喜欢PPT动画的人来说，PA插件绝对是一款好用的神器。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092545705.png" alt=""></p><h3 id="3-5、高级动画设计"><a href="#3-5、高级动画设计" class="headerlink" title="3.5、高级动画设计"></a>3.5、高级动画设计</h3><p>PA动画为我们的PPT动画设计带来了更多设计灵感和想象空间，它提供了众多强大的动画设计功能，简化了PPT的动画制作流程，提供了更多原有PPT很难做到的动画功能，使我们的动画制作出来更快、更和谐。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092559627.png" alt=""></p><h2 id="4、-iSlide"><a href="#4、-iSlide" class="headerlink" title="4、 iSlide"></a>4、 <a href="https://www.islide.cc/download" target="_blank" rel="noopener">iSlide</a></h2><p>iSlide是升级版的Nordri Tools，在Nordri Tools原来的工具属性功能外，增加了更多的素材，下面是iSlide的功能一栏，只能也只做简单介绍</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092617634.png" alt=""></p><h3 id="4-1、-一键优化"><a href="#4-1、-一键优化" class="headerlink" title="4.1、 一键优化"></a>4.1、 一键优化</h3><p>iSlide也提供了一键优化的功能，包括统一字体和统一段落，为设计提供了便捷快速的操作。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092639364.png" alt=""></p><h3 id="4-2、-素材资源"><a href="#4-2、-素材资源" class="headerlink" title="4.2、 素材资源"></a>4.2、 素材资源</h3><p>iSlide相对于Nordri Tools最大的特点就是增加了大量的素材资源，包括图标库、色彩库、图示库、以及智能图表。</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092708595.png" alt=""></p><h3 id="4-3、智能图表"><a href="#4-3、智能图表" class="headerlink" title="4.3、智能图表"></a>4.3、智能图表</h3><p>在这里要为大家重要讲解一下智能图表功能。iSlide提供了众多可视化的图表，更强大的事这些图表可以自由编辑和调整，数值颜色图标等都可以编辑替换。厉害了我的智能图表。<img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200514092804307.png" alt=""></p><p><strong>转载</strong>：<a href="https://www.jianshu.com/p/c986f4b09b93" target="_blank" rel="noopener">https://www.jianshu.com/p/c986f4b09b93</a></p><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Window CommandLine</title>
      <link href="/2020/04/27/ruan-jian-gong-ju/window/window-commandline/"/>
      <url>/2020/04/27/ruan-jian-gong-ju/window/window-commandline/</url>
      
        <content type="html"><![CDATA[<h2 id="1-PowerShell"><a href="#1-PowerShell" class="headerlink" title="1. PowerShell"></a>1. PowerShell</h2><pre class=" language-powershell"><code class="language-powershell"><span class="token comment" spellcheck="true">#  查看powershell 版本</span><span class="token function">get-host</span><span class="token variable">$host</span><span class="token punctuation">.</span>version<span class="token comment" spellcheck="true">#  新建目录</span><span class="token comment" spellcheck="true">#当前目录新建文件</span><span class="token function">new-item</span> FILENAME<span class="token punctuation">.</span>xxx <span class="token operator">-</span><span class="token function">type</span> file<span class="token comment" spellcheck="true">#当前目录新建文件夹</span><span class="token function">new-item</span> DIRECTORYNAME <span class="token operator">-</span><span class="token function">type</span> directory<span class="token comment" spellcheck="true">#在指定目录新建</span><span class="token function">new-item</span> TARGETDIR FILENAME<span class="token punctuation">.</span>xxx <span class="token operator">-</span><span class="token function">type</span> file<span class="token comment" spellcheck="true">#  重命名</span><span class="token comment" spellcheck="true">#把 C:/Scripts/Test.txt 重命名为 C:/Scripts/New_Name.txt:</span><span class="token function">Rename-Item</span> c:<span class="token operator">/</span>scripts<span class="token operator">/</span>Test<span class="token punctuation">.</span>txt new_name<span class="token punctuation">.</span>txt<span class="token comment" spellcheck="true">#  移动文件</span><span class="token function">Move-Item</span> c:\scripts\test<span class="token punctuation">.</span>zip c:\testX<span class="token comment" spellcheck="true">#  删除目录/文件</span><span class="token function">remove-item</span> file<span class="token comment" spellcheck="true">#显示文本内容</span><span class="token function">get-content</span> 1<span class="token punctuation">.</span>txt<span class="token comment" spellcheck="true">#罗列系统驱动器</span>get<span class="token operator">-</span>psdriver<span class="token comment" spellcheck="true">#下载文件</span>powershell <span class="token operator">-</span>Command <span class="token string">"(New-Object Net.WebClient).DownloadFile('https://ts', './src/ts')"</span><span class="token comment" spellcheck="true">#支持linux 文件  ls，dir，pwd，cat, more</span><span class="token comment" spellcheck="true"># 中文输出乱码</span>打开控制面板 <span class="token operator">-</span>> Change date<span class="token punctuation">,</span>time<span class="token punctuation">,</span>or number <span class="token operator">-</span>> 打开 “Region” 对话框选择 Administrative 选项卡，点击 change system locale选择</code></pre><h2 id="2-Cmd"><a href="#2-Cmd" class="headerlink" title="2. Cmd"></a>2. Cmd</h2><pre class=" language-shell"><code class="language-shell">where cmd #类似Linux中where 命令find /r 目录名 %变量名 in (匹配模式1,匹配模式2) do 命令for /r 目录名 %i in (匹配模式1,匹配模式2) do @echo %ifor /r TestDir %i in (*) do @echo %i  #将TestDir目录及所有子目录中所有的文件列举出来for /r TestDir %i in (*.txt) do @echo %i  #在TestDir目录所有子目录中找出所有的txt文件for /r TestDir %i in (.txt,.jpg) do @echo %i #找出所有的txt及jpg文件for /r TestDir %i in (test) do @echo %i  #找出所有文件名中包含test的文件Tree   #罗列文件目录</code></pre><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OperationSystem </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Typoro Command</title>
      <link href="/2020/04/26/ruan-jian-gong-ju/hexotypora/typoro-command/"/>
      <url>/2020/04/26/ruan-jian-gong-ju/hexotypora/typoro-command/</url>
      
        <content type="html"><![CDATA[<blockquote><p>​      Typora 是一个 Markdown 文本编辑器，它支持且仅支持 Markdown 语法的文本编辑。在 <a href="https://typora.io/" target="_blank" rel="noopener">Typora 官网</a> 上他们将 Typora 描述为 「A truly <strong>minimal</strong> markdown editor. 」</p></blockquote><h2 id="1-安装"><a href="#1-安装" class="headerlink" title="1. 安装"></a>1. 安装</h2><pre class=" language-shell"><code class="language-shell">#1 安装依赖包 sudo apt-get install libapt-pkg-dev  #2 安装、更新 sudo apt-get install apt-transport-httpssudo apt-get update#3 安装Typora源wget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add -sudo add-apt-repository ‘deb https://typora.io/linux ./‘sudo apt-get update#4 安装typora sudo apt-get install typora#首行缩进&emsp;&emsp;春天来了，又到了万物复苏的季节。#任务列表- [ ] 一次性水杯- [x] 西瓜#各种表情链接： https://www.webfx.com/tools/emoji-cheat-sheet/</code></pre><h2 id="2-图片排版"><a href="#2-图片排版" class="headerlink" title="2. 图片排版"></a>2. 图片排版</h2><p><strong>方法一：嵌入HTML代码</strong><br>使用img标签</p><pre class=" language-html"><code class="language-html">&lt;img src="./xxx.png" width = "300" height = "200" alt="图片名称" align=center /><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">'</span> <span class="token punctuation">'</span></span><span class="token style-attr language-css"><span class="token attr-name"> <span class="token attr-name">style</span></span><span class="token punctuation">='</span><span class="token attr-value"><span class="token property">float</span><span class="token punctuation">:</span>right<span class="token punctuation">;</span> <span class="token property">width</span><span class="token punctuation">:</span><span class="token number">300</span>px<span class="token punctuation">;</span><span class="token property">height</span><span class="token punctuation">:</span><span class="token number">100</span> px</span><span class="token punctuation">'</span></span><span class="token punctuation">/></span></span>#或者<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>div</span> <span class="token attr-name">align</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>center<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>   <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>图片地址<span class="token punctuation">"</span></span> <span class="token attr-name">height</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>300px<span class="token punctuation">"</span></span> <span class="token attr-name">alt</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>图片说明<span class="token punctuation">"</span></span> <span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>div</span><span class="token punctuation">></span></span></code></pre><p><strong>方法二：预定义类</strong></p><pre class=" language-html"><code class="language-html">#居中对齐，img间不要换行，否则识别不了<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>center</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>half<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>图片链接<span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>200<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>图片链接<span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>200<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>图片链接<span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>200<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>center</span><span class="token punctuation">></span></span>#左对齐并排<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>figure</span> <span class="token attr-name">class</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>third<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span><span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>200<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span><span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>200<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>img</span> <span class="token attr-name">src</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span><span class="token punctuation">"</span></span> <span class="token attr-name">width</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>200<span class="token punctuation">"</span></span><span class="token punctuation">/></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>figure</span><span class="token punctuation">></span></span></code></pre><h2 id="3-数学公式"><a href="#3-数学公式" class="headerlink" title="3. 数学公式"></a>3. 数学公式</h2><p><strong>开启行内公式</strong>：文件→偏好设置→Markdown，勾选内联公式，重启typora    </p><p><strong>分数，平方</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$\frac{7x+5}{1+y^2}$，$1/2$</td><td align="left">\frac{7x+5}{1+y^2} ,    1/2</td></tr></tbody></table><p><strong>下标</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$z=z_l$ , $z=z^1$</td><td align="left">下标： z=z_l,  上标 z=z^1</td></tr></tbody></table><p><strong>省略号</strong></p><table><thead><tr><th align="left">省略号</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">⋯</td><td align="left">\cdots</td></tr></tbody></table><p><strong>开根号</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$\sqrt{2};\sqrt[n]{3}$</td><td align="left">\sqrt{2};\sqrt[n]{3}</td></tr></tbody></table><p><strong>花括号</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$c(u)=\begin{cases} \sqrt\frac{1}{N}，u=0\ \sqrt\frac{2}{N}， u\neq0\end{cases}$</td><td align="left">c(u)=\begin{cases} \sqrt\frac{1}{N}，u=0\ \sqrt\frac{2}{N}， u\neq0\end{cases}     ,花括号</td></tr><tr><td align="left">$a \quad b$</td><td align="left">a \quad b  ,空格</td></tr></tbody></table><p><strong>矢量</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$\vec{a} \cdot \vec{b}=0$</td><td align="left">\vec{a} \cdot \vec{b}=0</td></tr></tbody></table><p><strong>积分</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$\int ^2_3 x^2 {\rm d}x$</td><td align="left">\int ^2_3 x^2 {\rm d}x</td></tr></tbody></table><p><strong>极限</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$\lim_{n\rightarrow+\infty} n$</td><td align="left">\lim_{n\rightarrow+\infty} n</td></tr></tbody></table><p><strong>累加</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$\sum \frac{1}{i^2}$</td><td align="left">\sum \frac{1}{i^2}</td></tr></tbody></table><p><strong>累乘</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$\prod \frac{1}{i^2}$</td><td align="left">\prod \frac{1}{i^2}</td></tr></tbody></table><p><strong>希腊字母</strong></p><table><thead><tr><th align="left">大写</th><th align="left">markdown</th><th align="left">小写</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">A</td><td align="left">A</td><td align="left">α</td><td align="left">\alpha</td></tr><tr><td align="left">B</td><td align="left">B</td><td align="left">β</td><td align="left">\beta</td></tr><tr><td align="left">Γ</td><td align="left">\Gamma</td><td align="left">γ</td><td align="left">\gamma</td></tr><tr><td align="left">Δ</td><td align="left">\Delta</td><td align="left">δ</td><td align="left">\delta</td></tr><tr><td align="left">E</td><td align="left">E</td><td align="left">ϵ</td><td align="left">\epsilon</td></tr><tr><td align="left"></td><td align="left"></td><td align="left">ε</td><td align="left">\varepsilon</td></tr><tr><td align="left">Z</td><td align="left">Z</td><td align="left">ζ</td><td align="left">\zeta</td></tr><tr><td align="left">H</td><td align="left">H</td><td align="left">η</td><td align="left">\eta</td></tr><tr><td align="left">Θ</td><td align="left">\Theta</td><td align="left">θ</td><td align="left">\theta</td></tr><tr><td align="left">I</td><td align="left">I</td><td align="left">ι</td><td align="left">\iota</td></tr><tr><td align="left">K</td><td align="left">K</td><td align="left">κ</td><td align="left">\kappa</td></tr><tr><td align="left">Λ</td><td align="left">\Lambda</td><td align="left">λ</td><td align="left">\lambda</td></tr><tr><td align="left">M</td><td align="left">M</td><td align="left">μ</td><td align="left">\mu</td></tr><tr><td align="left">N</td><td align="left">N</td><td align="left">ν</td><td align="left">\nu</td></tr><tr><td align="left">Ξ</td><td align="left">\Xi</td><td align="left">ξ</td><td align="left">\xi</td></tr><tr><td align="left">O</td><td align="left">O</td><td align="left">ο</td><td align="left">\omicron</td></tr><tr><td align="left">Π</td><td align="left">\Pi</td><td align="left">π</td><td align="left">\pi</td></tr><tr><td align="left">P</td><td align="left">P</td><td align="left">ρ</td><td align="left">\rho</td></tr><tr><td align="left">Σ</td><td align="left">\Sigma</td><td align="left">σ</td><td align="left">\sigma</td></tr></tbody></table><table><thead><tr><th align="left">大写</th><th align="left">markdown</th><th align="left">小写</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">T</td><td align="left">T</td><td align="left">τ</td><td align="left">\tau</td></tr><tr><td align="left">Υ</td><td align="left">\Upsilon</td><td align="left">υ</td><td align="left">\upsilon</td></tr><tr><td align="left">Φ</td><td align="left">\Phi</td><td align="left">ϕ</td><td align="left">\phi</td></tr><tr><td align="left"></td><td align="left"></td><td align="left">φ</td><td align="left">\varphi</td></tr><tr><td align="left">X</td><td align="left">X</td><td align="left">χ</td><td align="left">\chi</td></tr><tr><td align="left">Ψ</td><td align="left">\Psi</td><td align="left">ψ</td><td align="left">\psi</td></tr><tr><td align="left">Ω</td><td align="left">\Omega</td><td align="left">ω</td><td align="left">\omega</td></tr></tbody></table><p><strong>三角函数</strong></p><table><thead><tr><th align="left">三角函数</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">sin</td><td align="left">\sin</td></tr></tbody></table><p><strong>对数函数</strong></p><table><thead><tr><th align="left">算式</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">ln15</td><td align="left">\ln15</td></tr><tr><td align="left">log210</td><td align="left">\log_2 10</td></tr><tr><td align="left">lg7</td><td align="left">\lg7</td></tr></tbody></table><p><strong>关系运算符</strong></p><table><thead><tr><th align="left">运算符</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">±</td><td align="left">\pm</td></tr><tr><td align="left">×</td><td align="left">\times</td></tr><tr><td align="left">÷</td><td align="left">\div</td></tr><tr><td align="left">∑</td><td align="left">\sum</td></tr><tr><td align="left">∏</td><td align="left">\prod</td></tr><tr><td align="left">≠</td><td align="left">\neq</td></tr><tr><td align="left">≤</td><td align="left">\leq</td></tr><tr><td align="left">≥</td><td align="left">\geq</td></tr></tbody></table><p><strong>其它特殊字符</strong></p><table><thead><tr><th align="left">符号</th><th align="left">markdown</th></tr></thead><tbody><tr><td align="left">$\forall$</td><td align="left">\forall</td></tr><tr><td align="left">$\infty$</td><td align="left">\infty</td></tr><tr><td align="left">$\emptyset$</td><td align="left">\emptyset</td></tr><tr><td align="left">$\exists$</td><td align="left">\exists</td></tr><tr><td align="left">$\nabla$</td><td align="left">\nabla</td></tr><tr><td align="left">$\bot$</td><td align="left">\bot</td></tr><tr><td align="left">$\angle$</td><td align="left">\angle</td></tr><tr><td align="left">$\because$</td><td align="left">\because</td></tr><tr><td align="left">$\therefore$</td><td align="left">\therefore</td></tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo Blog Introduce</title>
      <link href="/2020/04/26/ruan-jian-gong-ju/hexotypora/hexo-blog-introduce/"/>
      <url>/2020/04/26/ruan-jian-gong-ju/hexotypora/hexo-blog-introduce/</url>
      
        <content type="html"><![CDATA[<blockquote><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is my very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a>.</p></blockquote><h2 id="Hexo-Introduce"><a href="#Hexo-Introduce" class="headerlink" title="Hexo Introduce"></a>Hexo Introduce</h2><blockquote><ul><li>Hexo is a fast, simple and powerful blog framework. You write posts in <a href="http://daringfireball.net/projects/markdown/" target="_blank" rel="noopener">Markdown</a> (or other markup languages) and Hexo generates static files with a beautiful theme in seconds.</li><li>Hexo+github+gitee blog deployment <a href="https://yafine66.gitee.io/posts/4ab2.html#toc-heading-60" target="_blank" rel="noopener">tutorial</a><ul><li>download Git&amp;&amp;Node.js</li><li>Github Register &amp;&amp; GithubPage Create</li><li>Configure Git user&amp;&amp;mail</li><li>Install Theme &amp;&amp; Config</li><li>Config Some Plugins</li></ul></li><li>Good Github Page Recommand:<ul><li><a href="https://mazhuang.org/" target="_blank" rel="noopener">https://mazhuang.org/</a></li><li><a href="http://www.liberxue.com/" target="_blank" rel="noopener">http://www.liberxue.com/</a></li><li><a href="https://rickfang666.github.io/about/" target="_blank" rel="noopener">https://rickfang666.github.io/about/</a></li><li><a href="https://ahrilove.top/" target="_blank" rel="noopener">https://ahrilove.top/</a></li></ul></li></ul></blockquote><h2 id="Hexo-Command"><a href="#Hexo-Command" class="headerlink" title="Hexo Command"></a>Hexo Command</h2><pre class=" language-bash"><code class="language-bash">$ <span class="token function">npm</span> <span class="token function">install</span> hexo -g <span class="token comment" spellcheck="true">#安装  </span>$ <span class="token function">npm</span> update hexo -g <span class="token comment" spellcheck="true">#升级  </span>$ hexo init <span class="token comment" spellcheck="true">#初始化</span>$ hexo new page <span class="token string">"categories"</span>  <span class="token comment" spellcheck="true">#新建页面</span><span class="token comment" spellcheck="true"># 简写</span>$ hexo n <span class="token string">"我的博客"</span> <span class="token operator">==</span> hexo new <span class="token string">"我的博客"</span> <span class="token comment" spellcheck="true">#新建文章</span>$ hexo p <span class="token operator">==</span> hexo publish$ hexo g <span class="token operator">==</span> hexo generate<span class="token comment" spellcheck="true">#生成</span>$ hexo s <span class="token operator">==</span> hexo server <span class="token comment" spellcheck="true">#启动服务预览  对跟配置文件修改需要重启</span>$ hexo d <span class="token operator">==</span> hexo deploy<span class="token comment" spellcheck="true">#部署</span><span class="token comment" spellcheck="true"># 服务器</span>$ hexo server <span class="token comment" spellcheck="true">#Hexo 会监视文件变动并自动更新，您无须重启服务器。</span>$ hexo server -s <span class="token comment" spellcheck="true">#静态模式</span>$ hexo server -p 5000 <span class="token comment" spellcheck="true">#更改端口</span>$ hexo server -i 192.168.1.1 <span class="token comment" spellcheck="true">#自定义 IP</span>$ hexo clean <span class="token comment" spellcheck="true">#清除缓存db.json 网页正常情况下可以忽略此条命令</span><span class="token comment" spellcheck="true">#需要删掉用命令新建的文章或页面时，只需要进入 Hexo 根目录下的 source 文件夹，删除对应文件或文件夹即可</span>$ hexo g <span class="token comment" spellcheck="true">#生成静态页面至public目录</span>$ hexo s <span class="token comment" spellcheck="true">#开启预览访问端口（默认端口4000，'ctrl + c'关闭server）</span>$ hexo d <span class="token comment" spellcheck="true">#将.deploy目录部署到GitHub</span><span class="token comment" spellcheck="true">#监视文件变动</span>hexo generate --watch <span class="token comment" spellcheck="true">#监视文件变动</span></code></pre><blockquote><table><thead><tr><th align="left">配置选项</th><th align="left">默认值</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">title</td><td align="left"><code>Markdown</code> 的文件标题</td><td align="left">文章标题，强烈建议填写此选项</td></tr><tr><td align="left">date</td><td align="left">文件创建时的日期时间</td><td align="left">发布时间，强烈建议填写此选项，且最好保证全局唯一</td></tr><tr><td align="left">author</td><td align="left">根 <code>_config.yml</code> 中的 <code>author</code></td><td align="left">文章作者</td></tr><tr><td align="left">img</td><td align="left"><code>featureImages</code> 中的某个值</td><td align="left">文章特征图，推荐使用图床(腾讯云、七牛云、又拍云等)来做图片的路径。如: <a href="https://yafine66.gitee.io/go.html?url=aHR0cDovL3h4eC5jb20veHh4LmpwZw==" target="_blank" rel="noopener">http://xxx.com/xxx.jpg</a></td></tr><tr><td align="left">top</td><td align="left"><code>true</code></td><td align="left">推荐文章（文章是否置顶），如果 <code>top</code> 值为 <code>true</code>，则会作为首页推荐文章</td></tr><tr><td align="left">cover</td><td align="left"><code>false</code></td><td align="left"><code>v1.0.2</code>版本新增，表示该文章是否需要加入到首页轮播封面中</td></tr><tr><td align="left">coverImg</td><td align="left">无</td><td align="left"><code>v1.0.2</code>版本新增，表示该文章在首页轮播封面需要显示的图片路径，如果没有，则默认使用文章的特色图片</td></tr><tr><td align="left">password</td><td align="left">无</td><td align="left">文章阅读密码，如果要对文章设置阅读验证密码的话，就可以设置 <code>password</code> 的值，该值必须是用 <code>SHA256</code> 加密后的密码，防止被他人识破。前提是在主题的 <code>config.yml</code> 中激活了 verifyPassword选项</td></tr><tr><td align="left">toc</td><td align="left"><code>true</code></td><td align="left">是否开启 TOC，可以针对某篇文章单独关闭 TOC 的功能。前提是在主题的 <code>config.yml</code> 中激活了 <code>toc</code> 选项</td></tr><tr><td align="left">mathjax</td><td align="left"><code>false</code></td><td align="left">是否开启数学公式支持 ，本文章是否开启 <code>mathjax</code>，且需要在主题的 <code>_config.yml</code> 文件中也需要开启才行</td></tr><tr><td align="left">summary</td><td align="left">无</td><td align="left">文章摘要，自定义的文章摘要内容，如果这个属性有值，文章卡片摘要就显示这段文字，否则程序会自动截取文章的部分内容作为摘要</td></tr><tr><td align="left">categories</td><td align="left">无</td><td align="left">文章分类，本主题的分类表示宏观上大的分类，只建议一篇文章一个分类</td></tr><tr><td align="left">tags</td><td align="left">无</td><td align="left">文章标签，一篇文章可以多个标签</td></tr><tr><td align="left">reprintPolicy</td><td align="left">cc_by</td><td align="left">文章转载规则， 可以是 cc_by, cc_by_nd, cc_by_sa, cc_by_nc, cc_by_nc_nd, cc_by_nc_sa, cc0, noreprint 或 pay 中的一个</td></tr></tbody></table></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux Operation</title>
      <link href="/2020/03/16/ruan-jian-gong-ju/linux/linux-operation/"/>
      <url>/2020/03/16/ruan-jian-gong-ju/linux/linux-operation/</url>
      
        <content type="html"><![CDATA[<h2 id="Linux-命令"><a href="#Linux-命令" class="headerlink" title="Linux 命令"></a>Linux 命令</h2><h3 id="链接-ln"><a href="#链接-ln" class="headerlink" title="链接  ln"></a>链接  ln</h3><pre class=" language-shell"><code class="language-shell">sudo ln -sf /usr/bin/g++-8 /usr/bin/g++ln - make links between filesSYNOPSIS       ln [OPTION]... [-T] TARGET LINK_NAME   (1st form)       ln [OPTION]... TARGET                  (2nd form)       ln [OPTION]... TARGET... DIRECTORY     (3rd form)       ln [OPTION]... -t DIRECTORY TARGET...  (4th form)</code></pre><h3 id="man-命令"><a href="#man-命令" class="headerlink" title="man 命令"></a>man 命令</h3><pre><code>man -b (向前翻一屏)  space (向后翻一屏)  /keyword 查找  n: 下一个whatis command # 查询命令执行什么功能</code></pre><h3 id="快捷键"><a href="#快捷键" class="headerlink" title="快捷键"></a>快捷键</h3><pre class=" language-bash"><code class="language-bash">Ctrl+c <span class="token comment" spellcheck="true">#在命令行下起着终止当前执行程序的作用，</span>Ctrl+d  <span class="token comment" spellcheck="true">#相当于exit命令，退出当前shell</span>win    <span class="token comment" spellcheck="true">#搜索浏览程序文件音乐文件</span>ctrl+L <span class="token comment" spellcheck="true">#清除屏幕</span>ctrl+A  <span class="token comment" spellcheck="true">#光标移到行首</span>super+R <span class="token comment" spellcheck="true"># terminal</span>ctrl+shift+prtsc  <span class="token comment" spellcheck="true">#截屏到剪切板</span>super+h <span class="token comment" spellcheck="true">#隐藏窗口</span>super+up <span class="token comment" spellcheck="true">#窗口最大化</span>super+down <span class="token comment" spellcheck="true">#窗口最小话</span></code></pre><h3 id="压缩包操作"><a href="#压缩包操作" class="headerlink" title="压缩包操作"></a>压缩包操作</h3><pre class=" language-bash"><code class="language-bash"><span class="token function">tar</span> -zxvf 4.1.2.tar.gzunzip -d /temp test.zip  <span class="token comment" spellcheck="true">#解压到指定的目录下，需要用到-d参数</span></code></pre><h3 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h3><pre class=" language-bash"><code class="language-bash"><span class="token function">wget</span> -O  <span class="token comment" spellcheck="true">#下载并以不同的文件名保存</span><span class="token function">wget</span> -b <span class="token comment" spellcheck="true">#后台下载   tail -f wget-log  查看下载速度</span><span class="token function">wget</span> –spider url <span class="token comment" spellcheck="true">#测试下载链接是否可用等等</span></code></pre><h3 id="软件安装命令-dpkg-apt-snap-ppa-使用"><a href="#软件安装命令-dpkg-apt-snap-ppa-使用" class="headerlink" title="软件安装命令 dpkg | apt | snap |ppa 使用:"></a>软件安装命令 dpkg | apt | snap |<strong>ppa 使用</strong>:</h3><pre class=" language-bash"><code class="language-bash">dpkg -p package-name  <span class="token comment" spellcheck="true">#显示包的具体信息</span>dpkg -s package-name  <span class="token comment" spellcheck="true">#报告指定包的状态信息    </span>dpkg -l                <span class="token comment" spellcheck="true">#显示所有已经安装的Deb包，同时显示版本号以及简短说明</span>dpkg -P            <span class="token comment" spellcheck="true">#删除一个包（包括配置信息）    </span>dpkg -A package_file  <span class="token comment" spellcheck="true">#从软件包里面读取软件的信息    </span>dpkg -i <span class="token operator">&lt;</span>.deb <span class="token function">file</span> name<span class="token operator">></span>  <span class="token comment" spellcheck="true">#安装软件    </span>apt update<span class="token operator">|</span><span class="token function">install</span><span class="token operator">|</span>upgradable<span class="token operator">|</span>remove<span class="token operator">|</span>purge<span class="token operator">|</span>search<span class="token comment" spellcheck="true"># tab键自动补全,apt下载时有锁</span><span class="token comment" spellcheck="true">#snap是一种全新的软件包管理方式，它类似一个容器拥有一个应用程序所有的文件和库，各个应用程序之间完全独立。所以使用snap包的好处就是它解决了应用程序之间的依赖问题，使应用程序之间更容易管理。但是由此带来的问题就是它占用更多的磁盘空间.snap软件包一般安装在/snap目录下</span>snap list <span class="token comment" spellcheck="true">#罗列</span>snap <span class="token function">find</span> <span class="token operator">|</span> <span class="token function">install</span> <span class="token operator">|</span> refresh <span class="token operator">|</span> remove packagesnap changes <span class="token comment" spellcheck="true"># 查看正在进行的下载</span>snap abort <span class="token function">id</span> <span class="token comment" spellcheck="true"># 停止下载</span><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> ppa-purgeTo purge a PPA, you must use the following command:<span class="token function">sudo</span> ppa-purge ppa:someppa/ppa     删除ppa 及对应软件<span class="token function">sudo</span> add-apt-repository ppa:someppa/ppa<span class="token function">sudo</span> apt update<span class="token function">sudo</span> add-apt-repository --remove ppa:someppa/ppa</code></pre><h3 id="网络命令-netstat-top"><a href="#网络命令-netstat-top" class="headerlink" title="网络命令 netstat ,top"></a>网络命令 netstat ,top</h3><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># net-tools   包括ifconfig,netstat 等网络工具</span>top: <span class="token comment" spellcheck="true">#查看电脑个进程占用资源情况  b 高亮显示当前进程.</span><span class="token function">netstat</span> -a :Listing all ports <span class="token punctuation">(</span>both TCP and UDP<span class="token punctuation">)</span> using option.<span class="token function">netstat</span> -l <span class="token keyword">:</span> active listening ports connections<span class="token function">netstat</span> -s <span class="token keyword">:</span> displays statistics by protocol<span class="token function">netstat</span> -i <span class="token keyword">:</span> show the network interface<span class="token function">netstat</span> -r <span class="token keyword">:</span> show the routing<span class="token function">netstat</span> -ie <span class="token keyword">:</span> like <span class="token function">ifconfig</span><span class="token function">netstat</span> -ap <span class="token operator">|</span> <span class="token function">grep</span> http <span class="token keyword">:</span> <span class="token function">find</span> the listening program<span class="token comment" spellcheck="true">#查找程序是否运行</span><span class="token comment" spellcheck="true">#pgrep command – Looks through the currently running bash processes on Linux and lists the process IDs (PID) on screen.</span>pgrep nginx<span class="token comment" spellcheck="true">#pidof command – Find the process ID of a running program on Linux or Unix-like system</span>pidof nginx<span class="token comment" spellcheck="true">#ps command – Get information about the currently running Linux or Unix processes, including their process identification numbers (PIDs).</span><span class="token function">ps</span> aux <span class="token operator">|</span> <span class="token function">grep</span> nginx</code></pre><h3 id="设置代理"><a href="#设置代理" class="headerlink" title="设置代理"></a>设置代理</h3><pre class=" language-shell"><code class="language-shell">set | grep -i all_proxy# Unset socks proxyunset all_proxy     #根据上个命令输出决定是否用大写还是小写unset ALL_PROXY      #系统中的设置还在# Install missing dependencies:pip install pysocks# Reset proxysource ~/.bashrc</code></pre><h3 id="evince-pdf-文件查看"><a href="#evince-pdf-文件查看" class="headerlink" title="evince  pdf 文件查看"></a>evince  pdf 文件查看</h3><h2 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h2><h3 id="CPU-温度"><a href="#CPU-温度" class="headerlink" title="CPU 温度"></a>CPU 温度</h3><pre class=" language-shell"><code class="language-shell">sudo apt install lm-sensors hddtempsudo sensors-detectsensors#如果有虚拟温度显示sudo apt install psensor  #设置开机自启,监控温度</code></pre><h3 id="VimOp"><a href="#VimOp" class="headerlink" title="VimOp"></a>VimOp</h3><table><thead><tr><th align="center">操作符</th><th align="right">作用</th></tr></thead><tbody><tr><td align="center"><code>control</code>+<code>A</code></td><td align="right">移动光标至行首</td></tr><tr><td align="center"><code>control</code>+<code>E</code></td><td align="right">移动光标至行尾</td></tr><tr><td align="center"><code>control</code>+<code>U</code></td><td align="right">删除整行命令</td></tr><tr><td align="center"><code>control</code>+<code>K</code></td><td align="right">删除光标后面的内容</td></tr><tr><td align="center"><code>option</code>+<code>←</code>、<code>→</code></td><td align="right">按词组移动光标</td></tr><tr><td align="center">!!</td><td align="right">执行上一条命令</td></tr><tr><td align="center">！</td><td align="right">重复命令，例如$ ! -3 执行前面三条命令; $ ! pod 重复最近一次pod命令</td></tr><tr><td align="center">|</td><td align="right">将左侧的命令结果人给右侧命令</td></tr><tr><td align="center">&gt;</td><td align="right">等待前一天命令结束</td></tr><tr><td align="center">&amp;&amp;</td><td align="right">多条命令同时执行</td></tr><tr><td align="center">&amp;</td><td align="right">不管前面执行是否成功都执行后面的命令</td></tr></tbody></table><h3 id="Cmake"><a href="#Cmake" class="headerlink" title="Cmake"></a>Cmake</h3><ul><li>中find_package() 工作原理：<a href="https://www.jianshu.com/p/46e9b8a6cb6a" target="_blank" rel="noopener">https://www.jianshu.com/p/46e9b8a6cb6a</a></li></ul><pre class=" language-shell"><code class="language-shell">cmake-gui #图像化cmakecmake --versionapt-get remove cmakecd /usr/local/srcwget https://github.com/Kitware/CMake/releases/download/v3.15.3/cmake-3.15.3.tar.gztar -xvzf cmake-3.15.3.tar.gzcd cmake-3.15.3./bootstrapmake -j4make install##python 使用C++11 框架 pylind11</code></pre><h3 id="gcc-cpp-g-区别"><a href="#gcc-cpp-g-区别" class="headerlink" title="gcc cpp g++ 区别"></a>gcc cpp g++ 区别</h3><pre class=" language-shell"><code class="language-shell">gcc和g++的主要区别# 1. 对于 *.c和*.cpp文件，gcc分别当做c和cpp文件编译（c和cpp的语法强度是不一样的）# 2. 对于 *.c和*.cpp文件，g++则统一当做cpp文件编译# 3. 使用g++编译文件时，g++会自动链接标准库STL，而gcc不会自动链接STL# 4. gcc在编译C文件时，可使用的预定义宏是比较少的# 5. gcc在编译cpp文件时/g++在编译c文件和cpp文件时（这时候gcc和g++调用的都是cpp文件的编译器），会加入一些额外的宏，这些宏如下：# 6. 在用gcc编译c++文件时，为了能够使用STL，需要加参数 –lstdc++ ，但这并不代表 gcc –lstdc++ 和 g++等价，它们的区别不仅仅是这个#gcc 版本gcc -versionsudo apt-get install gcc-5 g++-5sudo update-alternatives --install /usr/bin/gcc gcc/usr/bin/gcc-5 50  #change privilege</code></pre><h3 id="SCP文件互传"><a href="#SCP文件互传" class="headerlink" title="SCP文件互传"></a>SCP文件互传</h3><pre class=" language-shell"><code class="language-shell">scp ubuntu@140.143.210.30:/usr/local/apache-tomcat-9.0.22/webapps/temp.zip ~#scp 命令将服务器上文件拷贝至本地</code></pre><h3 id="ubuntu-VMWare-worstation-pro-15"><a href="#ubuntu-VMWare-worstation-pro-15" class="headerlink" title="ubuntu VMWare worstation pro 15"></a>ubuntu VMWare worstation pro 15</h3><pre class=" language-shell"><code class="language-shell">#下载地址 https://www.vmware.com/products/workstation-pro/workstation-pro-evaluation.html#VMware Workstation All Key：https://www.cnblogs.com/dunitian/p/8414055.htmlsudo ./VMWare-*sudo vmware-installer -u vmware-workstation  #卸载  </code></pre><h3 id="网速测量speedtest"><a href="#网速测量speedtest" class="headerlink" title="网速测量speedtest"></a>网速测量speedtest</h3><pre class=" language-shell"><code class="language-shell">git clone https://github.com/sivel/speedtest-cli.gitcd speedtest-clipython speedtest.py#具体可以看下readme操作，可以通过pip 方式安装</code></pre><h3 id="pytorch"><a href="#pytorch" class="headerlink" title="pytorch"></a>pytorch</h3><pre class=" language-shell"><code class="language-shell">#方式一：  #      通过官方网站（https://pytorch.org/）给的方法进行安装，根据自己的系统环境及相应python，CUDA版本运行相应的命令进行安装。如果电脑中只有python3，这里的pip3可以直接就用pip代替。conda install pytorch torchvision cudatoolkit=10.1 -c pytorch#遇问题 有关proxy#解决方案: 在 .bashrc 中添加: export all_proxy="socks5://127.0.0.1:1080"#cudatoolkit        pkgs/main/linux-64::cudatoolkit-10.1.243-h6bb024c_0#  ninja              pkgs/main/linux-64::ninja-1.9.0-py37hfd86e86_0 # pytorch            pytorch/linux-64::pytorch-1.3.1-py3.7_cuda10.1.243_cudnn7.6.3_0 # torchvision        pytorch/linux-64::torchvision-0.4.2-py37_cu101#方式二：   https://download.pytorch.org/whl/torch_stable.html#    直接下载torch的whl文件，通过pip install （路径+whl文件名）#    可以下载到本地 anaconda\install\Lib\site-packages路径下，或者在线下载安装</code></pre><h3 id="caffe-安装"><a href="#caffe-安装" class="headerlink" title="caffe 安装"></a>caffe 安装</h3><pre class=" language-shell"><code class="language-shell">sudo apt install caffe-cudasudo apt build-dep caffe-cuda       # dependencies for CUDA versionsudo vim /etc/apt/sources.list   #将deb-src 注释掉#遇到问题 dpkg-deb: error: paste subprocess was killed by signal (Broken pipe)#Errors were encountered while processing:# /var/cache/apt/archives/nvidia-cuda-dev_9.1.85-3ubuntu1_amd64.deb#sudo dpkg -i --force-overwrite /var/cache/apt/archives/nvidia-418_418.39-0ubuntu1_amd64.deb#sudo apt --fix-broken install</code></pre><h3 id="服务管理"><a href="#服务管理" class="headerlink" title="服务管理"></a>服务管理</h3><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> systemctl start application.service   <span class="token comment" spellcheck="true">#同 systemctl start application  ,系统默认查找application.service    stop, restart,reload</span><span class="token function">sudo</span> systemctl enable/disable application.service   <span class="token comment" spellcheck="true">#start a service at boot create a symbolic link from the system’s copy of the service file (usually in /lib/systemd/system or /etc/systemd/system) into the location on disk where systemd looks for autostart files (usually /etc/systemd/system/some_target.target.wants</span>systemctl status application.service  <span class="token comment" spellcheck="true">#查看服务状态</span>systemctl list-units  <span class="token comment" spellcheck="true"># list all of the units that systemd currently has active </span>systemctl list-dependencies application.service  <span class="token comment" spellcheck="true">#查找关系依赖树</span></code></pre><h3 id="搜狗输入法"><a href="#搜狗输入法" class="headerlink" title="搜狗输入法"></a>搜狗输入法</h3><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> remove ibus<span class="token function">sudo</span> <span class="token function">apt-get</span> purge ibus     <span class="token comment" spellcheck="true">#purge  </span><span class="token function">sudo</span>  <span class="token function">apt-get</span> remove indicator-keyboard<span class="token function">sudo</span> apt <span class="token function">install</span> fcitx-table-wbpy fcitx-config-gtkim-config -n fcitx选择系统设置语言 https://pinyin.sogou.com/linux/  <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> -ffcitx-config-gtk3fcitx设置 <span class="token operator">>></span>附加组件<span class="token operator">>></span>勾选高级 <span class="token operator">>></span>取消经典界面Configure<span class="token operator">>></span>  Addon  <span class="token operator">>></span>Advanced<span class="token operator">>></span>Classic,sogouyun<span class="token comment" spellcheck="true">#重启 把sogoupinyin放在第二个</span><span class="token comment" spellcheck="true">#只用sogou 输入法一种就行了</span><span class="token comment" spellcheck="true">#搜狗云输入的锅，在fcitx配置里把搜狗云拼音这个选项去掉就可以很完美的解决这问题了  解决占cpu</span><span class="token comment" spellcheck="true">#中文输入时没有汉字提示时下载一个 皮肤 ,用搜狗软件打开就行可</span><span class="token comment" spellcheck="true">#https://pinyin.sogou.com/skins/detail/view/info/588600?rf=cate_31_sign&amp;tf=p</span></code></pre><h3 id="多线程下载软件源"><a href="#多线程下载软件源" class="headerlink" title="多线程下载软件源:"></a>多线程下载软件源:</h3><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> add-apt-repository ppa:apt-fast/stable<span class="token function">sudo</span> <span class="token function">apt-get</span> update</code></pre><h3 id="JDK"><a href="#JDK" class="headerlink" title="JDK"></a>JDK</h3><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> apt <span class="token function">install</span> openjdk-11-jdk</code></pre><h3 id="VSCODE"><a href="#VSCODE" class="headerlink" title="VSCODE"></a>VSCODE</h3><ul><li><p>格式化代码</p><pre><code>vs code格式化代码的快捷键如下：（来源于这里）On Windows Shift + Alt + F.On Mac Shift + Option + F.On Ubuntu Ctrl + Shift + I.</code></pre></li><li><p>常用插件</p><ul><li>Beautify</li><li>TODO Highlight</li><li>Code Spell Checker</li><li>IntelliSense for CSS class names in HTML</li></ul></li><li><p>删除多余空行  全局替换  ^\s*(?=\r?$)\n     Alt+R 正则表达式</p></li></ul><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> ubuntu-make  <span class="token comment" spellcheck="true"># 像这种开发软件去官网下载安装包</span><span class="token comment" spellcheck="true">#查看版本</span>code --versioncode <span class="token comment" spellcheck="true">#运行vscode</span><span class="token comment" spellcheck="true">#Ctrl+Shift+P打开命令面板</span><span class="token comment" spellcheck="true">#c_cpp_properties.json  该文件用于指定一般的编译环境，包括头文件路径，编译器的路径等。通过 Ctrl + Shift + p 打开命令行，键入关键字 "C++"，在下拉菜单中选择 "C/C++ Edit configuration"，系统即自动在 .vscode 目录下创建 c_cpp_properties.json 文件，供用户进行编译方面的环境配置。</span><span class="token punctuation">{</span>    <span class="token string">"configurations"</span><span class="token keyword">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">{</span>            <span class="token string">"name"</span><span class="token keyword">:</span> <span class="token string">"Linux"</span>,            <span class="token string">"includePath"</span><span class="token keyword">:</span> <span class="token punctuation">[</span>                <span class="token string">"<span class="token variable">${workspaceFolder}</span>/**"</span>            <span class="token punctuation">]</span>,            <span class="token string">"defines"</span><span class="token keyword">:</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>,            <span class="token string">"compilerPath"</span><span class="token keyword">:</span> <span class="token string">"/usr/bin/gcc"</span>,            <span class="token string">"cStandard"</span><span class="token keyword">:</span> <span class="token string">"c11"</span>,            <span class="token string">"cppStandard"</span><span class="token keyword">:</span> <span class="token string">"c++17"</span>,            <span class="token string">"intelliSenseMode"</span><span class="token keyword">:</span> <span class="token string">"clang-x64"</span>        <span class="token punctuation">}</span>    <span class="token punctuation">]</span>,    <span class="token string">"version"</span><span class="token keyword">:</span> 4<span class="token punctuation">}</span><span class="token comment" spellcheck="true">#build.json  该文件用于指定程序的编译规则，即如何将源文件编译为可执行程序。通过 Ctrl + Shift + p 打开命令行，键入关键字 "task"，并在下拉菜单中选择 Tasks: Configure Default Build Task -> Create tassk.json file from template -> Others ，系统即自动在 .vscode 目录下创建 build.json 文件，供用户设置具体的编译规则</span><span class="token punctuation">{</span>    // See https://go.microsoft.com/fwlink/?LinkId<span class="token operator">=</span>733558    // <span class="token keyword">for</span> the documentation about the tasks.json <span class="token function">format</span>    <span class="token string">"version"</span><span class="token keyword">:</span> <span class="token string">"2.0.0"</span>,    <span class="token string">"tasks"</span><span class="token keyword">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">{</span>            <span class="token string">"label"</span><span class="token keyword">:</span> <span class="token string">"echo"</span>,            <span class="token string">"type"</span><span class="token keyword">:</span> <span class="token string">"shell"</span>,            <span class="token string">"command"</span><span class="token keyword">:</span> <span class="token string">"g++"</span>,                   //编译时执行的程序            <span class="token string">"args"</span><span class="token keyword">:</span> <span class="token punctuation">[</span><span class="token string">"-g"</span>, <span class="token string">"-o"</span>, <span class="token string">"test"</span>, <span class="token string">"test1.c"</span><span class="token punctuation">]</span>,    //传递给 <span class="token function">command</span> 的参数            <span class="token string">"problemMatcher"</span><span class="token keyword">:</span> <span class="token punctuation">[</span>                <span class="token string">"<span class="token variable">$gcc</span>"</span>            <span class="token punctuation">]</span>        <span class="token punctuation">}</span>    <span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true"># Ctrl+Shift+p 打开命令行，选择 Tasks:Run Build Task 运行上述编译过程</span><span class="token comment" spellcheck="true">#launch.json  该文件主要与程序的调试相关。用户可通过 Ctrl+Shift+p 打开命令行，键入关键字 "launch",选择 "Debug:Open launch.json" -> "C++(GDB/LLDB)"，即可打开调试的配置文件 launch.json。在 VSCode 中，用户按 F5 即可进入调试模式，上述 launch.json 文件即设置在调试时的基本内容和要求。</span></code></pre><h3 id="indicator-sysmonitor"><a href="#indicator-sysmonitor" class="headerlink" title="indicator-sysmonitor"></a>indicator-sysmonitor</h3><p>一款可以监视 CPU 占用率、 CPU 温度、内存占用率、网速等系统信息的小软件，在桌面最上方进行显示。Top 的图形化命令</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># sudo add-apt-repository ppa:fossfreedom/indicator-sysmonitor  </span><span class="token function">sudo</span> <span class="token function">apt-get</span> update<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> indicator-sysmonitor</code></pre><h3 id="GDebi"><a href="#GDebi" class="headerlink" title="GDebi"></a>GDebi</h3><pre class=" language-shell"><code class="language-shell">#若用 Ubuntu 自带的软件中心安装 deb 格式的文件不仅经常会崩溃而且会遇到各种各样的依赖问题。通过deb文件安装软件优选sudo apt-get install gdebi</code></pre><h3 id="Marp"><a href="#Marp" class="headerlink" title="Marp"></a>Marp</h3><p>用 Markdown 语法来制作 PPT，高效快速简洁实用，尤其是支持 LaTeX 语法，非常方便编辑大量的数学公式，值得推荐，官网有 deb 文件，下载后直接安装即可。</p><h3 id="新立得软件管理"><a href="#新立得软件管理" class="headerlink" title="新立得软件管理"></a>新立得软件管理</h3><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> synaptic  <span class="token comment" spellcheck="true">#  全面高效地管理各种软件和依赖。</span></code></pre><h3 id="Tim"><a href="#Tim" class="headerlink" title="Tim"></a>Tim</h3><blockquote><p>Tim 安装   去官网 下载linux QQ  但qq上没有我的设备<br><a href="https://im.qq.com/linuxqq/download.html" target="_blank" rel="noopener">https://im.qq.com/linuxqq/download.html</a><br><a href="https://github.com/wszqkzqk/deepin-wine-ubuntu/releases" target="_blank" rel="noopener">https://github.com/wszqkzqk/deepin-wine-ubuntu/releases</a>   #wine的一个版本<br><a href="https://www.lulinux.com/archives/1319" target="_blank" rel="noopener">https://www.lulinux.com/archives/1319</a>  #deepin-wine Tim安装教程<br><strong>Winehq</strong>:<a href="https://wiki.winehq.org/Ubuntu_zhcn" target="_blank" rel="noopener">https://wiki.winehq.org/Ubuntu_zhcn</a>   学习如何使用  回去学习下winehq使用教程<a href="https://wiki.winehq.org/Wine_User' target=" _blank"="" rel="noopener" s_guide"="">https://wiki.winehq.org/Wine_User%27s_Guide</a><br>Usage: wine PROGRAM [ARGUMENTS…]   Run the specified program<br>       wine –help                   Display this help and exit<br>       wine –version                Output version information and exit<br>运行方式1:cd ‘.wine/drive_c/Games/Tron’<br>         wine tron.exe<br>运行方式2:wine start ‘C:\Games\Tron\tron.exe’<br>        wine start “C:\Games\Tron\tron.exe”<br>        wine start /unix “$HOME/installers/TronSetup.exe”<br>        wine quake.exe -map e1m1   #带参数<br>        wine start whatever.msi<br>         wine control<br>         wine uninstaller</p></blockquote><h3 id="mega网盘安装"><a href="#mega网盘安装" class="headerlink" title="mega网盘安装"></a>mega网盘安装</h3><blockquote><p><a href="https://mega.nz/sync" target="_blank" rel="noopener">https://mega.nz/sync</a>   去官网安装 需要联网</p></blockquote><h3 id="V2Ray-安装"><a href="#V2Ray-安装" class="headerlink" title="V2Ray 安装"></a>V2Ray 安装</h3><pre class=" language-shell"><code class="language-shell">#然后编辑`/etc/v2ray/config.json`文件service v2ray stop service v2ray start service v2ray status#https://github.com/FelisCatus/SwitchyOmega/wiki/GFWList</code></pre><ul><li><h3 id="v2ray-go-sh脚本阅读记录"><a href="#v2ray-go-sh脚本阅读记录" class="headerlink" title="#v2ray/go.sh脚本阅读记录"></a>#v2ray/go.sh脚本阅读记录</h3></li></ul><pre class=" language-bash"><code class="language-bash">$<span class="token comment" spellcheck="true"># 表示执行脚本传入参数的个数</span><span class="token variable">$*</span>  表示执行脚本传入参数列表$$ 表示进程id<span class="token variable">$@</span>表示执行脚本传入所有参数<span class="token variable">$0</span> 表示执行脚本名称<span class="token variable">$1</span> 表示第一个参数<span class="token variable">$2</span> 表示第二个参数<span class="token variable">$?</span> 表示脚本执行状态0正常，其他表示有错误<span class="token comment" spellcheck="true">#提取文件到某个位置函数  </span><span class="token comment" spellcheck="true">#获取系统本版/检查版本更新   getVersion()/checkUpdate()</span><span class="token comment" spellcheck="true">#检查系统架构  SysArch()</span><span class="token comment" spellcheck="true">#获得系统 install update 指令   'command -v apt-get' 判断系统是否有apt-get 指令</span><span class="token comment" spellcheck="true">#prompt 颜色设置  colorEcho()</span><span class="token comment" spellcheck="true">#下载文件   downloadv2ray()</span><span class="token comment" spellcheck="true"># echo $VER | head -n 1 | cut -d " " -f2`  </span><span class="token comment" spellcheck="true">#关闭或启动软件  stopV2ray() startV2ray() 通过检查systemctl/service 命令</span><span class="token comment" spellcheck="true">#copy 文件  copyFile()</span><span class="token comment" spellcheck="true">#添加执行权限 makeExecutable()</span><span class="token comment" spellcheck="true"># help() 帮助提示框</span>installInitScript<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{</span>    <span class="token keyword">if</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> -n <span class="token string">"<span class="token variable">${SYSTEMCTL_CMD}</span>"</span> <span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">;</span><span class="token keyword">then</span>        <span class="token keyword">if</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token operator">!</span> -f <span class="token string">"/etc/systemd/system/v2ray.service"</span> <span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>            <span class="token keyword">if</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token operator">!</span> -f <span class="token string">"/lib/systemd/system/v2ray.service"</span> <span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>                <span class="token function">cp</span> <span class="token string">"<span class="token variable">${VSRC_ROOT}</span>/systemd/v2ray.service"</span> <span class="token string">"/etc/systemd/system/"</span>                systemctl <span class="token function">enable</span> v2ray.service            <span class="token keyword">fi</span>        <span class="token keyword">fi</span>        <span class="token keyword">return</span>    <span class="token keyword">elif</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> -n <span class="token string">"<span class="token variable">${SERVICE_CMD}</span>"</span> <span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">&amp;&amp;</span> <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token operator">!</span> -f <span class="token string">"/etc/init.d/v2ray"</span> <span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">;</span> <span class="token keyword">then</span>        installSoftware <span class="token string">"daemon"</span> <span class="token operator">||</span> <span class="token keyword">return</span> <span class="token variable">$?</span>        <span class="token function">cp</span> <span class="token string">"<span class="token variable">${VSRC_ROOT}</span>/systemv/v2ray"</span> <span class="token string">"/etc/init.d/v2ray"</span>        <span class="token function">chmod</span> +x <span class="token string">"/etc/init.d/v2ray"</span>        update-rc.d v2ray defaults    <span class="token keyword">fi</span>    <span class="token keyword">return</span><span class="token punctuation">}</span><span class="token function">sed</span> -i <span class="token string">"s/10086/<span class="token variable">${PORT}</span>/g"</span> <span class="token string">"/etc/v2ray/config.json"</span>  <span class="token comment" spellcheck="true">#学习这个指令</span>downloadV2Ray <span class="token operator">||</span> <span class="token keyword">return</span> <span class="token variable">$?</span>installV2Ray<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#包括下载到那个目录,copy了那些文件,如何根据配置文件进行配置的</span>remove<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span># 卸载停止服务,把安转时写入的文件全部删除      /etc/systemd/system/v2ray.service       /usr/bin/v2ray    /lib/systemd/system/v2ray.service    /etc/init.d/v2ray<span class="token punctuation">}</span></code></pre><h3 id="修改github-DNS"><a href="#修改github-DNS" class="headerlink" title="修改github DNS"></a>修改github DNS</h3><pre class=" language-shell"><code class="language-shell">#https://www.linuxidc.com/Linux/2019-05/158461.htm#github219.76.4.4 github-cloud.s3.amazonaws.com192.30.253.112 github.com151.101.185.194 github.global.ssl.fastly.netldd@ldd:~/v2ray$ sudo vim /etc/hostsldd@ldd:~/v2ray$ sudo /etc/init.d/networking restart </code></pre><h3 id="WPS-去官网下载"><a href="#WPS-去官网下载" class="headerlink" title="WPS 去官网下载"></a>WPS 去官网下载</h3><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#http://www.wps.cn/product/wpslinux  </span><span class="token function">sudo</span> dpkg -i wps-office_10.1.0.6757_amd64.deb</code></pre><h3 id="IDEA下载"><a href="#IDEA下载" class="headerlink" title="IDEA下载"></a><a href="https://www.jetbrains.com/idea/download/#section=linux" target="_blank" rel="noopener">IDEA下载</a></h3><h3 id="Teamview-deb-安装"><a href="#Teamview-deb-安装" class="headerlink" title="Teamview   deb 安装"></a>Teamview   deb 安装</h3><h3 id="proxyee-down命令行安装-百度云下载神器"><a href="#proxyee-down命令行安装-百度云下载神器" class="headerlink" title="proxyee-down命令行安装  百度云下载神器"></a>proxyee-down命令行安装  百度云下载神器</h3><h3 id="docky-桌面工具"><a href="#docky-桌面工具" class="headerlink" title="docky 桌面工具"></a>docky 桌面工具</h3><pre class=" language-bash"><code class="language-bash"><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span>  docky   <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> gnome-tweak-tool <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> gnome-shell-extensions <span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> gnome-shell-extension-dashtodock<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> gnome-shell-extension-autohidetopbar<span class="token comment" spellcheck="true">#也可以在Ubuntu软件中直接搜索hide top bar</span><span class="token function">sudo</span> <span class="token function">apt-get</span> remove gnome-shell-extension-autohidetopbar <span class="token comment" spellcheck="true">#卸载</span><span class="token comment" spellcheck="true">#快捷键设置</span>gnome-screenshot -ac  <span class="token comment" spellcheck="true"># 也具有qq截图到快捷键功能</span><span class="token comment" spellcheck="true">#在打开——系统设置——>键盘——快捷键——自定义快捷键，然后输入名字和上边工具的命令</span></code></pre><h3 id="Opencv"><a href="#Opencv" class="headerlink" title="Opencv"></a>Opencv</h3><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">#python 包</span>pip uninstall opencv-pythonpip <span class="token function">install</span> opencv-contrib-python <span class="token comment" spellcheck="true">#opencv4. 源码编译安装， 也可以直接编译Android 依赖库</span><span class="token comment" spellcheck="true">#https://www.pluvet.com/archives/223.html 安装教程</span><span class="token function">sudo</span> add-apt-repository “deb http://security.ubuntu.com/ubuntu xenial-security main”<span class="token function">sudo</span> apt update<span class="token function">sudo</span> apt <span class="token function">install</span> libjasper1 libjasper-dev  <span class="token function">sudo</span> apt-fast <span class="token function">install</span> build-essential cmake libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libdc1394-22-devcmake <span class="token punctuation">..</span><span class="token function">make</span> -j4<span class="token function">sudo</span> <span class="token function">make</span> <span class="token function">install</span></code></pre><h3 id="Python-命令转换"><a href="#Python-命令转换" class="headerlink" title="Python 命令转换"></a>Python 命令转换</h3><p>pip 切换镜像  最终写入文件 /home/ldd/.config/pip/pip.conf</p><pre class=" language-shell"><code class="language-shell">pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</code></pre><p>方式一：系统默认一个版本，在另装一个版本，通过软连接</p><pre class=" language-bash"><code class="language-bash"><span class="token comment" spellcheck="true"># 以后使用anaconda</span><span class="token comment" spellcheck="true">#查看当前默认Python版本</span>python --version <span class="token comment" spellcheck="true">#查看Python所在</span><span class="token function">which</span> is python<span class="token function">which</span> is python3<span class="token comment" spellcheck="true">#Python下载的库可以查看这里。/usr/local/lib/</span><span class="token comment" spellcheck="true">#显示Python代替版本信息</span>update-alternatives --list python<span class="token comment" spellcheck="true">#设置 /usr/bin/python3.5 设置的优先级为2 优先级越高越大</span>update-alternatives --install /usr/bin/python python /usr/bin/python2.7 1update-alternatives --install /usr/bin/python python /usr/bin/python3.5 2<span class="token comment" spellcheck="true">#再次显示Python代替版本信息</span>update-alternatives --remove python /usr/bin/python2.7<span class="token comment" spellcheck="true">#切换版本</span><span class="token function">sudo</span> update-alternatives --config python<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> python3-pip <span class="token comment" spellcheck="true"># #安装Python3对应的pip3</span><span class="token function">sudo</span> pip3 <span class="token function">install</span> --upgrade pip   <span class="token comment" spellcheck="true">#推荐在管理员模式下更新</span><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> python-pip  <span class="token comment" spellcheck="true">#安装Python2对应的pip</span><span class="token comment" spellcheck="true">#Pip  安装的库会放在这个目录下面：python2.7/site-packages；</span><span class="token comment" spellcheck="true">#pip3 新安装的库会放在这个目录下面：python3.6/site-packages；</span><span class="token comment" spellcheck="true">#参考https://www.cnblogs.com/carle-09/p/9907274.html</span><span class="token comment" spellcheck="true">#errorPermission denied: '/usr/local/lib/python3.6/dist-packages/cycler.py' Consider using the `--user` option or check the permissions.</span>pip3 <span class="token function">install</span> --user matplotlib  <span class="token comment" spellcheck="true">#The 'pip==9.0.3' distribution was not found and is required by the application</span><span class="token function">sudo</span> easy_install pip<span class="token operator">==</span>9.0.3  <span class="token comment" spellcheck="true">#解决</span></code></pre><p>方式二：安装anaconda，然后建立基于不同python版本的conda环境</p><p>方式三：建立虚拟机virtualenv，然后建立基于不同python版本的虚拟环境</p><h3 id="MySQL-安装"><a href="#MySQL-安装" class="headerlink" title="MySQL 安装"></a>MySQL 安装</h3><pre class=" language-shell"><code class="language-shell">sudo apt-get install mysql-serversudo mysql_secure_installation  #设置密码 liudongdongsudo mysql   #可以直接登录sudo systemctl start mysql</code></pre><h3 id="Net-core-安装"><a href="#Net-core-安装" class="headerlink" title=".Net core 安装"></a>.<a href="https://dotnet.microsoft.com/download/linux-package-manager/ubuntu18-04/sdk-current" target="_blank" rel="noopener">Net core 安装</a></h3><h3 id="mssql-server安装"><a href="#mssql-server安装" class="headerlink" title="mssql-server安装"></a><a href="https://docs.microsoft.com/zh-cn/sql/linux/quickstart-install-connect-ubuntu?view=sql-server-ver15#connect-locally" target="_blank" rel="noopener">mssql-server安装</a></h3><pre class=" language-shell"><code class="language-shell">sudo apt-fast install libodbc1 unixodbc msodbcsql mssql-tools unixodbc-dev</code></pre><h3 id="NVIDIA显卡驱动-cuda"><a href="#NVIDIA显卡驱动-cuda" class="headerlink" title="NVIDIA显卡驱动 cuda"></a>NVIDIA显卡驱动 cuda</h3><pre class=" language-bash"><code class="language-bash"> <span class="token comment" spellcheck="true">#驱动安装</span> <span class="token function">sudo</span> ubuntu-drivers devices  查看系统支持的显卡设备并下载<span class="token comment" spellcheck="true">#**系统设置** > **细节**窗口，你会发现Ubuntu正在使用Nvidia显卡。</span>lspci -k <span class="token operator">|</span> <span class="token function">grep</span> -A 2 -i <span class="token string">"VGA"</span>software-properties-gtknvidia-settings       <span class="token comment" spellcheck="true">#打开nvidia 设置软件页面</span>ubuntu-drivers devices    <span class="token comment" spellcheck="true">#推荐显卡和驱动</span><span class="token function">sudo</span> ubuntu-drivers autoinstall  <span class="token comment" spellcheck="true">#显示推荐的驱动</span><span class="token function">sudo</span> <span class="token function">apt-get</span> updateapt search nvidia-driver-418lshw -C video   <span class="token comment" spellcheck="true">#查看设备       </span>lspci <span class="token operator">|</span> <span class="token function">grep</span> -i nvidia  <span class="token comment" spellcheck="true">#verify you have a cuda-Capble GPU</span><span class="token comment" spellcheck="true">#查看当前NVIDIA驱动版本</span><span class="token function">sudo</span> dpkg --list <span class="token operator">|</span> <span class="token function">grep</span> nvidia-*<span class="token comment" spellcheck="true">#查看本机GPU</span><span class="token function">uname</span> -r <span class="token comment" spellcheck="true">#current running kernel</span><span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> linux-headers-<span class="token variable"><span class="token variable">$(</span><span class="token function">uname</span> -r<span class="token variable">)</span></span> <span class="token comment" spellcheck="true"># the kernel headers and development packages</span>gcc --version      <span class="token comment" spellcheck="true">#是否安装gcc</span><span class="token comment" spellcheck="true">#disable Nouveau  如果不适用nvida 驱动时需要改回</span>lsmod <span class="token operator">|</span> <span class="token function">grep</span> nouveau   <span class="token comment" spellcheck="true">#如果有输出则需要关闭</span><span class="token comment" spellcheck="true">#创建文件  vim /etc/modprobe.d/blacklist-nouveau.conf</span>blacklist nouveauoptions nouveau modeset<span class="token operator">=</span>0<span class="token comment" spellcheck="true">#then regenerate the kernel</span><span class="token function">sudo</span> update-initramfs -u<span class="token comment" spellcheck="true">#cuda 有俩中安装方式</span>    <span class="token comment" spellcheck="true"># 1: distribution-specific packages(RPM,Deb packages) recommended</span>    <span class="token comment" spellcheck="true"># 2: distribute-independent package(runfile package)  working across a wider set of linux distribution ,but doesn't update the native package management system</span><span class="token comment" spellcheck="true">#download the nvidia toolkit</span><span class="token comment" spellcheck="true"># http://develop.nvidia.com/cuda-downloads  包含 cuda 驱动和一些工具包括库,应用程序,示例程序等</span><span class="token comment" spellcheck="true">#校验下载是否正确</span>md5sum filename<span class="token comment" spellcheck="true">#具体安装下载时有说明</span><span class="token comment" spellcheck="true">#下载其他版本冲突情况看下表:</span><span class="token comment" spellcheck="true">#卸载分俩种情况</span><span class="token comment" spellcheck="true"># 1: 卸载通过 runfile  下载</span><span class="token function">sudo</span> /usr/local/cuda-x.y/bin/uninstall_cuda_x.y.pl<span class="token comment" spellcheck="true">#     卸载通过 runfile 下载的驱动</span><span class="token function">sudo</span> /usr/bin/nvidia-uninstall<span class="token comment" spellcheck="true"># 2: 卸载通过deb/RPM 包下载的软件</span><span class="token function">sudo</span> <span class="token function">apt-get</span> --purge remove <span class="token operator">&lt;</span>package_name<span class="token operator">></span> <span class="token comment" spellcheck="true"># Ubuntu</span><span class="token comment" spellcheck="true">#或者To remove CUDA Toolkit:</span>$ <span class="token function">sudo</span> <span class="token function">apt-get</span> --purge remove <span class="token string">"*cublas*"</span> <span class="token string">"cuda*"</span><span class="token comment" spellcheck="true">#To remove NVIDIA Drivers:</span>$ <span class="token function">sudo</span> <span class="token function">apt-get</span> --purge remove <span class="token string">"*nvidia*"</span><span class="token comment" spellcheck="true">#download the cuda toolkit packages https://developer.nvidia.com/cuda-downloads?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1804&amp;target_type=deblocal</span><span class="token comment" spellcheck="true">#安装cuda10.1</span><span class="token function">sudo</span> dpkg -i cuda-repo-ubuntu1804-10-1-local-10.1.168-418.67_1.0-1_amd64.deb<span class="token function">sudo</span> apt-key add /var/cuda-repo-<span class="token operator">&lt;</span>version<span class="token operator">></span>/7fa2af80.pub <span class="token comment" spellcheck="true">#本地文件，里面是一些NVidia deb安装包</span><span class="token function">sudo</span> <span class="token function">apt-get</span> update<span class="token function">sudo</span> <span class="token function">apt-get</span> <span class="token function">install</span> cudanvidia-smi    <span class="token comment" spellcheck="true">#查看NVIDIA 相关信息 ，这里是选择NVIDIA驱动才会显示</span><span class="token comment" spellcheck="true">#安装cuda toolkit  这和cuda 驱动没有关系</span><span class="token comment" spellcheck="true">#cudnn 在/usr/local/目录下  ； cuda_toolkit 在/usr/local/cuda-版本号</span><span class="token function">wget</span> https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin<span class="token function">sudo</span> <span class="token function">mv</span> cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600<span class="token function">wget</span> http://developer.download.nvidia.com/compute/cuda/10.2/Prod/local_installers/cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb<span class="token function">sudo</span> dpkg -i cuda-repo-ubuntu1804-10-2-local-10.2.89-440.33.01_1.0-1_amd64.deb<span class="token function">sudo</span> apt-key add /var/cuda-repo-10-2-local-10.2.89-440.33.01/7fa2af80.pub<span class="token function">sudo</span> <span class="token function">apt-get</span> update<span class="token function">sudo</span> <span class="token function">apt-get</span> -y <span class="token function">install</span> cuda       <span class="token comment" spellcheck="true">#下载更新cuda 和driven</span><span class="token comment" spellcheck="true">#这个步操作后，会把之前的的驱动改为418 ，这里显示驱动不匹配，把驱动删除，然后重新下载驱动，问题解决， 但是后期会不会存在问题不清楚  ，cuda10.1 对内核的要求不清楚，使用5.0.0-35 内核的，但好像不推荐</span><span class="token comment" spellcheck="true">#驱动匹配问题解决方法2:</span><span class="token function">ls</span> mod <span class="token operator">|</span> <span class="token function">grep</span> nvidia<span class="token function">sudo</span> rmmod nvidia_uvm<span class="token function">sudo</span> rmmod nvidia_modeset<span class="token function">sudo</span> rmmod nvidia<span class="token function">sudo</span> apt <span class="token function">install</span> cuda-drivers<span class="token comment" spellcheck="true">#Reboot the system to load the NVIDIA drivers.</span><span class="token comment" spellcheck="true">#Set up the development environment by modifying the PATH and LD_LIBRARY_PATH variables:</span><span class="token comment" spellcheck="true">#cuda 环境变量</span>$ <span class="token function">export</span> PATH<span class="token operator">=</span>/usr/local/cuda-10.2/bin<span class="token variable">${PATH:+:${PATH}</span><span class="token punctuation">}</span>$ <span class="token function">export</span> LD_LIBRARY_PATH<span class="token operator">=</span>/usr/local/cuda-10.2/lib64\                         <span class="token variable">${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">#Install a writable copy of the samples then build and run the nbody sample: 每一个toolkit 都有一个sample可以测试是够安装好</span>$ cuda-install-samples-10.2.sh ~$ <span class="token function">cd</span> ~/NVIDIA_CUDA-10.2_Samples/5_Simulations/nbody$ <span class="token function">make</span>$ ./nbody<span class="token comment" spellcheck="true">#运行效果如下图所示</span></code></pre><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191212213557977.png" alt="image-20191212213557977"></p><pre class=" language-shell"><code class="language-shell">#ubuntu cudnn 安装教程 https://developer.nvidia.com/rdp/cudnn-download  cudnn其实是一些加速CUDA性能的库，首先按照解压放到CUDA的相应路径中然后把其中的lib64关联到环境变量当中#将三个deb文件都下载下同时安装，否则会报错sudo dpkg -i libcudnn7*.deb</code></pre><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191206194750114.png" alt="image-20191206194750114"></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191212224333748.png" alt="image-20191212224333748"></p><p>网上一个脚本</p><pre class=" language-shell"><code class="language-shell"># WARNING: These steps seem to not work anymore!#!/bin/bash# Purge existign CUDA firstsudo apt --purge remove "cublas*" "cuda*"sudo apt --purge remove "nvidia*"# Install CUDA Toolkit 10wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.debsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub && sudo apt updatesudo dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.debsudo apt updatesudo apt install -y cuda# Install CuDNN 7 and NCCL 2wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.debsudo dpkg -i nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.debsudo apt updatesudo apt install -y libcudnn7 libcudnn7-dev libnccl2 libc-ares-devsudo apt autoremovesudo apt upgrade# Link libraries to standard locationssudo mkdir -p /usr/local/cuda-10.0/nccl/libsudo ln -s /usr/lib/x86_64-linux-gnu/libnccl.so.2 /usr/local/cuda/nccl/lib/sudo ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7 /usr/local/cuda-10.0/lib64/echo 'If everything worked fine, reboot now.'</code></pre><ul><li>window上查看cuda版本</li></ul><pre><code>nvcc --version   #使用命令#进入相应的目录  C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA#通过Nviadia 软件查看，这里俩个版本不一致，不清楚有没有问题？</code></pre><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20200203095344007.png" alt="image-20200203095344007"></p><ul><li><h2 id="相关故障记录"><a href="#相关故障记录" class="headerlink" title="相关故障记录"></a>相关故障记录</h2></li></ul><h3 id="内核相关的"><a href="#内核相关的" class="headerlink" title="内核相关的"></a>内核相关的</h3><ol><li><p>ctrl+alt+F1–F6  切换到相应的终端</p></li><li><p><strong>file ‘which update-initramfs’</strong>  学会这个命令</p><ul><li>编译内核的最后一步执行make install时会调用update-initramfs，update-initramfs继而调用mkinitramfs生成initrd.img.  一个往临时initrd目录copy文件的繁琐过程，mkinitramfs则用脚本替代了手工操作</li><li>1).在临时initrd目录下构建FHS规定的文件系统;2).按/etc/initramfs-tools/module和/etc/modules文件的配置，往lib/modules/目录拷贝模块，同时生成模块依赖文件modules.dep，以后内核启动后会从initramfs中(initrd.img被解压到内存中)按模块依赖关系modprobe模块;3).拷贝/etc/initramfs-tools/scripts和/usr/share/initramfs-tools/scripts下的配置文件到conf/目录下,以后内核启动，创建第一个进程init(initrd.img根目录下init.sh文件)会从conf/*读取配置，按一定的顺序加载模块/执行程序;4).模块的加载离不开modprobe工具集，因此需要拷贝modprobe工具集及其他工具到initrd目录结构下，同时解决这些工具的依赖关系(依赖的so文件的路径);5).所有步骤完成，调用cpio和gzip工具打包压缩临时initrd目录结构。</li></ul></li><li><p><strong>nouveau</strong>(英语：<a href="https://baike.baidu.com/item/%2F" target="_blank" rel="noopener">/</a><a href="https://baike.baidu.com/item/n" target="_blank" rel="noopener">n</a>uːˈ<a href="https://baike.baidu.com/item/v" target="_blank" rel="noopener">v</a>oʊ<a href="https://baike.baidu.com/item/%2F" target="_blank" rel="noopener">/</a>) 是一个自由开放源代码CPU驱动程序，是为AMD的<a href="https://baike.baidu.com/item/CPU" target="_blank" rel="noopener">CPU</a>所编写，也可用于属于<a href="https://baike.baidu.com/item/系统芯片" target="_blank" rel="noopener">系统芯片</a>的<a href="https://baike.baidu.com/item/高通" target="_blank" rel="noopener">高通</a>系列.</p><p>Nouveau的内核模块应该在系统启动时就已自动加载，如果没有的话：</p><ul><li>确保你的<a href="https://wiki.archlinux.org/index.php/Kernel_parameters" target="_blank" rel="noopener">内核参数</a>中没有<code>nomodeset</code> 或者 <code>vga=</code>， 因为Nouveau需要内核模式设置。</li><li>另外，确保你没有在 modprobe 配置文件 <code>/etc/modprobe.d/</code> 或 <code>/usr/lib/modprobe.d/</code> 中屏蔽 Nouveau。</li><li>检查 dmesg 中有没有 opcode 错误，如果有的话，将 <code>nouveau.config=NvBios=PRAMIN</code> 加入 <a href="https://wiki.archlinux.org/index.php/Kernel_parameters" target="_blank" rel="noopener">内核参数</a>禁止模块卸载</li><li>Nouveau 驱动依赖<a href="https://wiki.archlinux.org/index.php/Kernel_mode_setting" target="_blank" rel="noopener">Kernel mode setting</a> (KMS)。当系统启动时，KMS 模块会在其它模块之后启用，所以显示的分辨率发生改变。</li></ul></li><li><p><strong>dmesg</strong> 命令:用来显示开机信息, kernel会将开机信息存储在ring buffer中。开机时来不及查看信息，可利用dmesg来查看。开机信息亦保存在/var/log/dmesg</p><p>1) dmesg 是一个显示内核缓冲区系统控制信息的工具;比如系统在启动时的信息会写到/var/log/</p><p>2) dmesg 命令显示Linux内核的环形缓冲区信息，我们可以从中获得诸如系统架构、CPU、挂载的硬件，RAM等多个运行级别的大量的系统信息。当计算机启动时，系统内核（操作系统的核心部分）将会被加载到内存中。在加载的过程中会显示很多的信息，在这些信息中我们可以看到内核检测硬件设备</p><p>3) dmesg 命令设备故障的诊断是非常重要的。在dmesg命令的帮助下进行硬件的连接或断开连接操作时，我们可以看到硬件的检测或者断开连接的信息</p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213212239267.png" alt="image-20191213212239267"></p></li><li><p>watch :  execute a program periodically, showing output fullscreen  <font color="red">watch “dmesg | tail -20” </font></p></li><li><p><strong>rmmod</strong>: 可删除不需要的模块。Linux操作系统的核心具有模块化的特性，因此在编译核心时，不需要把全部的功能都放入核心。</p></li><li><p><strong>lsmod</strong>: 显示内核中的模块作用同 <strong>cat /proc/devices</strong> </p></li><li><p><strong>modinfo</strong> 能查看模块的信息，通过查看模块信息来判定这个模块的用途；</p></li><li><p><strong>insmod</strong>: 向linux 内核中加载摸块  </p></li><li><p><strong>modprobe</strong> :向Linux内核中加载摸块,能够处理 module 载入的相依问题.  <font color="red">modprobe会检查/lib/modules/<code>uname -r</code>下的所有模块，除了/etc/modprobe.conf配置文件和/etc/modprobe.d目录以外。所有/etc/modprobe.d/arch/目录下的文件将被忽略。</font></p></li><li><p><font color="red">unable to correct problems,you have held broken package</font>    </p><pre class=" language-shell"><code class="language-shell">sudo apt install -fsudo aptitude install <packagename>  #get the detail informationsudo apt update  | sudo apt upgradesudo dpkg --configure -asudo dpkg --get-selection | grep hold #get actual held packagesdpkg --get-selections | grep linux-image  #产看内核文件有哪些</code></pre></li></ol><ol start="12"><li><p>Grub2介绍</p><ul><li>/boot/grub/grub.cfg 文件<ul><li>官方文件只说/boot/grub/grub.cfg不要手工修改，这个文件是运行 update-grub自动生成的。要修改配置文件的只要打开/boot/grub/grub.cfg文件，找到想修改的地方，然后根据注释找到相应的 /etc/default/grub或/etc/grub.d/ (folder)进行修改。</li><li>grub.cfg文件中主要包含两个部分，一部分是 各个启动项的定义，第二部分是启动界面的设置。你可以直接用gedit打开该文件看其中的内容。</li></ul></li><li>/etc/grub.d/ 文件夹<ul><li>定义各个启动项，其中的文件代表了一个或多个启动项，命名规范都是”两个数字<em>名称”，前面的两位数字确定这个或这多个启动项在启动界面的位置， 默认的 “00</em>“是预留给”00_header”的，”10_是预留给当前系统内核的，20_是预留给第三方程序的，除了这些你都可以使用，增加自己的，比如 05_ , 15_，数字越小越前面。</li><li>执行前面说的”update-grub”或者update- grub2”命令之后，这个文件夹中的文件就是用于生成 grub.cfg 中启动项的定义的</li></ul></li><li>/etc/default/grub 文件<ul><li>启动界面的配置，比如默认的启动项，等待用户选择启动项的时间等。当执行前面说的”update-grub”或者update-grub2”命令之后，这个文件的内容就 用于生成 grub.cfg 中启动界面的设置。</li></ul></li></ul></li><li><p>内核降级</p></li></ol><ul><li><p><strong>linux-image-</strong>: 内核镜像</p></li><li><p><strong>linux-image-extra-</strong>: 额外的内核模块</p></li><li><p><strong>linux-headers-</strong>: 内核头文件</p></li><li><p><a href="https://www.kernel.org/" target="_blank" rel="noopener">https://www.kernel.org/</a>  查看稳定的内核</p></li><li><p>官网: <a href="https://kernel.ubuntu.com/~kernel-ppa/mainline/" target="_blank" rel="noopener">https://kernel.ubuntu.com/~kernel-ppa/mainline/</a>  以及相应内核安装位置</p></li><li><p>安装4.19</p><ul><li>wget -c <a href="http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.19/linux-headers-4.19.0-041900_4.19.0-041900.201810221809_all.deb" target="_blank" rel="noopener">http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.19/linux-headers-4.19.0-041900_4.19.0-041900.201810221809_all.deb</a></li></ul><p>wget -c <a href="http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.19/linux-headers-4.19.0-041900-generic_4.19.0-041900.201810221809_amd64.deb" target="_blank" rel="noopener">http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.19/linux-headers-4.19.0-041900-generic_4.19.0-041900.201810221809_amd64.deb</a></p><p>wget -c <a href="http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.19/linux-image-unsigned-4.19.0-041900-generic_4.19.0-041900.201810221809_amd64.deb" target="_blank" rel="noopener">http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.19/linux-image-unsigned-4.19.0-041900-generic_4.19.0-041900.201810221809_amd64.deb</a></p><p>wget -c <a href="http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.19/linux-modules-4.19.0-041900-generic_4.19.0-041900.201810221809_amd64.deb" target="_blank" rel="noopener">http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.19/linux-modules-4.19.0-041900-generic_4.19.0-041900.201810221809_amd64.deb</a></p><p>sudo dpkg -i *.deb</p></li></ul><pre class=" language-shell"><code class="language-shell">#查看可用的内核apt-cache search linux-image#备份软件源sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak#添加一个源sudo vim /etc/apt/sources.listdeb http://security.ubuntu.com/ubuntu trusty-security mainsudo apt update#查看所有内核dpkg --get-selections| grep linux#安装指定版本内核sudo apt install 内核名称<linux-image-4.4.0-75-generic>dpkg -l | grep 内核名称<linux-image-extra-3.16.0-43-generic>  #查看是否安装成功#编辑grub 文件GRUB_DEFAULT=0GRUB_DEFAULT="Advanced options for Ubuntu>Ubuntu, with Linux 内核名称<5.0.0-36-generic>"Ubuntu,with Linux 5.3.0-25-generic#更新grub 引导sudo update-grubsudo rebootuname -r #查看当前版本是否安装正确#卸载内核sudo apt remove --purge 内核名称<linux-image-extra-3.16.0-43-generic>sudo dpkg --purge linux-image-4.19.0-041900-generic linux-image-unsigned-4.19.0-041900-genericsudo dpkg -P 内核名称  #通过deb包暗装的#关闭启动内核自动更新sudo apt-mark hold linux-image-generic linux-headers-genericsudo apt-mark unhold linux-image-generic linux-headers-generic</code></pre><p>使用指定版本内核  /boot 文件是内核相关的信息</p><ul><li><pre><code>grep menuentry /boot/grub/grub.cfg</code></pre></li></ul><p>例如文件如下:</p><pre><code>if [ x"${feature_menuentry_id}" = xy ]; then  menuentry_id_option="--id"  menuentry_id_option=""export menuentry_id_optionmenuentry 'Ubuntu' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-simple-5bce3795-da96-4c6f-bed2-67d37185a77d' {submenu 'Ubuntu 高级选项' $menuentry_id_option 'gnulinux-advanced-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu，Linux 4.8.0-26-lowlatency' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-45-lowlatency-advanced-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu, with Linux 4.8.0-26-lowlatency (upstart)' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-45-lowlatency-init-upstart-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu, with Linux 4.8.0-26-lowlatency (recovery mode)' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-45-lowlatency-recovery-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu，Linux 4.8.0-26-generic' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-45-generic-advanced-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu, with Linux 4.8.0-26-generic (upstart)' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-45-generic-init-upstart-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu, with Linux 4.8.0-26-generic (recovery mode)' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-45-generic-recovery-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu，Linux 4.4.0-21-generic' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-21-generic-advanced-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu, with Linux 4.4.0-21-generic (upstart)' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-21-generic-init-upstart-5bce3795-da96-4c6f-bed2-67d37185a77d' {    menuentry 'Ubuntu, with Linux 4.4.0-21-generic (recovery mode)' --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option 'gnulinux-4.4.0-21-generic-recovery-5bce3795-da96-4c6f-bed2-67d37185a77d' {menuentry 'Memory test (memtest86+)' {menuentry 'Memory test (memtest86+, serial console 115200)' {</code></pre><p>menuentry 代表一个内核, 从0开始记数字: 例如如果使用<strong>以4.4.0-21内核版本启动，则将文件/etc/default/grub中</strong></p><pre><code>GRUB_DEFAULT=0 </code></pre><p>改为 </p><pre><code>GRUB_DEFAULT=6</code></pre><p>或者改为</p><pre><code>GRUB_DEFAULT=”Ubuntu，Linux 4.4.0-21-generic“</code></pre><ul><li><strong>sudo update-grub</strong> 然后重启执行uname -r  查看系统内核</li></ul><ol><li><font color="red">Nouveau unknown chipset at install </font>  显卡驱动问题</li></ol><pre class=" language-shell"><code class="language-shell">sudo apt updatesudo apt upgradesudo ubuntu-drivers devicessudo apt install nvidia-xxx#如果没有NVIDIA驱动的话,  </code></pre><h3 id="文件"><a href="#文件" class="headerlink" title="文件"></a><strong>文件</strong></h3><ul><li><p>/boot 文件:  系统内核文件 ,启动管理程序grub 的目录</p><ul><li>Initrd 文件,系统启动摸块的只要来源,系统启动所需加载的虚拟磁盘</li><li>System.map 系统内核中的变量对应表</li><li>vmlinuz 是启动过程系统实际所用的内核</li><li>grub目录是启动管理程序的<ul><li>grub.conf 文件 从哪个内核进入,启动时间等</li></ul></li><li>kernel kernel主要负责的是北桥、南桥、CPU及内存，可见它们都是整个主机最重要的硬件核心部分，kernel如果处了问题，系统肯定无法启动起来。</li><li>kernel、initrd和system module是依次加载的。initrd包含一部分内核模块，主要是一些关键的外部硬件，如SATA、SCSI和USB等外设。它如果失败当然也会影响系统启动。而system module这些系统中的模块，是与支持和启动无很大关系的硬件有关，如果没有这些硬件设备的支持，系统也可以启动完成，只是存在功能上的缺失，如声卡、网卡、显卡等。这些系统模块也可以在启动后，以modprobe</li></ul></li><li><p>/lib 标准程序设计库，又叫动态链接共享库，作用类似windows里的.dll文件</p></li><li><p>/sbin 系统管理命令，这里存放的是系统管理员使用的管理程序</p></li><li><p>/bin 二进制可执行命令<br>/dev 设备特殊文件<br>/etc 系统管理和配置文件<br>/etc/rc.d 启动的配置文件和脚本</p></li><li><p>/lost+found 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里</p></li><li><p>/usr 最庞大的目录，要用到的应用程序和文件几乎都在这个目录。其中包含：<br>/usr/x11r6 存放x window的目录<br>/usr/bin 众多的应用程序<br>/usr/sbin 超级用户的一些管理程序<br><strong>/usr/include linux下开发和编译应用程序所需要的头文件</strong><br><strong>/usr/lib 常用的动态链接库和软件包的配置文件</strong><br><strong>/usr/doc linux文档</strong> /usr/man 帮助文档  /usr/info<br><strong>/usr/src 源代码，linux内核的源代码就放在/usr/src/linux里</strong><br><strong>/usr/local/bin 本地增加的命令</strong><br><strong>/usr/local/lib 本地增加的库</strong></p><p><img src="https://gitee.com/github-25970295/blogImage/raw/master/img/image-20191213221715677.png" alt="image-20191213221715677"></p><p>/var 包含系统一般运行时要改变的数据。通常这些数据所在的目录的大小是要经常变化<br>或扩充的。原来/ v a r目录中有些内容是在/ u s r中的，但为了保持/ u s r目录的相对稳定，就把那<br>些需要经常改变的目录放到/ v a r中了。每个系统是特定的，即不通过网络与其他计算机共享。</p><ul><li>/var/log:各种程序的日志( l o g )文件，尤其是login (/var/log/wtmp log纪录所有到系统的登录和注<br>销) 和syslog (/var/log/messages 纪录存储所有核心和系统程序信息)。/var/log 里的文件经常不<br>确定地增长，应该定期清除。</li></ul><p>/proc文件系统<br>/proc 文件系统是一个伪的文件系统，就是说它是一个实际上不存在的目录，因而这是一<br>个非常特殊的目录。它并不存在于某个磁盘上，而是由核心在内存中产生。这个目录用于提<br>供关于系统的信息。下面说明一些最重要的文件和目录(/proc 文件系统在proc man页中有更详<br>细的说明)。<br>\1. /proc/x<br>关于进程x的信息目录，这一x是这一进程的标识号。每个进程在/proc 下有一个名为自<br>己进程号的目录。<br>\2. /proc/cpuinfo<br>存放处理器( c p u )的信息，如c p u的类型、制造商、型号和性能等。<br>\3. /proc/devices<br>当前运行的核心配置的设备驱动的列表。<br>\4. /proc/dma<br>显示当前使用的d m a通道。<br>\5. /proc/filesystems<br>核心配置的文件系统信息。<br>\6. /proc/interrupts<br>显示被占用的中断信息和占用者的信息，以及被占用的数量。<br>\7. /proc/ioports<br>当前使用的i / o端口。<br>\8. /proc/kcore<br>系统物理内存映像。与物理内存大小完全一样，然而实际上没有占用这么多内存；它仅<br>仅是在程序访问它时才被创建。(注意：除非你把它拷贝到什么地方，否则/proc 下没有任何<br>东西占用任何磁盘空间。)<br>\9. /proc/kmsg<br>核心输出的消息。也会被送到s y s l o g。<br>\10. /proc/ksyms<br>核心符号表。<br>\11. /proc/loadavg<br>系统“平均负载”； 3个没有意义的指示器指出系统当前的工作量。<br>\12. /proc/meminfo<br>各种存储器使用信息，包括物理内存和交换分区( s w a p )。<br>\13. /proc/modules<br>存放当前加载了哪些核心模块信息。<br>\14. /proc/net<br>网络协议状态信息。<br>\15. /proc/self<br>存放到查看/proc 的程序的进程目录的符号连接。当2个进程查看/proc 时，这将会是不同<br>的连接。这主要便于程序得到它自己的进程目录。<br>\16. /proc/stat<br>系统的不同状态，例如，系统启动后页面发生错误的次数。<br>\17. /proc/uptime<br>系统启动的时间长度。<br>\18. /proc/version<br>核心版本。</p></li></ul><ol><li><p><strong>ACPI在</strong>BIOS和其他系统硬件中被实现，它就可以由操作系统所调用(触发)。</p><p>ACPI可以实现的功能包括：</p><p>系统电源管理（System power management）</p><p>设备电源管理（Device power management）</p><p>处理器电源管理（Processor power management）</p><p>设备和处理器性能管理（Device and processor performance management）</p><p>配置/即插即用（Configuration/Plug and Play）</p><p>系统事件（System Event）</p><p>电池管理（Battery management）</p><p>温度管理（Thermal management）</p><p><a href="https://baike.baidu.com/item/嵌入式控制器" target="_blank" rel="noopener">嵌入式控制器</a>（Embedded Controller）</p><p>SMBus控制器（SMBus Controller</p></li></ol><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OperationSystem </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vue Style Typora Theme</title>
      <link href="/2018/11/19/ruan-jian-gong-ju/hexotypora/vue-wen-dang-feng-ge-de-typora-zhu-ti/"/>
      <url>/2018/11/19/ruan-jian-gong-ju/hexotypora/vue-wen-dang-feng-ge-de-typora-zhu-ti/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://github.com/blinkfox/typora-vue-theme" target="_blank" rel="noopener">typora-vue-theme</a> 是 Typora Markdown 文档编辑器中一款类似<a href="https://vuejs.org/" target="_blank" rel="noopener">Vue</a>文档风格的主题。</p></blockquote><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><a href="https://www.typora.io/" target="_blank" rel="noopener">Typora</a>是一款支持实时预览的 Markdown 编辑器和阅读器，支持<code>Windows</code>、<code>macOS</code>、<code>Linux</code>三大平台。Typora 作为一款合格的 Markdown 编辑器，支持图片、列表、表格、代码、公式、目录等功能，同时这款软件还支持（一键）动态预览功能，让一切都变得如此干净、纯粹。并且有多种主题模板。<strong><a href="https://github.com/blinkfox/typora-vue-theme" target="_blank" rel="noopener">typora-vue-theme</a>就是参考了<a href="https://vuejs.org/" target="_blank" rel="noopener">Vue</a>文档风格而开发的一个 Typora 自定义主题</strong>。</p><h2 id="安装主题"><a href="#安装主题" class="headerlink" title="安装主题"></a>安装主题</h2><ol><li>下载本主题中的<code>vue.css</code>、<code>vue-dark.css</code>文件和包含字体的<code>vue</code>文件夹；</li><li>打开 Typora，点击“<strong>偏好设置</strong>” =&gt; “<strong>打开主题文件夹</strong>”按钮，将弹出 Typora 的主题文件夹；</li><li>将下载好的<code>vue.css</code>和<code>vue-dark.css</code>文件和包含字体的<code>vue</code>文件夹放到 Typora 的主题文件夹中；</li><li>关闭并重新打开 Typora，从菜单栏中选择 “<strong>主题</strong>” =&gt; “<strong>Vue</strong>” 或者 “<strong>Vue Dark</strong>” 即可。</li></ol><h2 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h2><p><img src="http://static.blinkfox.com/typora_vue_theme_screen_01.png" alt=""></p><p><img src="http://static.blinkfox.com/typora_vue_theme_screen_02.png" alt=""></p><p><img src="http://static.blinkfox.com/typora_vue_theme_screen_03.png" alt=""></p><h3 id="Vue-Dark"><a href="#Vue-Dark" class="headerlink" title="Vue Dark"></a>Vue Dark</h3><p><img src="https://github.com/MamoruDS/typora-vue-theme/raw/master/screenshots/screenshot_01.png" alt=""></p><p><img src="https://github.com/MamoruDS/typora-vue-theme/raw/master/screenshots/screenshot_02.png" alt=""></p><blockquote><p><strong>感谢</strong>: 本主题中的<code>vue-dark.css</code>来自<a href="https://github.com/MamoruDS/typora-vue-dark-theme" target="_blank" rel="noopener">typora-vue-dark-theme</a>.</p></blockquote><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> Markdown </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
